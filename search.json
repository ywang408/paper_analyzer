[
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Pacakges used in this notebook:"
  },
  {
    "objectID": "visualization.html#merge-datasets",
    "href": "visualization.html#merge-datasets",
    "title": "Data Visualization",
    "section": "Merge datasets",
    "text": "Merge datasets\nConcatenate all the dataframes from different subjects into one dataframe.\n\nif not os.path.exists('data.csv'):\n    df = pd.DataFrame()\n    for path in glob.glob(\"./datasets/*\"):\n        cat_csv = glob.glob(f\"{path}/*.csv\")\n        for csv in cat_csv:\n            df = pd.concat([df, pd.read_csv(csv)], axis=0)\n    df.to_csv(f\"./data.csv\", index=False)\n\nelse:\n    df = pd.read_csv(\"./data.csv\")"
  },
  {
    "objectID": "visualization.html#data-cleaning",
    "href": "visualization.html#data-cleaning",
    "title": "Data Visualization",
    "section": "Data cleaning",
    "text": "Data cleaning\nGenerally the data returned by arxiv api is pretty clean, we only need to perform some basic cleaning on merged dataset:\n\nDrop columns if no title and summary\nDrop duplicates if title and summary are the same\nSort by date again\n\nFurther cleaning will be done in the specific tasks.\n\ndf = df.dropna(subset=['title', 'authors'])\ndf = df.sort_values(by='updated', ascending=False)\ndf = df.drop_duplicates(subset=['title', 'authors'])\nlen(df)\n\n146021\n\n\n\nExtract papers with doi for BIA660 analysis\n\ndoi_papers = df[df['doi'].notnull()]\ndoi_papers.to_csv(\"./doi_papers.csv\", index=False)\ndoi_papers['main_category'].value_counts()\n\nmain_category\nastro-ph    8470\ncond-mat    6128\nphysics     5665\nnlin        4184\nq-bio       3110\ncs          2778\nq-fin       2395\nmath        1812\neess        1323\nquant-ph    1065\nstat         850\nmath-ph      819\necon         690\nhep-th       668\ngr-qc        630\nhep-ph       395\nnucl-th      104\nhep-ex        63\nhep-lat       28\nnucl-ex       23\nchao-dyn       1\nName: count, dtype: int64\n\n\n\n\nUse q-fin data for analysis inside one subject\n\nfin_papers = df[df['main_category'] == 'q-fin']\nfin_papers.to_csv(\"./fin_papers.csv\", index=False)\n\nfin_papers['term'].value_counts()\n\nterm\nq-fin.ST    1738\nq-fin.GN    1424\nq-fin.MF    1316\nq-fin.PR    1137\nq-fin.RM    1104\nq-fin.CP    1027\nq-fin.PM     983\nq-fin.TR     920\nq-fin.EC     383\nName: count, dtype: int64"
  },
  {
    "objectID": "visualization.html#data-visualization",
    "href": "visualization.html#data-visualization",
    "title": "Data Visualization",
    "section": "Data visualization",
    "text": "Data visualization\n\nCategories distribution\n\n\nCode\ndef filter(data, pct: float):\n    \"\"\"make entries with less than pct of total sum as others\"\"\"\n    n = data.sum()\n    data['others'] = data[data < n * pct].sum()\n    data = data[data >= n * pct]\n    return data.sort_values(ascending=False).to_dict()\n\n\n\n\nCode\nall_counts = df['main_category'].value_counts()\nall_counts = filter(all_counts, 0.02)\n\nsns.set_style(\"dark\")\nsns.set_palette('pastel')\nplt.pie(all_counts.values(), labels=all_counts.keys(),\n        autopct='%1.1f%%', labeldistance=1.05, pctdistance=0.75,)\nplt.title('Main Category Distribution')\nplt.show()\n\n\n\n\n\nOur whole dataset is balanced, next look at the distribution of each subject in the dataset.\n\n\nCode\nall_cats = all_counts.keys()\nall_cats = set(all_cats) - set(['others'])\n\n\nfig, axs = plt.subplots(4, 3, figsize=(20, 15))\nfor i, cat in enumerate(all_cats):\n    row = i // 3\n    col = i % 3\n    cat_df = df.query(f\"main_category == '{cat}'\")\n    cat_count = filter(cat_df['term'].value_counts(), 0.02)\n    axs[row, col].pie(cat_count.values(), labels=cat_count.keys(),\n                      autopct='%1.1f%%', labeldistance=1.05, pctdistance=0.75,)\n    axs[row, col].set_title(f\"{cat} Distribution\")\nplt.show()\n\n\n\n\n\n\n\nPublication percentage\n\n\nCode\npublished = {}\n\nif all_counts.get('others'):\n    all_counts.pop('others')\n\nfor cat in all_counts:\n    cat_df = df.query(f\"main_category == '{cat}'\")\n    cat_published = cat_df['doi'].count()\n    published[cat] = cat_published\ncategories = list(published.keys())\npaper_num = list(all_counts.values())\npublished_num = list(published.values())\npct_published = [p / n for p, n in zip(published_num, paper_num)]\n\nsns.set_style('darkgrid')  # Set the plot style\ncolors = sns.color_palette(\"Paired\")\nplt.figure(figsize=(12, 8))\nplt.title('Published Papers vs All Papers')\nax = sns.barplot(x=categories, y=paper_num, alpha=0.9,\n                 color=colors[0], errorbar=\"sd\", width=0.5,\n                 label='Num of Papers')\nsns.barplot(x=categories, y=published_num,\n            alpha=0.8, color=colors[1], errorbar=\"sd\", width=0.5,\n            label='Num of Published Papers')\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.0f', label_type='edge')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nWho likes to publish on arxiv?\nPaper submitted/updated from March 2023 to April 2023\n\n\nCode\ncolors = sns.color_palette(\"Set2\")\ncut_off = '2023-3-15'\nsub_df = df.query(\"main_category in @categories\")\nsub_df = df[df['updated'] > cut_off]\ngrouped = sub_df.groupby([pd.Grouper(key='updated', freq='D'),\n                      'main_category']).count()\n\npivot = grouped['paper_id'].unstack().fillna(0)\npivot = pivot.cumsum()\n\n# drop categories with less than 500 papers\nnum_papers = pivot.iloc[-1]\ncols = num_papers[num_papers > 500].index\npivot = pivot[cols]\n\nax = pivot.plot(figsize=(10, 6))\nax.set_title('Number of paper updated over time')\nax.set_xlabel('Update Date')\nplt.show()"
  },
  {
    "objectID": "visualization.html#goal",
    "href": "visualization.html#goal",
    "title": "Data Visualization",
    "section": "Goal",
    "text": "Goal\n\nAutomatically classify papers into different subjects\nExtract keywords(methods, datasets, etc.) from papers\nFind out is there any feature that can help us to determine whether a paper can be published"
  },
  {
    "objectID": "scrape.html",
    "href": "scrape.html",
    "title": "Scrape data from Arxiv",
    "section": "",
    "text": "In this part we levarage the official API of Arxiv to scrape the data we need. The api is well documented here."
  },
  {
    "objectID": "scrape.html#auxiliary-functions-for-extracting-extra-features-from-scraped-data",
    "href": "scrape.html#auxiliary-functions-for-extracting-extra-features-from-scraped-data",
    "title": "Scrape data from Arxiv",
    "section": "Auxiliary functions for extracting extra features from scraped data",
    "text": "Auxiliary functions for extracting extra features from scraped data\nSometimes the features we need are not directly available from the API. We need to extract them from the raw data. Here are some auxiliary functions for this purpose:\n\nget_num_figures: Get the number of figures in the paper from comment field.\nget_num_pages: Get the number of pages in the paper from comment field.\nget_num_eqs: For heavy math papers, we can get the number of equations from summary field.\nreplace_eq: Replace the equation in summary field with a placeholder.\n\n\n\nCode\nimport re\n\n\ndef get_num_figures(comment: str):\n    if comment is None:\n        return 0\n\n    # regex pattern to match \"n figures\" or \"n + m figures\", or mixtures of both\n    pattern = r\"\\d+\\s*\\+\\s*\\d+\\s*figures|\\d+\\s*figures\"\n    matches = re.findall(pattern, comment)\n\n    if len(matches) == 0:\n        return 0\n    else:\n        num_figures = sum(sum(int(n) for n in re.findall(r\"\\d+\", match))\n                        for match in matches)\n    return num_figures\n\n\ndef get_num_pages(comment: str):\n    if comment is None:\n        return 0\n\n    # regex pattern to match \"n pages\" or \"n + m pages\", no mixtures\n    pattern = r\"\\d+\\s*\\+\\s*\\d+\\s*pages|\\d+\\s*pages\"\n    match = re.search(pattern, comment)\n\n    if not match:\n        return 0\n    else:\n        match = match.group()\n        num_pages = sum(int(n) for n in re.findall(r\"\\d+\", match))\n    return num_pages\n\n\ndef get_number_eqs(summary: str):\n    if summary is None:\n        return 0\n\n    pattern = r\"\\${1,2}([\\s\\S]+?)\\${1,2}\"\n    matches = re.findall(pattern, summary)\n    return len(matches)\n\n\ndef replace_eq(summary: str, replacement: str):\n    if summary is None:\n        return summary\n\n    pattern = r\"\\${1,2}([\\s\\S]+?)\\${1,2}\"\n    return re.sub(pattern, replacement, summary)"
  },
  {
    "objectID": "scrape.html#scraper",
    "href": "scrape.html#scraper",
    "title": "Scrape data from Arxiv",
    "section": "Scraper",
    "text": "Scraper\nCompared to the pre-existing package(arxiv.arxiv) and official API, we use request instead of urllib to make the code more readable and easier to maintain, and we leverage the beautifulsoup package to parse the xml response.\nOur scraper contains two classes Paper and Search:\n\nPaper: A class for storing the information of a single paper.\nSearch: A class for sending request to the API and parsing the response, the responses will be stored in a list of Paper objects and return by the results method. We also implement a to_dataframe method to convert the results to a pandas dataframe, which is useful in the later part.\n\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\nclass Paper:\n    def __init__(self, *args, **kwargs) -> None:\n        self.paper_id = kwargs.get('paper_id')\n        self.updated = kwargs.get('updated')\n        self.published = kwargs.get('published')\n        self.title = kwargs.get('title')\n        self.authors = kwargs.get('authors')\n        self.summary = kwargs.get('summary')\n        self.comment = kwargs.get('comment')\n        self.link = kwargs.get('link')\n        self.doi = kwargs.get('doi')\n        self.pdf_link = kwargs.get('pdf_link')\n        self.term = kwargs.get('term')\n        self.metadata = self.to_dict()\n\n    def __repr__(self) -> str:\n        return f'Paper({self.paper_id})'\n\n    def __str__(self) -> str:\n        return (f\"title: {self.title}\\n\"\n                f\"authors: {self.authors}\\n\"\n                f\"link: {self.link}\\n\"\n                f\"doi: {self.doi}\\n\"\n                f\"pdf_link: {self.pdf_link}\\n\"\n                f\"term: {self.term}\\n\")\n\n    def to_dict(self) -> dict:\n        return {\n            'paper_id': self.paper_id,\n            'updated': self.updated,\n            'published': self.published,\n            'title': self.title,\n            'authors': self.authors,\n            'summary': self.summary,\n            'comment': self.comment,\n            'link': self.link,\n            'doi': self.doi,\n            'pdf_link': self.pdf_link,\n            'term': self.term\n        }\n\n    def download_pdf(self, path=\"./data\"):\n        # TODO: implement\n        pass\n\n\nclass Search:\n    def __init__(self, query: str = None, id_list: list = None, start: int = 0,\n                max_results: int = 2000, sort_by: str = \"relevance\", sort_order: str = 'descending') -> None:\n        self.query = query\n        self.id_list = id_list\n        self.start = start\n        self.max_results = max_results\n        self.sort_by = sort_by\n        self.sort_order = sort_order\n        self.url = self._url()  # query url\n        self.response = self._check_response()  # response from query\n\n    def __str__(self) -> str:\n        return f\"query at {self.query_date} for {self.query}/{self.id_list}\"\n\n    def _url(self) -> str:\n        base_url = \"http://export.arxiv.org/api/query?\"\n        if self.query:\n            query = f\"search_query={self.query}\"\n        elif self.id_list:\n            query = f\"id_list={','.join(self.id_list)}\"\n        else:\n            raise ValueError(\"Must provide query or id_list\")\n\n        url =  (f\"{base_url}\"\n                f\"{query}&start={self.start}\"\n                f\"&max_results={self.max_results}\"\n                f\"&sortBy={self.sort_by}\"\n                f\"&sortOrder={self.sort_order}\")\n        return url\n\n    def _check_response(self) -> None:\n        url = self._url()\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'xml')\n        if soup.find(\"entry\") is None:\n            print(f\"No results found for {url}\")\n        if soup.find(\"entry\").find(\"title\", recursive=False).text == \"Error\":\n            print(url)\n            raise ValueError(\"Invalid query or id_list\")\n        else:\n            return soup\n\n    def _parse_xml(self, xml):\n        paper_id = xml.find('id').text\n        updated = xml.find('updated').text\n        published = xml.find('published').text\n        title = xml.find('title').text\n        authors = [author.find(\n            'name').text for author in xml.find_all('author')]\n        summary = xml.find('summary').text\n        comment = xml.find('arxiv:comment').text if xml.find(\n            'arxiv:comment') else None\n        link = xml.find(\"link\", attrs={\"rel\": \"alternate\"})['href']\n        pdf_link = xml.find(\"link\", attrs={\"title\": \"pdf\"})['href']\n        doi = xml.find(\"arxiv:doi\").text if xml.find(\"arxiv:doi\") else None\n        term = xml.find('arxiv:primary_category')['term']\n        return Paper(paper_id=paper_id, updated=updated, published=published, title=title, authors=authors,\n                    summary=summary, comment=comment, link=link, doi=doi, pdf_link=pdf_link, term=term)\n\n    def results(self) -> list:\n        soup = self.response\n        entries = soup.find_all('entry')\n        papers = []\n        for entry in entries:\n            paper = self._parse_xml(entry)\n            papers.append(paper)\n        return papers\n\n    def to_dataframe(self) -> pd.DataFrame:\n        papers = self.results()\n        df = pd.DataFrame([paper.to_dict() for paper in papers])\n        df['num_figures'] = df['comment'].apply(get_num_figures)\n        df['num_pages'] = df['comment'].apply(get_num_pages)\n        df['num_eqs'] = df['summary'].apply(get_number_eqs)\n        df['main_category'] = df['term'].apply(lambda x: x.split('.')[0])\n        df['updated'] = pd.to_datetime(df['updated'])\n        df['published'] = pd.to_datetime(df['published'])\n        return df\n\n\nA typical workflow of using the scraper is as follows:\n\nsearch = Search(query=\"cat:q-fin.*\", start=0, max_results=10,\n                sort_by=\"submittedDate\", sort_order=\"descending\")\ndf = search.to_dataframe()\nprint(len(df))\ndf.sample(3)\n\n10\n\n\n\n\n\n\n  \n    \n      \n      paper_id\n      updated\n      published\n      title\n      authors\n      summary\n      comment\n      link\n      doi\n      pdf_link\n      term\n      num_figures\n      num_pages\n      num_eqs\n      main_category\n    \n  \n  \n    \n      5\n      http://arxiv.org/abs/2305.00541v1\n      2023-04-30 17:55:16+00:00\n      2023-04-30 17:55:16+00:00\n      A Stationary Mean-Field Equilibrium Model of I...\n      [René Aid, Matteo Basei, Giorgio Ferrari]\n      We consider a mean-field model of firms comp...\n      32 pages; 4 figures\n      http://arxiv.org/abs/2305.00541v1\n      None\n      http://arxiv.org/pdf/2305.00541v1\n      math.OC\n      4\n      32\n      0\n      math\n    \n    \n      4\n      http://arxiv.org/abs/2305.00545v1\n      2023-04-30 18:11:34+00:00\n      2023-04-30 18:11:34+00:00\n      Optimal multi-action treatment allocation: A t...\n      [Achim Ahrens, Alessandra Stampi-Bombelli, Sel...\n      The challenge of assigning optimal treatment...\n      None\n      http://arxiv.org/abs/2305.00545v1\n      None\n      http://arxiv.org/pdf/2305.00545v1\n      econ.GN\n      0\n      0\n      0\n      econ\n    \n    \n      9\n      http://arxiv.org/abs/2305.00200v1\n      2023-04-29 08:54:20+00:00\n      2023-04-29 08:54:20+00:00\n      Calibration of Local Volatility Models with St...\n      [Gregoire Loeper, Jan Obloj, Benjamin Joseph]\n      We develop a non-parametric, optimal transpo...\n      None\n      http://arxiv.org/abs/2305.00200v1\n      None\n      http://arxiv.org/pdf/2305.00200v1\n      q-fin.MF\n      0\n      0\n      0\n      q-fin"
  },
  {
    "objectID": "scrape.html#scraping-data",
    "href": "scrape.html#scraping-data",
    "title": "Scrape data from Arxiv",
    "section": "Scraping data",
    "text": "Scraping data\nIn this part, we scrape 16000 papers from all the categories of Arxiv. And to reuse the code, we create two json files main_cats.json and physics_cats.json to store the categories.(Physics has too many subcategories, so we split all the categories into two parts.)\n\nimport json\n\nwith open(\"data/main_cats.json\") as f:\n    main_cats = json.load(f)\n\nwith open('data/physics_cats.json') as f:\n    physics_cats = json.load(f)\n    \nall_cats = list(main_cats.keys()) + list(physics_cats.keys())\nprint(all_cats)\n\n['cs', 'econ', 'eess', 'math', 'q-bio', 'q-fin', 'stat', 'astro-ph', 'cond-mat', 'gr-qc', 'hep-ex', 'math-ph', 'nlin', 'nucl-ex', 'nucl-th', 'physics', 'quant-ph']\n\n\nAs documented in arxiv API,\n\nthe maximum number of a single call is limited to 30000 in slices of 2000 at time, using the start and max_results parameters. A request for 30000 results will typically take a little over 2 minutes to return a response of over 15MB. Requests for fewer results are much faster and correspondingly smaller.\n\nSo we set max_results to 2000 and scrape 8 times for each category such that the total number of papers for each category is 16000.\nSometimes the API returns error or empty response, for such cases we will save the failed categoreis and retry them later.\n\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm\n\ndef query_range(cat, start, max_results):\n    search = Search(query=f\"cat:{cat}.*\", start=start, max_results=max_results,\n                    sort_by='lastUpdatedDate', sort_order='descending')\n    temp_df = search.to_dataframe()\n    if not os.path.exists(f\"datasets/{cat}\"):\n        os.mkdir(f\"datasets/{cat}\")\n    temp_df.to_csv(f\"datasets/{cat}/{cat}_{int(start/2000)}.csv\", index=False)\n\n\nfailed = {}\n\nfor cat in physics_cats:\n    print(f\"Querying {cat}\")\n    for i in tqdm(range(0, 16000, 2000)):\n        start = i\n        end = i + 2000\n        try:\n            query_range(cat, start, 2000)\n            time.sleep(3)\n        except:\n            failed.setdefault(cat, []).append(start)\n            print(f\"Error at {cat} from {start} to {end}\")\n\nQuerying astro-ph\n\n\n100%|██████████| 8/8 [03:49<00:00, 28.69s/it]\n\n\nQuerying cond-mat\n\n\n100%|██████████| 8/8 [03:37<00:00, 27.21s/it]\n\n\nQuerying gr-qc\n\n\n 12%|█▎        | 1/8 [00:00<00:01,  4.02it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 0 to 2000\n\n\n 25%|██▌       | 2/8 [00:00<00:01,  4.19it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=2000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 2000 to 4000\n\n\n 38%|███▊      | 3/8 [00:00<00:01,  3.76it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 4000 to 6000\n\n\n 50%|█████     | 4/8 [00:01<00:01,  3.95it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=6000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 6000 to 8000\n\n\n 62%|██████▎   | 5/8 [00:01<00:00,  4.09it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=8000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 8000 to 10000\n\n\n 75%|███████▌  | 6/8 [00:01<00:00,  4.18it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=10000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 10000 to 12000\n\n\n 88%|████████▊ | 7/8 [00:01<00:00,  4.17it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 12000 to 14000\n\n\n100%|██████████| 8/8 [00:01<00:00,  4.11it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:gr-qc.*&start=14000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at gr-qc from 14000 to 16000\nQuerying hep-ex\n\n\n 12%|█▎        | 1/8 [00:00<00:01,  4.16it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 0 to 2000\n\n\n 25%|██▌       | 2/8 [00:00<00:01,  4.25it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=2000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 2000 to 4000\n\n\n 38%|███▊      | 3/8 [00:00<00:01,  4.33it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 4000 to 6000\n\n\n 50%|█████     | 4/8 [00:00<00:00,  4.39it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=6000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 6000 to 8000\n\n\n 62%|██████▎   | 5/8 [00:01<00:00,  4.39it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=8000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 8000 to 10000\n\n\n 75%|███████▌  | 6/8 [00:01<00:00,  4.38it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=10000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 10000 to 12000\n\n\n 88%|████████▊ | 7/8 [00:01<00:00,  4.37it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 12000 to 14000\n\n\n100%|██████████| 8/8 [00:01<00:00,  4.36it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:hep-ex.*&start=14000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at hep-ex from 14000 to 16000\nQuerying math-ph\n\n\n 12%|█▎        | 1/8 [00:00<00:01,  4.46it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 0 to 2000\n\n\n 25%|██▌       | 2/8 [00:00<00:01,  4.46it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=2000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 2000 to 4000\n\n\n 38%|███▊      | 3/8 [00:00<00:01,  4.36it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 4000 to 6000\n\n\n 50%|█████     | 4/8 [00:00<00:00,  4.34it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=6000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 6000 to 8000\n\n\n 62%|██████▎   | 5/8 [00:01<00:00,  4.37it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=8000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 8000 to 10000\n\n\n 75%|███████▌  | 6/8 [00:01<00:00,  4.40it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=10000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 10000 to 12000\n\n\n 88%|████████▊ | 7/8 [00:01<00:00,  4.36it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 12000 to 14000\nNo results found for http://export.arxiv.org/api/query?search_query=cat:math-ph.*&start=14000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at math-ph from 14000 to 16000\n\n\n100%|██████████| 8/8 [00:01<00:00,  4.40it/s]\n\n\nQuerying nlin\n\n\n100%|██████████| 8/8 [04:21<00:00, 32.63s/it]\n\n\nQuerying nucl-ex\n\n\n 12%|█▎        | 1/8 [00:00<00:01,  4.33it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 0 to 2000\n\n\n 25%|██▌       | 2/8 [00:00<00:01,  4.35it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=2000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 2000 to 4000\n\n\n 38%|███▊      | 3/8 [00:00<00:01,  4.29it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 4000 to 6000\n\n\n 50%|█████     | 4/8 [00:00<00:00,  4.37it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=6000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 6000 to 8000\n\n\n 62%|██████▎   | 5/8 [00:01<00:00,  4.35it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=8000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 8000 to 10000\n\n\n 75%|███████▌  | 6/8 [00:01<00:00,  4.33it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=10000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 10000 to 12000\n\n\n 88%|████████▊ | 7/8 [00:01<00:00,  4.32it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 12000 to 14000\n\n\n100%|██████████| 8/8 [00:01<00:00,  4.32it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-ex.*&start=14000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-ex from 14000 to 16000\nQuerying nucl-th\n\n\n 12%|█▎        | 1/8 [00:00<00:01,  4.40it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 0 to 2000\n\n\n 25%|██▌       | 2/8 [00:00<00:01,  4.24it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=2000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 2000 to 4000\n\n\n 38%|███▊      | 3/8 [00:00<00:01,  4.23it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 4000 to 6000\n\n\n 50%|█████     | 4/8 [00:00<00:00,  4.30it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=6000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 6000 to 8000\n\n\n 62%|██████▎   | 5/8 [00:01<00:00,  4.31it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=8000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 8000 to 10000\n\n\n 75%|███████▌  | 6/8 [00:01<00:00,  4.33it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=10000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 10000 to 12000\n\n\n 88%|████████▊ | 7/8 [00:01<00:00,  4.36it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 12000 to 14000\n\n\n100%|██████████| 8/8 [00:01<00:00,  4.34it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:nucl-th.*&start=14000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at nucl-th from 14000 to 16000\nQuerying physics\n\n\n 38%|███▊      | 3/8 [00:47<01:05, 13.14s/it]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:physics.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at physics from 4000 to 6000\n\n\n 88%|████████▊ | 7/8 [02:07<00:15, 15.71s/it]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:physics.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at physics from 12000 to 14000\n\n\n100%|██████████| 8/8 [02:34<00:00, 19.31s/it]\n\n\nQuerying quant-ph\n\n\n 12%|█▎        | 1/8 [00:00<00:01,  4.02it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=0&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 0 to 2000\n\n\n 25%|██▌       | 2/8 [00:00<00:01,  4.27it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=2000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 2000 to 4000\n\n\n 38%|███▊      | 3/8 [00:00<00:01,  4.33it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=4000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 4000 to 6000\n\n\n 50%|█████     | 4/8 [00:00<00:00,  4.39it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=6000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 6000 to 8000\n\n\n 62%|██████▎   | 5/8 [00:01<00:00,  4.36it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=8000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 8000 to 10000\n\n\n 75%|███████▌  | 6/8 [00:01<00:00,  4.35it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=10000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 10000 to 12000\n\n\n 88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=12000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 12000 to 14000\n\n\n100%|██████████| 8/8 [00:01<00:00,  4.32it/s]\n\n\nNo results found for http://export.arxiv.org/api/query?search_query=cat:quant-ph.*&start=14000&max_results=2000&sortBy=lastUpdatedDate&sortOrder=descending\nError at quant-ph from 14000 to 16000\n\n\n\n\n\n\nfailed\n\n{'gr-qc': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'hep-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'math-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-th': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'physics': [4000, 12000],\n 'quant-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]}\n\n\n\nredos = failed.copy()\nfailed = {}\n\nfor cat in redos:\n    print(f\"Querying {cat}\")\n    for i in tqdm(redos[cat]):\n        start = i\n        end = i + 2000\n        try:\n            query_range(cat, start, 2000)\n            time.sleep(3)\n        except:\n            failed.setdefault(cat, []).append(start)\n            print(f\"Error at {cat} from {start} to {end}\")\n\n{'gr-qc': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'hep-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'math-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-th': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'quant-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]}\n\n\nSometimes the number of papers returned by a single call is less than 1600, we check the number of papers returned by each call and save the failed categories and retry them later.\n\nimport glob\nimport re\ncats_data = glob.glob(\"datasets/*\")\n\nall_data = []\nfor cat in cats_data:\n    all_data += glob.glob(f\"{cat}/*.csv\")\n\n\nincomplete = []\n\nfor d in all_data:\n    len_df = len(pd.read_csv(d))\n    match = re.search(r\"datasets/([\\w-]+)/\\1_(\\d+)\\.csv$\", d)\n    cat = match.group(1)\n    num = int(match.group(2))\n    if len_df < 2000 and num != 7:\n        print(f\"{cat} {num} has {len_df} rows\")\n        incomplete.append((cat, num))\n\necon 3 has 1573 rows\n\n\n\n\nCode\nfor cat, num in incomplete:\n    start = num*2000\n    end = start + 2000\n    try:\n        query_range(cat, num*2000, 2000)\n        time.sleep(3)\n    except:\n        failed2.setdefault(cat, []).append(start)\n        print(f\"Error at {cat} from {start} to {end}\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Main page",
    "section": "",
    "text": "This is the main page for BIA 660/667 project.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 4, 2023\n\n\nData Visualization\n\n\n\n\n\n\n\n\nNo matching items"
  }
]