[
  {
    "objectID": "ada.html",
    "href": "ada.html",
    "title": "Load the OpenAI model and the dataset",
    "section": "",
    "text": "In this section, we are going to use the Ada model from OpenAI to embed the text. Similar to BERT, the Ada model is a transformer-based model. We are going to explore the performance of the Ada model on the same dataset as BERT. And we only test the embedding’s performance on the CNN classification model.\nThe following codes used to load API and fetch results are adapted from Andrew Ng’s prompt course on deeplearning.ai and OpenAI’s example.\nimport tiktoken\nimport openai\nfrom openai.embeddings_utils import get_embedding\nimport os\nimport pandas as pd\nimport json\nfrom dotenv import load_dotenv, find_dotenv\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n# embedding model parameters\nembedding_model = \"text-embedding-ada-002\"\nembedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\nmax_tokens = 500  # the maximum for text-embedding-ada-002 is 8191\ndf = pd.read_csv(\"../fin_papers.csv\")\ndf[\"combined\"] = (\n    \"Title: \" + df.title.str.strip() + \"; Content: \" + df.summary.str.strip()\n)\n\ncontents = df.combined.tolist()\nprint(contents[0])\n\nTitle: Maximum Implied Variance Slope -- Practical Aspects; Content: In the Black-Scholes model, the absence of arbitrages imposes necessary\nconstraints on the slope of the implied variance in terms of log-moneyness,\nasymptotically for large log-moneyness. The constraints are used for example in\nthe SVI implied volatility parameterization to ensure the resulting smile has\nno arbitrages. This note shows that those no-arbitrage contraints are very\nmild, and that arbitrage is almost always guaranteed in a large range of slopes\nwhere the contraints are enforced."
  },
  {
    "objectID": "ada.html#a-test-example-to-get-the-embedding",
    "href": "ada.html#a-test-example-to-get-the-embedding",
    "title": "Load the OpenAI model and the dataset",
    "section": "A test example to get the embedding",
    "text": "A test example to get the embedding\n\nencoding = tiktoken.get_encoding(embedding_encoding)\nresults = get_embedding(contents[0], engine=embedding_model)\n\nAttributeError: 'list' object has no attribute 'shape'\n\n\n\nprint(len(results))\n\n1536\n\n\n\nprint(type(results))\n\n&lt;class 'list'&gt;"
  },
  {
    "objectID": "gpt3.5.html",
    "href": "gpt3.5.html",
    "title": "Load API, data and helper functions",
    "section": "",
    "text": "In this section, we are going to leverage the most popular LLM model’s api from OpenAI to see if it could help us to classify papers in quantitive finance filed accurately.\nThe following api_key settings and helper function are adapted from Andrew Ng’s prompt course on deeplearning.ai.\nimport openai\nimport os\nimport pandas as pd\nimport json\nfrom dotenv import load_dotenv, find_dotenv\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\n\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\ndf = pd.read_csv(\"./qfin_processed_data.csv\")\ndf = df.drop_duplicates()\ndf = df[df.subcategory != 'EC']\nencoder = LabelEncoder()\ndf['label'] = encoder.fit_transform(df['subcategory'].values)\n\n\ntitles = df[\"title\"].values\nabstracts = df[\"summary\"].values\n\nprint(titles[0])\nprint(\"\\n\", abstracts[0])\n\nMaximum Implied Variance Slope -- Practical Aspects\n\n   In the Black-Scholes model, the absence of arbitrages imposes necessary\nconstraints on the slope of the implied variance in terms of log-moneyness,\nasymptotically for large log-moneyness. The constraints are used for example in\nthe SVI implied volatility parameterization to ensure the resulting smile has\nno arbitrages. This note shows that those no-arbitrage contraints are very\nmild, and that arbitrage is almost always guaranteed in a large range of slopes\nwhere the contraints are enforced.\n# Andrew mentioned that the prompt/ completion paradigm is preferable for this class\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0,  # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]"
  },
  {
    "objectID": "gpt3.5.html#prompt-design",
    "href": "gpt3.5.html#prompt-design",
    "title": "Load API, data and helper functions",
    "section": "Prompt Design",
    "text": "Prompt Design\n\ndef get_prompt(title, abstract):\n    prompt = f\"\"\"\nYour task is to determine a category for the given paper \\\nand give a short justification for your decision.\nThis paper is from Quantitative Finance filed, the category \\\nshould be one of the following: \n    1. Computational Finance \n    2. General Finance \n    3. Mathematical Finance \n    4. Portfolio Management \n    5. Pricing of Securities \n    6. Risk Management \n    7. Statistical Finance \n    8. Trading and Market Microstructure\n    \nGiven the title and abstract below, delimited by triple backticks.\nPlease determine the category and give a short justification for your decision in 30 words.\n\nTitle: ```{title}```\nAbstract: ```{abstract}```\n\nUsing the following format for your response:\nCategory: &lt;category name&gt;\nJustification: &lt;your justification&gt;\nOutput Json: &lt;json with category and justification&gt;\n\"\"\"\n    return prompt"
  },
  {
    "objectID": "gpt3.5.html#simple-example-to-test-the-api",
    "href": "gpt3.5.html#simple-example-to-test-the-api",
    "title": "Load API, data and helper functions",
    "section": "Simple example to test the api",
    "text": "Simple example to test the api\nA simple example could be the following case:\n\ntest_prompt = get_prompt(titles[0], abstracts[0])\nprint(test_prompt)\n\n\nYour task is to determine a category for the given paper and give a short justification for your decision.\nThis paper is from Quantitative Finance filed, the category should be one of the following: \n    1. Computational Finance \n    2. General Finance \n    3. Mathematical Finance \n    4. Portfolio Management \n    5. Pricing of Securities \n    6. Risk Management \n    7. Statistical Finance \n    8. Trading and Market Microstructure\n    \nGiven the title and abstract below, delimited by triple backticks.\nPlease determine the category and give a short justification for your decision in 30 words.\n\nTitle: ```Maximum Implied Variance Slope -- Practical Aspects```\nAbstract: ```  In the Black-Scholes model, the absence of arbitrages imposes necessary\nconstraints on the slope of the implied variance in terms of log-moneyness,\nasymptotically for large log-moneyness. The constraints are used for example in\nthe SVI implied volatility parameterization to ensure the resulting smile has\nno arbitrages. This note shows that those no-arbitrage contraints are very\nmild, and that arbitrage is almost always guaranteed in a large range of slopes\nwhere the contraints are enforced.\n```\n\nUsing the following format for your response:\nCategory: &lt;category name&gt;\nJustification: &lt;your justification&gt;\nOutput Json: &lt;json with category and justification&gt;\n\n\n\nThen we can use the helper functions to get the result:\n\nresponse = get_completion(test_prompt)\nprint(\"\\nResult of test example:\")\nprint(response)\n\n\nResult of test example:\nCategory: Mathematical Finance\nJustification: The paper discusses the necessary constraints on the slope of implied variance in the Black-Scholes model, which is a fundamental concept in mathematical finance. It also highlights the mildness of these constraints and the possibility of arbitrage, which is a crucial aspect of financial modeling. \nOutput Json: {\"Category\": \"Mathematical Finance\", \"Justification\": \"The paper discusses the necessary constraints on the slope of implied variance in the Black-Scholes model, which is a fundamental concept in mathematical finance. It also highlights the mildness of these constraints and the possibility of arbitrage, which is a crucial aspect of financial modeling.\"}\n\n\nWe can further use the json output to parse the result:\n\nstart_index = response.find('{')\nend_index = response.rfind('}')\njson_string = response[start_index:end_index+1]\n\nparsed_dict = json.loads(json_string)\nparsed_dict\n\n{'Category': 'Mathematical Finance',\n 'Justification': 'The paper discusses the necessary constraints on the slope of implied variance in the Black-Scholes model, which is a fundamental concept in mathematical finance. It also highlights the mildness of these constraints and the possibility of arbitrage, which is a crucial aspect of financial modeling.'}"
  },
  {
    "objectID": "gpt3.5.html#chatgpt-can-you-classify-papers-into-the-correct-category",
    "href": "gpt3.5.html#chatgpt-can-you-classify-papers-into-the-correct-category",
    "title": "Load API, data and helper functions",
    "section": "ChatGPT, can you classify papers into the correct category?",
    "text": "ChatGPT, can you classify papers into the correct category?\nNext we define a helper function to automatically generate the prompt and call the api to get the result. Here we save the result every 100 iterations to avoid losing the result due to the api call limit and also we can save money by avoiding accidents.\n\ndef parse_result(response):\n    start_index = response.find('{')\n    end_index = response.rfind('}')\n    json_string = response[start_index:end_index+1]\n    parsed_dict = json.loads(json_string)\n    return parsed_dict['Category'], parsed_dict['Justification']\n\n\nn = len(titles)\nfor i in tqdm(range(n)):\n    try:\n        title = titles[i]\n        abstract = abstracts[i]\n        curr_prompt = get_prompt(title, abstract)\n        response = get_completion(curr_prompt)\n        curr_cat, curr_jus = parse_result(response)\n        df.loc[i, 'predicted_category'] = curr_cat\n        df.loc[i, 'predicted_justification'] = curr_jus\n    except:\n        print(f\"Error at index {i}\")\n    if i % 50 == 0:\n        df.to_csv(\"./qfin_predictions.csv\", index=False)\n\n100%|██████████| 3/3 [00:22&lt;00:00,  7.58s/it]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Main page",
    "section": "",
    "text": "In this website, we present a novel NLP-based approach to classify academic papers into their respective subcategories, extract the most important keywords in each subcategory, and develop a recommendation system to suggest the most relevant papers in relation to a given input.\nOur proposed model leverages state-of-the-art NLP techniques, including web scraping, topic modeling, word embedding, and deep learning, to achieve these objectives. By providing an integrated solution for classification, keyword extraction, and recommendation, our approach aims to streamline the research process for academics, making it easier for them to identify and explore relevant works in their fields. This, in turn, can foster a more efficient knowledge transfer, and enable researchers to build upon the insights of others more effectively. The following posts provide a detailed description of each step in our approach and the results we obtained."
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Main page",
    "section": "",
    "text": "In this website, we present a novel NLP-based approach to classify academic papers into their respective subcategories, extract the most important keywords in each subcategory, and develop a recommendation system to suggest the most relevant papers in relation to a given input.\nOur proposed model leverages state-of-the-art NLP techniques, including web scraping, topic modeling, word embedding, and deep learning, to achieve these objectives. By providing an integrated solution for classification, keyword extraction, and recommendation, our approach aims to streamline the research process for academics, making it easier for them to identify and explore relevant works in their fields. This, in turn, can foster a more efficient knowledge transfer, and enable researchers to build upon the insights of others more effectively. The following posts provide a detailed description of each step in our approach and the results we obtained."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Main page",
    "section": "Contents",
    "text": "Contents"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "In this section we are showing an example of our workflow on applying models on classifying main categories of academic papers, i.e. Computer Science, Mathematics, Physics, Statistics, Quantitative Biology, Quantitative Finance. Our model is based on random forest, and we achieve an accuracy of 0.85 on the test set.\nTherefore, we change our goal to be classifying academic papers inside a main category into sub-categories in the rest of the project, it’s more realistic and challenging, because researchers are more interested in the sub-categories of their research field."
  },
  {
    "objectID": "Introduction.html#introduction",
    "href": "Introduction.html#introduction",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "In this section we are showing an example of our workflow on applying models on classifying main categories of academic papers, i.e. Computer Science, Mathematics, Physics, Statistics, Quantitative Biology, Quantitative Finance. Our model is based on random forest, and we achieve an accuracy of 0.85 on the test set.\nTherefore, we change our goal to be classifying academic papers inside a main category into sub-categories in the rest of the project, it’s more realistic and challenging, because researchers are more interested in the sub-categories of their research field."
  },
  {
    "objectID": "Introduction.html#classification-of-academic-papers",
    "href": "Introduction.html#classification-of-academic-papers",
    "title": "Introduction and Motivation",
    "section": "Classification of Academic Papers",
    "text": "Classification of Academic Papers\nIn the following part, we are going to apply random forest model to classify academic papers into different categories, the dataset is from our scrapper, you can find more details in Scrape data from Arxiv section.\nPackages used in this section:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom sklearn.model_selection import train_test_split\nimport ast\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\n\nData processing:\n\ndf = pd.read_csv(\"processed_data.csv\")\ndf = df.drop(columns = ['Unnamed: 0'])\ndf['summary'] = df['title'] + df['summary'] \n\ncategories = np.unique(df['main_category']).tolist() \ndf[categories] = pd.get_dummies(df['main_category'])\n\n\n\nMain categories of academic papers\n\nunique_label = (list(set(df.main_category)))\nunique_label\n\n['cs', 'math', 'stat', 'q-bio', 'q-fin', 'physics']\n\n\n\nprint(df['main_category'].value_counts())\n\ncs         1840\nphysics    1155\nmath        803\nq-bio       743\nstat        652\nq-fin       591\nName: main_category, dtype: int64\n\n\n\n\nRandom forest classifier\nHere we use random forest to classify academic papers into different categories, its performance is really good, this function is from TODO. Classification report, precision-recall curve and ROC curve are shown below.\n\n\nCode\ndef supervised_learning(train_df, test_df, min_df = 1, stopwords = None, base_model = LinearSVC()):\n    tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    X_train = tfidf_vect.fit_transform(train_df['summary'])\n    y_train = train_df[categories]\n    X_test = tfidf_vect.transform(test_df['summary'])\n    y_test = test_df[categories].values\n\n    model = OneVsRestClassifier(base_model)\n    model.fit(X_train, y_train)\n        \n    try:\n        predict_p = model.predict_proba(X_test)\n        y_pred = np.zeros_like(predict_p)\n        y_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        \n        precision = dict()\n        recall = dict()\n        for i in range(len(categories)):\n            precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        predict_p[:, i])\n            plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(categories[i]))\n    \n        plt.xlabel(\"Recall\")\n        plt.ylabel(\"Precision\")\n        plt.title(\"Precision-Recall Curve\")\n        plt.legend(loc=\"lower left\")\n        plt.show();\n    \n        # roc curve\n        fpr = dict()\n        tpr = dict()\n\n        for i in range(len(categories)):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i],predict_p[:, i])\n            plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(categories[i]))\n\n        plt.xlabel(\"false positive rate\")\n        plt.ylabel(\"true positive rate\")\n        plt.legend(loc=\"best\")\n        plt.title(\"ROC curve\")\n        plt.show();\n        return model,tfidf_vect\n    \n    except:\n        y_pred = model.predict(X_test)\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        print('The base model has no predict_proba function. ROC, PRC curve cannot be plotted!')\n        return model,tfidf_vect\n\n\n\nbase_model = RandomForestClassifier()\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nmodel,tfidf_vect = supervised_learning(train_df, test_df, min_df = 1, stopwords = 'english', base_model =base_model)\n\n              precision    recall  f1-score   support\n\n          cs       0.82      0.95      0.88       366\n        math       0.86      0.94      0.90       152\n     physics       0.90      0.84      0.87       233\n       q-bio       0.86      0.76      0.81       137\n       q-fin       0.94      0.94      0.94       139\n        stat       0.97      0.67      0.79       130\n\n   micro avg       0.87      0.87      0.87      1157\n   macro avg       0.89      0.85      0.86      1157\nweighted avg       0.88      0.87      0.87      1157\n samples avg       0.87      0.87      0.87      1157\n\n\n\n\n\n\n\n\n\n\n\nTop words in each category\n\n\nCode\nfor i in range(len(categories)):\n\n    important_features = []\n\n    for feature, importance in sorted(zip(tfidf_vect.get_feature_names_out(), model.estimators_[i].feature_importances_), \\\n                                  key=lambda x: x[1], reverse=True):\n        #print(feature, importance)\n        important_features.append(feature)\n    top_words=important_features[0:10]\n    print ('{}: {}'.format(categories[i],top_words))\n    print(\"\\n\")\n\n\ncs: ['learning', 'tasks', 'performance', 'training', 'language', 'art', 'trained', 'dataset', 'task', 'existing']\n\n\nmath: ['prove', 'mathbb', 'let', 'conjecture', 'theorem', 'algebras', 'spaces', 'model', 'mathcal', 'result']\n\n\nphysics: ['optical', 'fluid', 'beam', 'radiation', 'waves', 'wave', 'plasma', 'energy', 'magnetic', 'electron']\n\n\nq-bio: ['protein', 'brain', 'biological', 'genome', 'cells', 'metabolic', 'gene', 'population', 'proteins', 'biology']\n\n\nq-fin: ['market', 'volatility', 'risk', 'financial', 'price', 'portfolio', 'pricing', 'stock', 'asset', 'markets']\n\n\nstat: ['bayesian', 'outcome', 'estimators', 'statistical', 'inference', 'covariates', 'estimation', 'data', 'trial', 'simulation']"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning Models",
    "section": "",
    "text": "In this section, we explore unsupervised machine learning algorithms such as K-means clustering, Gaussian Mixture Model (GMM), and Latent Dirichlet Allocation (LDA) to group papers based on their underlying topics and extract the most important keywords within each subcategory.\nThese approaches rely on the assumption that similar papers will have similar distributions of words or features, and thus can be clustered together. LDA, a popular topic modeling technique, can uncover the underlying themes and topics present in a corpus of documents, generating a set of topics represented by a distribution of words.\nBy analyzing the most probable words within each topic, we can extract the most important keywords characterizing each subcategory, thus having valuable insights into the thematic organization of the Quantitative Finance domain.\nPackages used in this section:\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n#import matplotlib\n#matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom nltk.cluster import KMeansClusterer, cosine_distance, euclidean_distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nfrom sklearn import mixture\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.metrics.pairwise import cosine_similarity\n#from sklearn.metrics import pairwise_distances\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom sklearn.model_selection import train_test_split\nimport ast\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\n2023-05-03 15:04:44.566271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "ml.html#topic-modeling",
    "href": "ml.html#topic-modeling",
    "title": "Machine Learning Models",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\ndf = pd.read_csv('qfin_processed_data.csv')\ndf = df.drop(columns=['Unnamed: 0'])\ndf[\"tokenized_summary\"] = df[\"tokenized_summary\"].apply(lambda x: ast.literal_eval(x))\n#Combine the tokenized list back to a string \ndf[\"tokenized_summary_new\"] = df[\"tokenized_summary\"].apply(lambda x: ' '.join(x)) \ndf.head()\n\n\n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\nterm\nnum_figures\nnum_pages\nnum_eqs\nmain_category\nsubcategory\ncombined_Bert\ntokenized_summary\ntokenized_summary_new\n\n\n\n\n0\nhttp://arxiv.org/abs/2304.13610v1\n2023-04-26 15:05:19+00:00\n2023-04-26 15:05:19+00:00\nMaximum Implied Variance Slope -- Practical As...\n[\"Fabien Le Floc'h\", 'Winfried Koller']\nMaximum Implied Variance Slope -- Practical As...\nNaN\nhttp://arxiv.org/abs/2304.13610v1\nNaN\nhttp://arxiv.org/pdf/2304.13610v1\nq-fin.PR\n0\n0\n0\nq-fin\nPR\nTitle: Maximum Implied Variance Slope -- Pract...\n[maximum, implied, variance, slope, practical,...\nmaximum implied variance slope practical aspec...\n\n\n1\nhttp://arxiv.org/abs/2206.02582v2\n2023-04-26 13:13:28+00:00\n2022-06-06 12:43:06+00:00\nMaking heads or tails of systemic risk measures\n['Aleksy Leeuwenkamp']\nMaking heads or tails of systemic risk measure...\nRevised version of the $\\Delta$-CoES paper, no...\nhttp://arxiv.org/abs/2206.02582v2\nNaN\nhttp://arxiv.org/pdf/2206.02582v2\nq-fin.RM\n0\n22\n4\nq-fin\nRM\nTitle: Making heads or tails of systemic risk ...\n[making, head, tail, systemic, risk, measure, ...\nmaking head tail systemic risk measure paper s...\n\n\n2\nhttp://arxiv.org/abs/2301.00790v2\n2023-04-26 10:56:51+00:00\n2022-12-30 17:19:00+00:00\nDynamic Feature Engineering and model selectio...\n['Thomas Wong', 'Mauricio Barahona']\nDynamic Feature Engineering and model selectio...\nNaN\nhttp://arxiv.org/abs/2301.00790v2\nNaN\nhttp://arxiv.org/pdf/2301.00790v2\nq-fin.CP\n0\n0\n0\nq-fin\nCP\nTitle: Dynamic Feature Engineering and model s...\n[dynamic, feature, engineering, model, selecti...\ndynamic feature engineering model selection me...\n\n\n3\nhttp://arxiv.org/abs/2304.13402v1\n2023-04-26 09:28:41+00:00\n2023-04-26 09:28:41+00:00\nConvexity adjustments à la Malliavin\n['David García-Lorite', 'Raul Merino']\nConvexity adjustments à la Malliavin In this ...\nNaN\nhttp://arxiv.org/abs/2304.13402v1\nNaN\nhttp://arxiv.org/pdf/2304.13402v1\nq-fin.MF\n0\n0\n0\nq-fin\nMF\nTitle: Convexity adjustments à la Malliavin; C...\n[convexity, adjustment, la, malliavin, paper, ...\nconvexity adjustment la malliavin paper develo...\n\n\n4\nhttp://arxiv.org/abs/2304.13128v1\n2023-04-25 20:16:36+00:00\n2023-04-25 20:16:36+00:00\nLearning Volatility Surfaces using Generative ...\n['Andrew Na', 'Meixin Zhang', 'Justin Wan']\nLearning Volatility Surfaces using Generative ...\nThis is a working draft\nhttp://arxiv.org/abs/2304.13128v1\nNaN\nhttp://arxiv.org/pdf/2304.13128v1\nq-fin.CP\n0\n0\n0\nq-fin\nCP\nTitle: Learning Volatility Surfaces using Gene...\n[learning, volatility, surface, using, generat...\nlearning volatility surface using generative a...\n\n\n\n\n\n\n\n\ntrain_df, test_df= train_test_split(df, test_size=0.2, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\n\nUnsupervised learning\nWe first tried clustering and LDA that were discussed in topic modeling session. We used the K-means, GMM and LDA.\nThe code chunk shows the helper function cluster for all of the three unsupervised learning models.\n\n\nCode\ndef cluster(train_data, test_data, num_clusters, min_df = 1, stopwords = None, method = 'k-mean'):\n    \n    train_temp = train_data.copy() \n    test_temp = test_data.copy()\n    \n    train_temp[\"tokenized_summary\"] = train_data[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n    test_temp[\"tokenized_summary\"] = test_data[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n    \n    \n   # add your code\n    if method == 'k-mean':\n        clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=20)\n        \n        tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)     \n        \n        dtm = tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        clusters = clusterer.cluster(dtm.toarray(), assign_clusters=True)\n    \n        test_dtm = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        predicted = [clusterer.classify(v) for v in test_dtm.toarray()]\n        \n    \n    \n    elif method == 'gmm':\n        gmm = mixture.GaussianMixture(n_components=num_clusters,\n                                      covariance_type='diag', random_state=42)\n        \n        tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    \n        dtm= tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        gmm.fit(dtm.toarray())\n    \n        test_dtm = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        predicted = gmm.predict(test_dtm.toarray())\n    \n    else: \n        tfidf_vect = CountVectorizer(min_df = min_df, stop_words = stopwords)\n    \n        X_train = tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        lda = LatentDirichletAllocation(n_components=num_clusters, \\\n                                max_iter=20,verbose=1,\n                                evaluate_every=1, n_jobs=1,\n                                random_state=0).fit(X_train)\n        \n        X_test = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        doc_topic = lda.transform(X_test)\n        \n        predicted = doc_topic.argmax(axis = 1)\n    \n    \n    confusion_df = pd.DataFrame(list(zip(test_temp[\"subcategory\"].values, predicted)),columns = [\"label\", \"cluster\"])\n    \n    print (pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label))\n    \n    #Apply majority vote rule to dynamically map each cluster to a ground-truth label in test_data\n    label_allocation = pd.DataFrame(confusion_df.groupby('cluster')['label'].apply\\\n                                    (lambda x: x.value_counts().idxmax()))\n    \n    \n    #Interpret each cluster by centroid\n    \n    if method in ['k-mean','gmm']:\n        \n        if method =='k-mean':\n            centroids=np.array(clusterer.means())\n        else:\n            centroids=np.array(gmm.means_)\n        \n        sorted_centroids = centroids.argsort()[:, ::-1]\n\n        voc_lookup= tfidf_vect.get_feature_names_out()\n    \n        for i in range(num_clusters):\n            top_words=[voc_lookup[word_index] for word_index in sorted_centroids[i, :10]]\n            print ('Cluster {} -&gt; Topic {}'.format(i,label_allocation['label'][i]))\n            print ('    top words: ' +str(top_words))\n        \n    else: \n        voc_lookup= tfidf_vect.get_feature_names_out()\n    \n    \n        for topic_idx, topic in enumerate(lda.components_):\n            top_words=[voc_lookup[i] for i in topic.argsort()[::-1][0:10]]\n            print ('Cluster {} -&gt; Topic {}'.format(topic_idx,label_allocation['label'][topic_idx]))\n            print ('    top words: ' +str(top_words))\n    \n    #classification report\n    cluster_dict = {}\n    for i in range(num_clusters):\n        cluster_dict[i] = label_allocation['label'][i]\n    \n    predicted_target=[cluster_dict[i] for i in predicted]\n    \n    print(metrics.classification_report(test_temp[\"subcategory\"], predicted_target))\n     \n    return None\n\n\n\nK-means\n\ncluster(train_df, test_df, num_clusters = 8, min_df = 5, stopwords = 'english', method = 'k-mean')\n\nlabel    CP   GN  MF  PM  PR   RM   ST   TR\ncluster                                    \n0        86    3  46   2  78    6    3    3\n1        29    6  40   2  43    3   37    4\n2        11   21  17  63   3  101   17    0\n3        17   20  39   3  58   43   10    4\n4        22    8  73  82  18   15    3    9\n5        19  176   7   3   3   24   93   20\n6         8   21  27   7   7    4   26  115\n7        25   45   8  17   5   11  182   29\nCluster 0 -&gt; Topic CP\n    top words: ['option', 'pricing', 'model', 'price', 'method', 'hedging', 'black', 'scholes', 'numerical', 'american']\nCluster 1 -&gt; Topic PR\n    top words: ['volatility', 'model', 'implied', 'stochastic', 'price', 'option', 'process', 'rough', 'heston', 'jump']\nCluster 2 -&gt; Topic RM\n    top words: ['risk', 'portfolio', 'measure', 'model', 'optimization', 'asset', 'return', 'approach', 'distribution', 'value']\nCluster 3 -&gt; Topic PR\n    top words: ['default', 'credit', 'model', 'rate', 'risk', 'arbitrage', 'bond', 'bank', 'market', 'pricing']\nCluster 4 -&gt; Topic PM\n    top words: ['optimal', 'problem', 'utility', 'strategy', 'portfolio', 'investment', 'function', 'solution', 'time', 'process']\nCluster 5 -&gt; Topic GN\n    top words: ['distribution', 'model', 'financial', 'economic', 'network', 'time', 'data', 'agent', 'market', 'growth']\nCluster 6 -&gt; Topic TR\n    top words: ['order', 'trading', 'market', 'price', 'book', 'impact', 'strategy', 'limit', 'model', 'agent']\nCluster 7 -&gt; Topic ST\n    top words: ['stock', 'market', 'price', 'correlation', 'data', 'model', 'return', 'financial', 'time', 'learning']\n              precision    recall  f1-score   support\n\n          CP       0.38      0.40      0.39       217\n          GN       0.51      0.59      0.55       300\n          MF       0.00      0.00      0.00       257\n          PM       0.36      0.46      0.40       179\n          PR       0.28      0.47      0.35       215\n          RM       0.43      0.49      0.46       207\n          ST       0.57      0.49      0.53       371\n          TR       0.53      0.62      0.58       184\n\n    accuracy                           0.44      1930\n   macro avg       0.38      0.44      0.41      1930\nweighted avg       0.39      0.44      0.41      1930\n\n\n\n\n\nGaussian Mixture Model\n\ncluster(train_df, test_df, num_clusters = 8, min_df = 5, stopwords = 'english', method = 'gmm')\n\nlabel    CP   GN  MF  PM  PR  RM   ST  TR\ncluster                                  \n0         2   13   4   5   5   9   25   4\n1        23    1  31   2  27   3   17   2\n2         8   14  20  16   4   7   28  45\n3         1    0   2   2   1   3    8  13\n4        84  243  61  61  58  91  251  98\n5        69    3  52   3  66  15    9   3\n6        15   15  24  27  28  58   26  10\n7        15   11  63  63  26  21    7   9\nCluster 0 -&gt; Topic ST\n    top words: ['distribution', 'model', 'return', 'income', 'tail', 'wealth', 'time', 'law', 'probability', 'financial']\nCluster 1 -&gt; Topic MF\n    top words: ['volatility', 'model', 'implied', 'stochastic', 'option', 'price', 'rough', 'process', 'time', 'market']\nCluster 2 -&gt; Topic TR\n    top words: ['trading', 'market', 'agent', 'strategy', 'price', 'model', 'learning', 'impact', 'cost', 'stock']\nCluster 3 -&gt; Topic TR\n    top words: ['order', 'book', 'limit', 'price', 'market', 'model', 'impact', 'lob', 'flow', 'stock']\nCluster 4 -&gt; Topic ST\n    top words: ['market', 'stock', 'model', 'financial', 'time', 'network', 'price', 'data', 'correlation', 'analysis']\nCluster 5 -&gt; Topic CP\n    top words: ['option', 'pricing', 'model', 'price', 'method', 'hedging', 'equation', 'process', 'numerical', 'american']\nCluster 6 -&gt; Topic RM\n    top words: ['risk', 'measure', 'model', 'portfolio', 'default', 'credit', 'systemic', 'financial', 'network', 'capital']\nCluster 7 -&gt; Topic MF\n    top words: ['optimal', 'portfolio', 'problem', 'utility', 'strategy', 'risk', 'investment', 'function', 'asset', 'optimization']\n              precision    recall  f1-score   support\n\n          CP       0.31      0.32      0.32       217\n          GN       0.00      0.00      0.00       300\n          MF       0.29      0.37      0.33       257\n          PM       0.00      0.00      0.00       179\n          PR       0.00      0.00      0.00       215\n          RM       0.29      0.28      0.28       207\n          ST       0.27      0.74      0.40       371\n          TR       0.34      0.32      0.33       184\n\n    accuracy                           0.29      1930\n   macro avg       0.19      0.25      0.21      1930\nweighted avg       0.19      0.29      0.22      1930\n\n\n\n\n\nLatent Dirichlet Allocation\n\ncluster(train_df, test_df, num_clusters = 8, min_df = 5, stopwords = 'english', method = 'lda')\n\niteration: 1 of max_iter: 20, perplexity: 1662.1941\niteration: 2 of max_iter: 20, perplexity: 1474.4942\niteration: 3 of max_iter: 20, perplexity: 1372.6859\niteration: 4 of max_iter: 20, perplexity: 1318.5665\niteration: 5 of max_iter: 20, perplexity: 1288.2508\niteration: 6 of max_iter: 20, perplexity: 1270.3625\niteration: 7 of max_iter: 20, perplexity: 1259.0478\niteration: 8 of max_iter: 20, perplexity: 1251.2028\niteration: 9 of max_iter: 20, perplexity: 1245.4672\niteration: 10 of max_iter: 20, perplexity: 1241.2476\niteration: 11 of max_iter: 20, perplexity: 1238.0928\niteration: 12 of max_iter: 20, perplexity: 1235.7949\niteration: 13 of max_iter: 20, perplexity: 1234.0271\niteration: 14 of max_iter: 20, perplexity: 1232.5545\niteration: 15 of max_iter: 20, perplexity: 1231.4097\niteration: 16 of max_iter: 20, perplexity: 1230.4636\niteration: 17 of max_iter: 20, perplexity: 1229.6428\niteration: 18 of max_iter: 20, perplexity: 1228.9761\niteration: 19 of max_iter: 20, perplexity: 1228.3584\niteration: 20 of max_iter: 20, perplexity: 1227.8524\nlabel    CP   GN   MF  PM  PR  RM   ST  TR\ncluster                                   \n0         5    9   16  63   1   5   17  70\n1        19   17  128  66  49  13    4  12\n2        55    3   57   1  85   7   37   5\n3        13  150    6   7  11  38   39  18\n4        14   73   10   6   7  16  149  44\n5        33   28    8  16   2  20  119  32\n6        74    2   26  17  53  75    4   2\n7         4   18    6   3   7  33    2   1\nCluster 0 -&gt; Topic TR\n    top words: ['portfolio', 'market', 'trading', 'strategy', 'order', 'asset', 'optimal', 'risk', 'price', 'model']\nCluster 1 -&gt; Topic MF\n    top words: ['optimal', 'problem', 'time', 'market', 'price', 'model', 'utility', 'process', 'strategy', 'function']\nCluster 2 -&gt; Topic PR\n    top words: ['model', 'volatility', 'process', 'option', 'price', 'stochastic', 'pricing', 'time', 'rate', 'jump']\nCluster 3 -&gt; Topic GN\n    top words: ['financial', 'market', 'network', 'risk', 'bank', 'economic', 'firm', 'crisis', 'systemic', 'country']\nCluster 4 -&gt; Topic ST\n    top words: ['market', 'time', 'distribution', 'model', 'stock', 'price', 'return', 'financial', 'correlation', 'dynamic']\nCluster 5 -&gt; Topic ST\n    top words: ['stock', 'data', 'model', 'market', 'learning', 'price', 'financial', 'based', 'using', 'network']\nCluster 6 -&gt; Topic RM\n    top words: ['risk', 'measure', 'option', 'method', 'pricing', 'approach', 'value', 'hedging', 'problem', 'model']\nCluster 7 -&gt; Topic RM\n    top words: ['model', 'risk', 'rate', 'insurance', 'capital', 'bond', 'company', 'loss', 'life', 'income']\n              precision    recall  f1-score   support\n\n          CP       0.00      0.00      0.00       217\n          GN       0.53      0.50      0.52       300\n          MF       0.42      0.50      0.45       257\n          PM       0.00      0.00      0.00       179\n          PR       0.34      0.40      0.37       215\n          RM       0.33      0.52      0.40       207\n          ST       0.46      0.72      0.57       371\n          TR       0.38      0.38      0.38       184\n\n    accuracy                           0.42      1930\n   macro avg       0.31      0.38      0.34      1930\nweighted avg       0.34      0.42      0.37      1930\n\n\n\n\n\n\nSupervised Learning\nAs it is a multi-class classification problem, we used the OneVsRestClassifer in scikit-learn package. For the base classifiers, we tried Linear SVC and RandomForest Classifiers.\n\n#One Hot Encoding \ncategories = np.unique(df['subcategory']).tolist()\ndf[categories] = pd.get_dummies(df['subcategory'])\ndf[categories]\n\n\n\n\n\n\n\n\nCP\nGN\nMF\nPM\nPR\nRM\nST\nTR\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9644\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n9645\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n9646\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n9647\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n9648\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n9649 rows × 8 columns\n\n\n\nLikewise, we defined a helper function supervised_learning to offer the flexibility of using different base classifiers.\n\n\nCode\ndef supervised_learning(train_df, test_df, min_df = 1, stopwords = None, base_model = LinearSVC()):\n    tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    X_train = tfidf_vect.fit_transform(train_df['tokenized_summary_new'])\n    y_train = train_df[categories]\n    X_test = tfidf_vect.transform(test_df['tokenized_summary_new'])\n    y_test = test_df[categories].values\n\n    model = OneVsRestClassifier(base_model)\n    model.fit(X_train, y_train)\n        \n    try:\n        predict_p = model.predict_proba(X_test)\n        y_pred = np.zeros_like(predict_p)\n        y_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        \n        precision = dict()\n        recall = dict()\n        for i in range(len(categories)):\n            precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        predict_p[:, i])\n            plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(categories[i]))\n    \n        plt.xlabel(\"Recall\")\n        plt.ylabel(\"Precision\")\n        plt.title(\"Precision-Recall Curve\")\n        plt.legend(loc=\"lower left\")\n        plt.show();\n    \n        # roc curve\n        fpr = dict()\n        tpr = dict()\n\n        for i in range(len(categories)):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i],predict_p[:, i])\n            plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(categories[i]))\n\n        plt.xlabel(\"false positive rate\")\n        plt.ylabel(\"true positive rate\")\n        plt.legend(loc=\"best\")\n        plt.title(\"ROC curve\")\n        plt.show();\n        return model,tfidf_vect\n    \n    except:\n        y_pred = model.predict(X_test)\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        print('The base model has no predict_proba function. ROC, PRC curve cannot be plotted!')\n        return model,tfidf_vect\n\n\n\nSupport Vector Machine\n\nbase_model = LinearSVC()\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n_,_= supervised_learning(train_df, test_df, min_df = 1, stopwords = 'english', base_model =base_model)\n\n              precision    recall  f1-score   support\n\n          CP       0.64      0.36      0.46       217\n          GN       0.77      0.55      0.64       300\n          MF       0.50      0.30      0.37       257\n          PM       0.68      0.49      0.57       179\n          PR       0.56      0.43      0.49       215\n          RM       0.71      0.50      0.59       207\n          ST       0.71      0.56      0.63       371\n          TR       0.73      0.46      0.56       184\n\n   micro avg       0.67      0.46      0.55      1930\n   macro avg       0.66      0.46      0.54      1930\nweighted avg       0.67      0.46      0.55      1930\n samples avg       0.45      0.46      0.45      1930\n\nThe base model has no predict_proba function. ROC, PRC curve cannot be plotted!\n\n\n\n\nRandom Forest\n\nbase_model = RandomForestClassifier()\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nmodel,tfidf_vect = supervised_learning(train_df, test_df, min_df = 1, stopwords = 'english', base_model =base_model)\n\n              precision    recall  f1-score   support\n\n          CP       0.66      0.41      0.51       217\n          GN       0.66      0.72      0.69       300\n          MF       0.49      0.38      0.43       257\n          PM       0.61      0.64      0.62       179\n          PR       0.56      0.67      0.61       215\n          RM       0.69      0.64      0.66       207\n          ST       0.62      0.76      0.68       371\n          TR       0.66      0.64      0.65       184\n\n   micro avg       0.62      0.62      0.62      1930\n   macro avg       0.62      0.61      0.61      1930\nweighted avg       0.62      0.62      0.61      1930\n samples avg       0.62      0.62      0.62      1930\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\n\nCode\nX_test = tfidf_vect.transform(test_df['tokenized_summary_new'])\ny_test = test_df[categories].values\ny_test = np.argmax(y_test, axis=1)\npredict_p = model.predict_proba(X_test)\ny_pred = np.zeros_like(predict_p)\ny_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\ny_pred = np.argmax(y_pred, axis=1)\n\nlabel_dict = dict()\nfor i in range(8):\n    label_dict[i] = categories[i]\n    \ny_true_labels = [label_dict[num] for num in y_test]\ny_pred_labels = [label_dict[num] for num in y_pred]\n\nConfusionMatrixDisplay.from_predictions(y_true_labels, y_pred_labels,cmap=plt.cm.Blues,values_format='g');\n\n\n\n\n\n\n\nCode\n#feature importance \n\nfor i in range(len(categories)):\n\n    important_features = []\n\n    for feature, importance in sorted(zip(tfidf_vect.get_feature_names_out(), model.estimators_[i].feature_importances_), \\\n                                  key=lambda x: x[1], reverse=True):\n        #print(feature, importance)\n        important_features.append(feature)\n    top_words=important_features[0:10]\n    print ('{}: {}'.format(categories[i],top_words))\n    print(\"\\n\")\n\n\nCP: ['carlo', 'monte', 'method', 'numerical', 'option', 'scheme', 'algorithm', 'computational', 'approximation', 'neural']\n\n\nGN: ['economic', 'economy', 'country', 'growth', 'economics', 'risk', 'income', 'production', 'business', 'firm']\n\n\nMF: ['arbitrage', 'problem', 'existence', 'stochastic', 'process', 'optimal', 'model', 'martingale', 'pricing', 'price']\n\n\nPM: ['portfolio', 'optimization', 'investment', 'optimal', 'problem', 'markowitz', 'utility', 'selection', 'strategy', 'constraint']\n\n\nPR: ['pricing', 'option', 'formula', 'price', 'swap', 'volatility', 'derivative', 'european', 'implied', 'model']\n\n\nRM: ['risk', 'measure', 'shortfall', 'loss', 'insurance', 'price', 'systemic', 'market', 'var', 'default']\n\n\nST: ['series', 'stock', 'correlation', 'index', 'data', 'return', 'prediction', 'time', 'analysis', 'financial']\n\n\nTR: ['book', 'order', 'trading', 'execution', 'impact', 'trader', 'market', 'limit', 'trade', 'volume']"
  },
  {
    "objectID": "scrape.html",
    "href": "scrape.html",
    "title": "Scrape data from Arxiv",
    "section": "",
    "text": "In this part we levarage the official API of Arxiv to scrape the data we need. The api is well documented in Arxiv’s user manual."
  },
  {
    "objectID": "scrape.html#auxiliary-functions-for-extracting-extra-features-from-scraped-data",
    "href": "scrape.html#auxiliary-functions-for-extracting-extra-features-from-scraped-data",
    "title": "Scrape data from Arxiv",
    "section": "Auxiliary functions for extracting extra features from scraped data",
    "text": "Auxiliary functions for extracting extra features from scraped data\nSometimes the features we need are not directly available from the API. We need to extract them from the raw data. Here are some auxiliary functions for this purpose:\n\nget_num_figures: Get the number of figures in the paper from comment field.\nget_num_pages: Get the number of pages in the paper from comment field.\nget_num_eqs: For heavy math papers, we can get the number of equations from summary field.\nreplace_eq: Replace the equation in summary field with a placeholder.\n\n\n\nCode\nimport re\n\n\ndef get_num_figures(comment: str):\n    if comment is None:\n        return 0\n\n    # regex pattern to match \"n figures\" or \"n + m figures\", or mixtures of both\n    pattern = r\"\\d+\\s*\\+\\s*\\d+\\s*figures|\\d+\\s*figures\"\n    matches = re.findall(pattern, comment)\n\n    if len(matches) == 0:\n        return 0\n    else:\n        num_figures = sum(sum(int(n) for n in re.findall(r\"\\d+\", match))\n                        for match in matches)\n    return num_figures\n\n\ndef get_num_pages(comment: str):\n    if comment is None:\n        return 0\n\n    # regex pattern to match \"n pages\" or \"n + m pages\", no mixtures\n    pattern = r\"\\d+\\s*\\+\\s*\\d+\\s*pages|\\d+\\s*pages\"\n    match = re.search(pattern, comment)\n\n    if not match:\n        return 0\n    else:\n        match = match.group()\n        num_pages = sum(int(n) for n in re.findall(r\"\\d+\", match))\n    return num_pages\n\n\ndef get_number_eqs(summary: str):\n    if summary is None:\n        return 0\n\n    pattern = r\"\\${1,2}([\\s\\S]+?)\\${1,2}\"\n    matches = re.findall(pattern, summary)\n    return len(matches)\n\n\ndef replace_eq(summary: str, replacement: str):\n    if summary is None:\n        return summary\n\n    pattern = r\"\\${1,2}([\\s\\S]+?)\\${1,2}\"\n    return re.sub(pattern, replacement, summary)"
  },
  {
    "objectID": "scrape.html#scraper",
    "href": "scrape.html#scraper",
    "title": "Scrape data from Arxiv",
    "section": "Scraper",
    "text": "Scraper\nCompared to the pre-existing package(arxiv.arxiv) and official API, we use request instead of urllib to make the code more readable and easier to maintain, and we leverage the beautifulsoup package to parse the xml response.\nOur scraper contains two classes Paper and Search:\n\nPaper: A class for storing the information of a single paper.\nSearch: A class for sending request to the API and parsing the response, the responses will be stored in a list of Paper objects and return by the results method. We also implement a to_dataframe method to convert the results to a pandas dataframe, which is useful in the later part.\n\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\nclass Paper:\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.paper_id = kwargs.get('paper_id')\n        self.updated = kwargs.get('updated')\n        self.published = kwargs.get('published')\n        self.title = kwargs.get('title')\n        self.authors = kwargs.get('authors')\n        self.summary = kwargs.get('summary')\n        self.comment = kwargs.get('comment')\n        self.link = kwargs.get('link')\n        self.doi = kwargs.get('doi')\n        self.pdf_link = kwargs.get('pdf_link')\n        self.term = kwargs.get('term')\n        self.metadata = self.to_dict()\n\n    def __repr__(self) -&gt; str:\n        return f'Paper({self.paper_id})'\n\n    def __str__(self) -&gt; str:\n        return (f\"title: {self.title}\\n\"\n                f\"authors: {self.authors}\\n\"\n                f\"link: {self.link}\\n\"\n                f\"doi: {self.doi}\\n\"\n                f\"pdf_link: {self.pdf_link}\\n\"\n                f\"term: {self.term}\\n\")\n\n    def to_dict(self) -&gt; dict:\n        return {\n            'paper_id': self.paper_id,\n            'updated': self.updated,\n            'published': self.published,\n            'title': self.title,\n            'authors': self.authors,\n            'summary': self.summary,\n            'comment': self.comment,\n            'link': self.link,\n            'doi': self.doi,\n            'pdf_link': self.pdf_link,\n            'term': self.term\n        }\n\n    def download_pdf(self, path=\"./data\"):\n        # TODO: implement\n        pass\n\n\nclass Search:\n    def __init__(self, query: str = None, id_list: list = None, start: int = 0,\n                max_results: int = 2000, sort_by: str = \"relevance\", sort_order: str = 'descending') -&gt; None:\n        self.query = query\n        self.id_list = id_list\n        self.start = start\n        self.max_results = max_results\n        self.sort_by = sort_by\n        self.sort_order = sort_order\n        self.url = self._url()  # query url\n        self.response = self._check_response()  # response from query\n\n    def __str__(self) -&gt; str:\n        return f\"query at {self.query_date} for {self.query}/{self.id_list}\"\n\n    def _url(self) -&gt; str:\n        base_url = \"http://export.arxiv.org/api/query?\"\n        if self.query:\n            query = f\"search_query={self.query}\"\n        elif self.id_list:\n            query = f\"id_list={','.join(self.id_list)}\"\n        else:\n            raise ValueError(\"Must provide query or id_list\")\n\n        url =  (f\"{base_url}\"\n                f\"{query}&start={self.start}\"\n                f\"&max_results={self.max_results}\"\n                f\"&sortBy={self.sort_by}\"\n                f\"&sortOrder={self.sort_order}\")\n        return url\n\n    def _check_response(self) -&gt; None:\n        url = self._url()\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'xml')\n        if soup.find(\"entry\") is None:\n            print(f\"No results found for {url}\")\n        if soup.find(\"entry\").find(\"title\", recursive=False).text == \"Error\":\n            print(url)\n            raise ValueError(\"Invalid query or id_list\")\n        else:\n            return soup\n\n    def _parse_xml(self, xml):\n        paper_id = xml.find('id').text\n        updated = xml.find('updated').text\n        published = xml.find('published').text\n        title = xml.find('title').text\n        authors = [author.find(\n            'name').text for author in xml.find_all('author')]\n        summary = xml.find('summary').text\n        comment = xml.find('arxiv:comment').text if xml.find(\n            'arxiv:comment') else None\n        link = xml.find(\"link\", attrs={\"rel\": \"alternate\"})['href']\n        pdf_link = xml.find(\"link\", attrs={\"title\": \"pdf\"})['href']\n        doi = xml.find(\"arxiv:doi\").text if xml.find(\"arxiv:doi\") else None\n        term = xml.find('arxiv:primary_category')['term']\n        return Paper(paper_id=paper_id, updated=updated, published=published, title=title, authors=authors,\n                    summary=summary, comment=comment, link=link, doi=doi, pdf_link=pdf_link, term=term)\n\n    def results(self) -&gt; list:\n        soup = self.response\n        entries = soup.find_all('entry')\n        papers = []\n        for entry in entries:\n            paper = self._parse_xml(entry)\n            papers.append(paper)\n        return papers\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        papers = self.results()\n        df = pd.DataFrame([paper.to_dict() for paper in papers])\n        df['num_figures'] = df['comment'].apply(get_num_figures)\n        df['num_pages'] = df['comment'].apply(get_num_pages)\n        df['num_eqs'] = df['summary'].apply(get_number_eqs)\n        df['main_category'] = df['term'].apply(lambda x: x.split('.')[0])\n        df['updated'] = pd.to_datetime(df['updated'])\n        df['published'] = pd.to_datetime(df['published'])\n        return df\n\n\nA typical workflow of using the scraper is as follows:\n\nsearch = Search(query=\"cat:q-fin.*\", start=0, max_results=10,\n                sort_by=\"submittedDate\", sort_order=\"descending\")\ndf = search.to_dataframe()\nprint(len(df))\ndf.sample(3)\n\n10\n\n\n\n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\nterm\nnum_figures\nnum_pages\nnum_eqs\nmain_category\n\n\n\n\n5\nhttp://arxiv.org/abs/2305.00541v1\n2023-04-30 17:55:16+00:00\n2023-04-30 17:55:16+00:00\nA Stationary Mean-Field Equilibrium Model of I...\n[René Aid, Matteo Basei, Giorgio Ferrari]\nWe consider a mean-field model of firms comp...\n32 pages; 4 figures\nhttp://arxiv.org/abs/2305.00541v1\nNone\nhttp://arxiv.org/pdf/2305.00541v1\nmath.OC\n4\n32\n0\nmath\n\n\n4\nhttp://arxiv.org/abs/2305.00545v1\n2023-04-30 18:11:34+00:00\n2023-04-30 18:11:34+00:00\nOptimal multi-action treatment allocation: A t...\n[Achim Ahrens, Alessandra Stampi-Bombelli, Sel...\nThe challenge of assigning optimal treatment...\nNone\nhttp://arxiv.org/abs/2305.00545v1\nNone\nhttp://arxiv.org/pdf/2305.00545v1\necon.GN\n0\n0\n0\necon\n\n\n9\nhttp://arxiv.org/abs/2305.00200v1\n2023-04-29 08:54:20+00:00\n2023-04-29 08:54:20+00:00\nCalibration of Local Volatility Models with St...\n[Gregoire Loeper, Jan Obloj, Benjamin Joseph]\nWe develop a non-parametric, optimal transpo...\nNone\nhttp://arxiv.org/abs/2305.00200v1\nNone\nhttp://arxiv.org/pdf/2305.00200v1\nq-fin.MF\n0\n0\n0\nq-fin"
  },
  {
    "objectID": "scrape.html#scraping-data",
    "href": "scrape.html#scraping-data",
    "title": "Scrape data from Arxiv",
    "section": "Scraping data",
    "text": "Scraping data\nIn this part, we scrape 16000 papers from all the categories of Arxiv. And to reuse the code, we create two json files main_cats.json and physics_cats.json to store the categories.(Physics has too many subcategories, so we split all the categories into two parts.)\n\nimport json\n\nwith open(\"data/main_cats.json\") as f:\n    main_cats = json.load(f)\n\nwith open('data/physics_cats.json') as f:\n    physics_cats = json.load(f)\n    \nall_cats = list(main_cats.keys()) + list(physics_cats.keys())\nprint(all_cats)\n\n['cs', 'econ', 'eess', 'math', 'q-bio', 'q-fin', 'stat', 'astro-ph', 'cond-mat', 'gr-qc', 'hep-ex', 'math-ph', 'nlin', 'nucl-ex', 'nucl-th', 'physics', 'quant-ph']\n\n\nAs documented in arxiv API,\n\nthe maximum number of a single call is limited to 30000 in slices of 2000 at time, using the start and max_results parameters. A request for 30000 results will typically take a little over 2 minutes to return a response of over 15MB. Requests for fewer results are much faster and correspondingly smaller.\n\nSo we set max_results to 2000 and scrape 8 times for each category such that the total number of papers for each category is 16000.\nSometimes the API returns error or empty response, for such cases we will save the failed categoreis and retry them later.\n\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm\n\ndef query_range(cat, start, max_results):\n    search = Search(query=f\"cat:{cat}.*\", start=start, max_results=max_results,\n                    sort_by='lastUpdatedDate', sort_order='descending')\n    temp_df = search.to_dataframe()\n    if not os.path.exists(f\"datasets/{cat}\"):\n        os.mkdir(f\"datasets/{cat}\")\n    temp_df.to_csv(f\"datasets/{cat}/{cat}_{int(start/2000)}.csv\", index=False)\n\n\nfailed = {}\n\nfor cat in physics_cats:\n    print(f\"Querying {cat}\")\n    for i in tqdm(range(0, 16000, 2000)):\n        start = i\n        end = i + 2000\n        try:\n            query_range(cat, start, 2000)\n            time.sleep(3)\n        except:\n            failed.setdefault(cat, []).append(start)\n            print(f\"Error at {cat} from {start} to {end}\")\n\n\nfailed\n\n{'gr-qc': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'hep-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'math-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-th': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'physics': [4000, 12000],\n 'quant-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]}\n\n\n\nredos = failed.copy()\nfailed = {}\n\nfor cat in redos:\n    print(f\"Querying {cat}\")\n    for i in tqdm(redos[cat]):\n        start = i\n        end = i + 2000\n        try:\n            query_range(cat, start, 2000)\n            time.sleep(3)\n        except:\n            failed.setdefault(cat, []).append(start)\n            print(f\"Error at {cat} from {start} to {end}\")\n\n{'gr-qc': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'hep-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'math-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-th': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'quant-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]}\n\n\nSometimes the number of papers returned by a single call is less than 1600, we check the number of papers returned by each call and save the failed categories and retry them later.\n\nimport glob\nimport re\ncats_data = glob.glob(\"datasets/*\")\n\nall_data = []\nfor cat in cats_data:\n    all_data += glob.glob(f\"{cat}/*.csv\")\n\n\nincomplete = []\n\nfor d in all_data:\n    len_df = len(pd.read_csv(d))\n    match = re.search(r\"datasets/([\\w-]+)/\\1_(\\d+)\\.csv$\", d)\n    cat = match.group(1)\n    num = int(match.group(2))\n    if len_df &lt; 2000 and num != 7:\n        print(f\"{cat} {num} has {len_df} rows\")\n        incomplete.append((cat, num))\n\necon 3 has 1573 rows\n\n\n\n\nCode\nfor cat, num in incomplete:\n    start = num*2000\n    end = start + 2000\n    try:\n        query_range(cat, num*2000, 2000)\n        time.sleep(3)\n    except:\n        failed2.setdefault(cat, []).append(start)\n        print(f\"Error at {cat} from {start} to {end}\")"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Pacakges used in this notebook:\nCode\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\n\nimport seaborn as sns\nimport seaborn.objects as so\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "visualization.html#merge-datasets",
    "href": "visualization.html#merge-datasets",
    "title": "Data Visualization",
    "section": "Merge datasets",
    "text": "Merge datasets\nConcatenate all the dataframes from different subjects into one dataframe.\n\nif not os.path.exists('data.csv'):\n    df = pd.DataFrame()\n    for path in glob.glob(\"./datasets/*\"):\n        cat_csv = glob.glob(f\"{path}/*.csv\")\n        for csv in cat_csv:\n            df = pd.concat([df, pd.read_csv(csv)], axis=0)\n    df.to_csv(f\"./data.csv\", index=False)\n\nelse:\n    df = pd.read_csv(\"./data.csv\")"
  },
  {
    "objectID": "visualization.html#data-cleaning",
    "href": "visualization.html#data-cleaning",
    "title": "Data Visualization",
    "section": "Data cleaning",
    "text": "Data cleaning\nGenerally the data returned by arxiv api is pretty clean, we only need to perform some basic cleaning on merged dataset:\n\nDrop columns if no title and summary\nDrop duplicates if title and summary are the same\nSort by date again\n\nFurther cleaning will be done in the specific tasks.\n\ndf = df.dropna(subset=['title', 'authors'])\ndf = df.sort_values(by='updated', ascending=False)\ndf = df.drop_duplicates(subset=['title', 'authors'])\nlen(df)\n\n146021\n\n\n\nExtract papers with doi for introduction part\n\ndoi_papers = df[df['doi'].notnull()]\ndoi_papers.to_csv(\"./doi_papers.csv\", index=False)\ndoi_papers['main_category'].value_counts()\n\nmain_category\nastro-ph    8470\ncond-mat    6128\nphysics     5665\nnlin        4184\nq-bio       3110\ncs          2778\nq-fin       2395\nmath        1812\neess        1323\nquant-ph    1065\nstat         850\nmath-ph      819\necon         690\nhep-th       668\ngr-qc        630\nhep-ph       395\nnucl-th      104\nhep-ex        63\nhep-lat       28\nnucl-ex       23\nchao-dyn       1\nName: count, dtype: int64\n\n\n\n\nUse q-fin data for analysis inside one subject\n\nfin_papers = df[df['main_category'] == 'q-fin']\nfin_papers.to_csv(\"./fin_papers.csv\", index=False)\n\nfin_papers['term'].value_counts()\n\nterm\nq-fin.ST    1738\nq-fin.GN    1424\nq-fin.MF    1316\nq-fin.PR    1137\nq-fin.RM    1104\nq-fin.CP    1027\nq-fin.PM     983\nq-fin.TR     920\nq-fin.EC     383\nName: count, dtype: int64"
  },
  {
    "objectID": "visualization.html#data-visualization",
    "href": "visualization.html#data-visualization",
    "title": "Data Visualization",
    "section": "Data visualization",
    "text": "Data visualization\n\nCategories distribution\n\n\nCode\ndef filter(data, pct: float):\n    \"\"\"make entries with less than pct of total sum as others\"\"\"\n    n = data.sum()\n    data['others'] = data[data &lt; n * pct].sum()\n    data = data[data &gt;= n * pct]\n    return data.sort_values(ascending=False).to_dict()\n\n\n\n\nCode\nall_counts = df['main_category'].value_counts()\nall_counts = filter(all_counts, 0.02)\n\nsns.set_style(\"dark\")\nsns.set_palette('pastel')\nplt.pie(all_counts.values(), labels=all_counts.keys(),\n        autopct='%1.1f%%', labeldistance=1.05, pctdistance=0.75,)\nplt.title('Main Category Distribution')\nplt.show()\n\n\n\n\n\nCategory Distribution\n\n\n\n\nOur whole dataset is balanced, next look at the distribution of each subject in the dataset.\n\n\nCode\nall_cats = all_counts.keys()\nall_cats = set(all_cats) - set(['others'])\n\n\nfig, axs = plt.subplots(4, 3, figsize=(20, 15))\nfor i, cat in enumerate(all_cats):\n    row = i // 3\n    col = i % 3\n    cat_df = df.query(f\"main_category == '{cat}'\")\n    cat_count = filter(cat_df['term'].value_counts(), 0.02)\n    axs[row, col].pie(cat_count.values(), labels=cat_count.keys(),\n                      autopct='%1.1f%%', labeldistance=1.05, pctdistance=0.75,)\n    axs[row, col].set_title(f\"{cat} Distribution\")\nplt.show()\n\n\n\n\n\nCategory Distribution For Each Sub-Category\n\n\n\n\n\n\nPublication percentage\n\n\nCode\npublished = {}\n\nif all_counts.get('others'):\n    all_counts.pop('others')\n\nfor cat in all_counts:\n    cat_df = df.query(f\"main_category == '{cat}'\")\n    cat_published = cat_df['doi'].count()\n    published[cat] = cat_published\ncategories = list(published.keys())\npaper_num = list(all_counts.values())\npublished_num = list(published.values())\npct_published = [p / n for p, n in zip(published_num, paper_num)]\n\nsns.set_style('darkgrid')  # Set the plot style\ncolors = sns.color_palette(\"Paired\")\nplt.figure(figsize=(12, 8))\nplt.title('Published Papers vs All Papers')\nax = sns.barplot(x=categories, y=paper_num, alpha=0.9,\n                 color=colors[0], errorbar=\"sd\", width=0.5,\n                 label='Num of Papers')\nsns.barplot(x=categories, y=published_num,\n            alpha=0.8, color=colors[1], errorbar=\"sd\", width=0.5,\n            label='Num of Published Papers')\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.0f', label_type='edge')\n\nplt.legend()\nplt.show()\n\n\n\n\n\nPublished Papers vs All Papers\n\n\n\n\n\n\nWho likes to publish on arxiv?\nPaper submitted/updated from March 2023 to April 2023\n\n\nCode\ncolors = sns.color_palette(\"Set2\")\ncut_off = '2023-3-15'\nsub_df = df.query(\"main_category in @categories\")\nsub_df = df[df['updated'] &gt; cut_off]\ngrouped = sub_df.groupby([pd.Grouper(key='updated', freq='D'),\n                      'main_category']).count()\n\npivot = grouped['paper_id'].unstack().fillna(0)\npivot = pivot.cumsum()\n\n# drop categories with less than 500 papers\nnum_papers = pivot.iloc[-1]\ncols = num_papers[num_papers &gt; 500].index\npivot = pivot[cols]\n\nax = pivot.plot(figsize=(10, 6))\nax.set_title('Number of paper updated over time')\nax.set_xlabel('Update Date')\nplt.show()\n\n\n\n\n\nNumber of Papers Updated Over Time"
  }
]