[
  {
    "objectID": "ada.html",
    "href": "ada.html",
    "title": "Prepare data",
    "section": "",
    "text": "This notebook uses the BertForSequenceClassification, its source code can be found on github. TL.DR, it’s just BERT embedding + linear classifier.\nimport numpy as np\nimport pandas as pd\nimport ast\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\n\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv('qfin_processed_data.csv')\ndf = df[df.subcategory != 'EC']\n\ndf[\"tokenized_summary\"] = df[\"tokenized_summary\"].apply(lambda x: ast.literal_eval(x))\ndf[\"tokenized_summary_new\"] = df[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n\ncategories = df['subcategory'].unique()\nn_labels = len(categories)\nprint(n_labels)\ncat_dict = dict(zip(categories, range(n_labels)))\ndf['cat_id'] = df['subcategory'].map(cat_dict)\ndf.sample(3)\n\n8\n\n\n\n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\nterm\nnum_figures\nnum_pages\nnum_eqs\nmain_category\nsubcategory\ntokenized_summary\ntokenized_summary_new\ncat_id\n\n\n\n\n768\nhttp://arxiv.org/abs/2208.14267v1\n2022-08-30 13:42:55+00:00\n2022-08-30 13:42:55+00:00\nCommon Idiosyncratic Quantile Risk\n['Jozef Barunik', 'Matej Nevrla']\nWe propose a new model of asset returns with...\nNaN\nhttp://arxiv.org/abs/2208.14267v1\nNaN\nhttp://arxiv.org/pdf/2208.14267v1\nq-fin.GN\n0\n0\n0\nq-fin\nGN\n[propose, new, asset, return, common, factor, ...\npropose new asset return common factor shift r...\n6\n\n\n3981\nhttp://arxiv.org/abs/1902.08821v1\n2019-02-23 17:52:46+00:00\n2019-02-23 17:52:46+00:00\nClosed-End Formula for options linked to Targe...\n['Luca Di Persio', 'Luca Prezioso', 'Kai Wallb...\nRecent years have seen an emerging class of ...\n21 pages, 12 figures\nhttp://arxiv.org/abs/1902.08821v1\nNaN\nhttp://arxiv.org/pdf/1902.08821v1\nq-fin.PR\n12\n21\n0\nq-fin\nPR\n[recent, year, see, emerge, class, structured,...\nrecent year see emerge class structured financ...\n0\n\n\n237\nhttp://arxiv.org/abs/2302.08731v1\n2023-02-17 07:00:00+00:00\n2023-02-17 07:00:00+00:00\nOptimal management of DB pension fund under bo...\n['Guohui Guan', 'Zongxia Liang', 'Yi Xia']\nThis paper investigates the optimal manageme...\nNaN\nhttp://arxiv.org/abs/2302.08731v1\nNaN\nhttp://arxiv.org/pdf/2302.08731v1\nq-fin.PM\n0\n0\n0\nq-fin\nPM\n[investigate, optimal, management, aggregated,...\ninvestigate optimal management aggregated defi...\n7\ndef get_pretrained_wordvector(sentences, tokenizer):\n\n    input_ids = []\n    attention_masks = []\n\n    # Tokenize each sentence\n    for sent in sentences:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 150,           # Pad & truncate all sentences.\n                        truncation=True,\n                        padding = 'max_length',\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                        )\n        temp_ids = encoded_dict['input_ids'].squeeze()\n        temp_masks = encoded_dict['attention_mask'].squeeze()\n        input_ids.append(temp_ids)\n        attention_masks.append(temp_masks)\n    return input_ids, attention_masks\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\nsentences = df['tokenized_summary_new'].values\n\ninput_ids, attention_masks = get_pretrained_wordvector(sentences, tokenizer)\n# print(input_ids.shape, attention_masks.shape)\nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])\nprint('Attention Mask:', attention_masks[0])\n\nOriginal:  black Scholes absence arbitrage impose necessary constraint slope imply variance term log moneyness asymptotically large log moneyness constraint example SVI imply volatility parameterization ensure result smile arbitrage note show arbitrage contraint mild arbitrage guarantee large range slope contraint enforce\nToken IDs: tensor([  101,  2304,  8040, 19990,  6438, 12098, 16313, 24449, 17607,  4072,\n        27142,  9663, 19515, 23284,  2744,  8833,  2769,  2791,  2004, 24335,\n        13876, 20214,  3973,  2312,  8833,  2769,  2791, 27142,  2742, 17917,\n         2072, 19515,  5285, 10450, 18605, 16381,  3989,  5676,  2765,  2868,\n        12098, 16313, 24449,  3602,  2265, 12098, 16313, 24449, 24528, 18447,\n        10256, 12098, 16313, 24449, 11302,  2312,  2846,  9663, 24528, 18447,\n        16306,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\nAttention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0])\nprint(input_ids[0].shape)\n\ntorch.Size([150])\nclass BertDataset(Dataset):\n    def __init__(self, features, labels) -&gt; None:\n        super(BertDataset).__init__()\n        self.lengeth = len(labels)\n        self.features = features\n        self.labels = torch.tensor(labels)\n        \n    def __getitem__(self, index):\n        return self.features[index], self.labels[index]\n\n    def __len__(self):\n        return self.lengeth\nX = list(zip(input_ids, attention_masks))\ny = df['cat_id'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\ntrain_dataset = BertDataset(X_train, y_train)\nval_dataset = BertDataset(X_val, y_val)\ntest_dataset = BertDataset(X_test, y_test)\n\nlen(train_dataset), len(val_dataset), len(test_dataset)\n\n(6947, 772, 1930)"
  },
  {
    "objectID": "ada.html#model-settings",
    "href": "ada.html#model-settings",
    "title": "Prepare data",
    "section": "Model settings",
    "text": "Model settings\n\nfrom transformers import BertForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = n_labels, # The number of output labels--2 for binary classification.  \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.to(device)\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=8, bias=True)\n)\n\n\n\ndef tune_bert(model, train_dataset, val_dataset,\n            batch_size=32, epochs=10, lr=2e-5, eps=1e-8):\n    model = model.to(device)\n    train_loader = DataLoader(train_dataset,\n                            batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset,\n                            batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset,\n                            batch_size=batch_size, shuffle=False)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=eps)\n    \n    history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n    \n    for epoch in range(epochs):\n        train_loss, train_acc = 0, 0\n        val_loss, val_acc = 0, 0\n        model.train()\n        for (input_ids, attention_masks), labels in train_loader:\n            input_ids = input_ids.to(device)\n            attention_masks = attention_masks.to(device)\n            labels = labels.to(device)\n            outputs = model(input_ids, \n                            token_type_ids=None, \n                            attention_mask=attention_masks, \n                            labels=labels)\n            pred = torch.argmax(outputs.logits, dim=1)\n            loss = outputs.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n            train_loss += loss.item()\n            train_acc += (pred == labels).sum().item()\n        \n        model.eval()\n        with torch.no_grad():\n            for (input_ids, attention_masks), labels in val_loader:\n                input_ids = input_ids.to(device)\n                attention_masks = attention_masks.to(device)\n                labels = labels.to(device)\n                outputs = model(input_ids, \n                                token_type_ids=None, \n                                attention_mask=attention_masks, \n                                labels=labels)\n                pred = torch.argmax(outputs.logits, dim=1)\n                loss = outputs.loss\n                val_loss += loss.item()\n                val_acc += (pred == labels).sum().item()\n        \n        train_loss /= len(train_dataset)\n        train_acc /= len(train_dataset)\n        val_loss /= len(val_dataset)\n        val_acc /= len(val_dataset)\n        history['train_acc'].append(train_acc)\n        history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc)\n        history['val_loss'].append(val_loss)\n        print(f'Epoch {epoch+1}/{epochs}: train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n\n\ndef eval_model(model, test_dataset, batch_size=32):\n    model = model.to(device)\n    test_loader = DataLoader(test_dataset, \n                            batch_size=batch_size, shuffle=False)\n    model.eval()\n    true_labels = np.array([])\n    predictions = np.array([])\n    with torch.no_grad():\n        for (input_ids, attention_masks), labels in test_loader:\n                input_ids = input_ids.to(device)\n                attention_masks = attention_masks.to(device)\n                labels = labels.to(device)\n                outputs = model(input_ids, \n                                token_type_ids=None, \n                                attention_mask=attention_masks, \n                                labels=labels)\n                pred = torch.argmax(outputs.logits, dim=1)\n                true_labels = np.concatenate((true_labels, labels.cpu().numpy()))\n                predictions = np.concatenate((predictions, pred.cpu().numpy()))\n    print('Test Accuracy: ', (true_labels == predictions).sum() / len(true_labels))\n    return true_labels, predictions\n    \n\n\ntune_bert(model, train_dataset, val_dataset,\n        batch_size=32, epochs=3, lr=2e-5, eps=1e-8)\n\nEpoch 1/3: train_loss: 0.0486, train_acc: 0.4491, val_loss: 0.0377, val_acc: 0.6282\nEpoch 2/3: train_loss: 0.0363, train_acc: 0.5938, val_loss: 0.0349, val_acc: 0.6438\nEpoch 3/3: train_loss: 0.0309, train_acc: 0.6610, val_loss: 0.0357, val_acc: 0.6269\n\n\n\ntrue, pred = eval_model(model, test_dataset, batch_size=32)\n\nTest Accuracy:  0.5854922279792746\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(true, pred, target_names=categories))\n\n              precision    recall  f1-score   support\n\n          PR       0.57      0.44      0.50       215\n          RM       0.66      0.54      0.59       207\n          CP       0.67      0.42      0.52       217\n          MF       0.44      0.43      0.43       257\n          TR       0.62      0.63      0.63       184\n          ST       0.62      0.70      0.65       371\n          GN       0.56      0.78      0.65       300\n          PM       0.61      0.64      0.62       179\n\n    accuracy                           0.59      1930\n   macro avg       0.59      0.57      0.57      1930\nweighted avg       0.59      0.59      0.58      1930"
  },
  {
    "objectID": "bert_ml.html",
    "href": "bert_ml.html",
    "title": "Analysis of Arxiv Papers",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport ast\nimport matplotlib.pyplot as plt\n\n\nfrom nltk.cluster import KMeansClusterer, cosine_distance, euclidean_distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nfrom sklearn import mixture\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\nimport os\nfrom google.colab import drive\n\ndrive.mount('/content/drive') # mount the drive\n\nMounted at /content/drive\n\n\n\n#data_path = '/content/drive/My Drive/Final_proj/'\ndata_path = '/content/drive/My Drive/BIA667/Final Project/'\ndf = pd.read_csv(data_path+\"bert_embeddings.csv\")\nembeddings_df = df.drop(columns=['Unnamed: 0'])\nembeddings_df.head()\n\n\n  \n    \n      \n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\n...\nx758\nx759\nx760\nx761\nx762\nx763\nx764\nx765\nx766\nx767\n\n\n\n\n0\nhttp://arxiv.org/abs/2304.13610v1\n2023-04-26 15:05:19+00:00\n2023-04-26 15:05:19+00:00\nMaximum Implied Variance Slope -- Practical As...\n[\"Fabien Le Floc'h\", 'Winfried Koller']\nMaximum Implied Variance Slope -- Practical As...\nNaN\nhttp://arxiv.org/abs/2304.13610v1\nNaN\nhttp://arxiv.org/pdf/2304.13610v1\n...\n-0.006476\n-0.232883\n-0.193446\n0.046816\n-0.278641\n-0.420216\n-0.197272\n-0.048829\n-0.082687\n0.439996\n\n\n1\nhttp://arxiv.org/abs/2206.02582v2\n2023-04-26 13:13:28+00:00\n2022-06-06 12:43:06+00:00\nMaking heads or tails of systemic risk measures\n['Aleksy Leeuwenkamp']\nMaking heads or tails of systemic risk measure...\nRevised version of the $\\Delta$-CoES paper, no...\nhttp://arxiv.org/abs/2206.02582v2\nNaN\nhttp://arxiv.org/pdf/2206.02582v2\n...\n-0.149508\n-0.160621\n-0.250387\n-0.054146\n-0.316383\n-0.395322\n-0.337699\n-0.328789\n0.033148\n0.372287\n\n\n2\nhttp://arxiv.org/abs/2301.00790v2\n2023-04-26 10:56:51+00:00\n2022-12-30 17:19:00+00:00\nDynamic Feature Engineering and model selectio...\n['Thomas Wong', 'Mauricio Barahona']\nDynamic Feature Engineering and model selectio...\nNaN\nhttp://arxiv.org/abs/2301.00790v2\nNaN\nhttp://arxiv.org/pdf/2301.00790v2\n...\n-0.213331\n-0.276196\n-0.192677\n-0.023698\n-0.290676\n-0.251346\n-0.158298\n-0.411816\n-0.089801\n0.378311\n\n\n3\nhttp://arxiv.org/abs/2304.13402v1\n2023-04-26 09:28:41+00:00\n2023-04-26 09:28:41+00:00\nConvexity adjustments à la Malliavin\n['David García-Lorite', 'Raul Merino']\nConvexity adjustments à la Malliavin In this ...\nNaN\nhttp://arxiv.org/abs/2304.13402v1\nNaN\nhttp://arxiv.org/pdf/2304.13402v1\n...\n-0.001260\n-0.162045\n-0.476206\n-0.060368\n-0.249610\n-0.298294\n-0.164128\n-0.035170\n0.047379\n0.358566\n\n\n4\nhttp://arxiv.org/abs/2304.13128v1\n2023-04-25 20:16:36+00:00\n2023-04-25 20:16:36+00:00\nLearning Volatility Surfaces using Generative ...\n['Andrew Na', 'Meixin Zhang', 'Justin Wan']\nLearning Volatility Surfaces using Generative ...\nThis is a working draft\nhttp://arxiv.org/abs/2304.13128v1\nNaN\nhttp://arxiv.org/pdf/2304.13128v1\n...\n-0.293317\n-0.257803\n-0.157204\n-0.021314\n-0.284447\n-0.150996\n-0.072684\n-0.162023\n0.086110\n0.400750\n\n\n\n\n\n5 rows × 795 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nBERT\nIn Deep Learning Module, we applied the bert tokenizer and bert model and got a tensor with shape (9674, 200, 768). We took the mean value along its second dimension and retained a vector with size 768 for each document. We used this vector as the features and applied unsupervised method as cluster and supverised machine learning models like Random Forest.\nBERT Clustering\n\nfeatures = []\nfor i in range(768):\n  features.append('x'+str(i))\n\n\ndef cluster_bert(train_data, test_data, num_clusters, method = 'k-mean'):\n    \n    train_temp = train_data.copy() \n    test_temp = test_data.copy()\n    \n    #train_temp[\"tokenized_summary\"] = train_data[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n    #test_temp[\"tokenized_summary\"] = test_data[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n    \n    X_train = train_temp[features].values\n    X_test = test_temp[features].values\n    \n   # add your code\n    if method == 'k-mean':\n        clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=30)\n        \n        #tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)     \n        \n        #dtm = tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n\n        #X_train = train_data[features]\n        #X_test = test_data[features]\n    \n        clusters = clusterer.cluster(X_train, assign_clusters=True)\n    \n        #test_dtm = tfidf_vect.transform(test_temp[\"summary\"])\n\n        predicted = [clusterer.classify(v) for v in X_test]\n        \n    \n    \n    elif method == 'gmm':\n        gmm = mixture.GaussianMixture(n_components=num_clusters,\n                                      covariance_type='diag', random_state=42)\n        \n        #tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    \n        #dtm= tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        gmm.fit(X_train)\n    \n        #test_dtm = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        predicted = gmm.predict(X_test)\n\n    \n    \n    confusion_df = pd.DataFrame(list(zip(test_temp[\"subcategory\"].values, predicted)),columns = [\"label\", \"cluster\"])\n    \n    print (pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label))\n    \n    #Apply majority vote rule to dynamically map each cluster to a ground-truth label in test_data\n    label_allocation = pd.DataFrame(confusion_df.groupby('cluster')['label'].apply\\\n                                    (lambda x: x.value_counts().idxmax()))\n    \n    \n    for i in range(num_clusters):\n        print ('Cluster {} -&gt; Topic {}'.format(i,label_allocation['label'][i]))\n\n    #classification report\n    cluster_dict = {}\n    for i in range(num_clusters):\n        cluster_dict[i] = label_allocation['label'][i]\n    \n    predicted_target=[cluster_dict[i] for i in predicted]\n    \n    print(metrics.classification_report(test_temp[\"subcategory\"], predicted_target))\n     \n    return None\n\n\ntrain_df, test_df= train_test_split(embeddings_df, test_size=0.2, random_state=42)\n\n\ncluster_bert(train_df, test_df, num_clusters = 8, method = 'k-mean')\n\nlabel    CP  GN  MF  PM  PR  RM  ST  TR\ncluster                                \n0        21  34  30  20  22  25  26  16\n1        35  13  61  37  39  21  20  12\n2        13  48  10  15   9  32  45  31\n3        41   7  59  17  51  12  39   3\n4        57  16  35  34  35  32  76  24\n5        16  26  54  39  38  31  18  35\n6        12  64   3   5   5  19  90  28\n7        22  92   5  12  16  35  57  35\nCluster 0 -&gt; Topic GN\nCluster 1 -&gt; Topic MF\nCluster 2 -&gt; Topic GN\nCluster 3 -&gt; Topic MF\nCluster 4 -&gt; Topic ST\nCluster 5 -&gt; Topic MF\nCluster 6 -&gt; Topic ST\nCluster 7 -&gt; Topic GN\n              precision    recall  f1-score   support\n\n          CP       0.00      0.00      0.00       217\n          GN       0.26      0.58      0.36       300\n          MF       0.24      0.68      0.35       257\n          PM       0.00      0.00      0.00       179\n          PR       0.00      0.00      0.00       215\n          RM       0.00      0.00      0.00       207\n          ST       0.31      0.45      0.37       371\n          TR       0.00      0.00      0.00       184\n\n    accuracy                           0.27      1930\n   macro avg       0.10      0.21      0.13      1930\nweighted avg       0.13      0.27      0.17      1930\n\n\n\nBERT+Random Forest\n\ncategories = np.unique(embeddings_df['subcategory']).tolist()\n\n\ntrain_df, test_df= train_test_split(embeddings_df, test_size=0.2, random_state=42)\nX_train = train_df[features].values\ny_train = train_df[categories]\n\nX_test = test_df[features].values\ny_test = test_df[categories].values\n\n\nmodel = OneVsRestClassifier(RandomForestClassifier());\nmodel.fit(X_train, y_train)\n#y_pred = model.predict(X_test)\npredict_p = model.predict_proba(X_test)\n\ny_pred = np.zeros_like(predict_p)\ny_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\n#y_pred\n\nprint(metrics.classification_report(y_test, y_pred,target_names = categories))\n\nOneVsRestClassifier(estimator=RandomForestClassifier())In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneVsRestClassifierOneVsRestClassifier(estimator=RandomForestClassifier())estimator: RandomForestClassifierRandomForestClassifier()RandomForestClassifierRandomForestClassifier()\n\n\n              precision    recall  f1-score   support\n\n          CP       0.50      0.34      0.41       217\n          GN       0.57      0.69      0.63       300\n          MF       0.39      0.49      0.44       257\n          PM       0.55      0.44      0.48       179\n          PR       0.48      0.44      0.46       215\n          RM       0.58      0.43      0.49       207\n          ST       0.52      0.68      0.59       371\n          TR       0.60      0.42      0.50       184\n\n   micro avg       0.52      0.52      0.52      1930\n   macro avg       0.53      0.49      0.50      1930\nweighted avg       0.52      0.52      0.51      1930\n samples avg       0.52      0.52      0.52      1930\n\n\n\n\n#prc curve\nprecision = dict()\nrecall = dict()\nfor i in range(len(categories)):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        predict_p[:, i])\n    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(categories[i]))\n    \nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend(loc=\"lower left\")\nplt.show();\n\n\n\n\n\n# roc curve\nfpr = dict()\ntpr = dict()\n\nfor i in range(len(categories)):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i],predict_p[:, i])\n    plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(categories[i]))\n\nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\nplt.legend(loc=\"best\")\nplt.title(\"ROC curve\")\nplt.show();\n\n\n\n\n\nlabel_dict = dict()\nfor i in range(8):\n    label_dict[i] = categories[i]\n\ny_pred = np.argmax(y_pred, axis=1)\ny_test = np.argmax(y_test, axis=1)\n\ny_true_labels = [label_dict[num] for num in y_test]\ny_pred_labels = [label_dict[num] for num in y_pred]\n\nConfusionMatrixDisplay.from_predictions(y_true_labels, y_pred_labels,cmap=plt.cm.Blues,values_format='g');"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning Models on Classification",
    "section": "",
    "text": "Three deep learning models were used to do the classification, including the BERT+NN, BERT+CNN and Word2Vec +CNN. For BERT+NN and BERT+CNN, the pre-trained BERT model was used as the encoder, followed by a neural network or convolutional neural network for classification. For Word2Vec+CNN, the Word2Vec model was used to create word embeddings for the abstracts, which were then used as input to a convolutional neural network for classification. The models were trained on a training set of 60% of the abstracts, validated on a validation set of 20% of the dataset and tested on a test set of 20% of the abstracts. The accuracy, precision, recall, and F1 score were used as evaluation metrics. Early stopping was also applied to fight overfitting in this part."
  },
  {
    "objectID": "dl.html#data-preparation",
    "href": "dl.html#data-preparation",
    "title": "Deep Learning Models on Classification",
    "section": "Data Preparation",
    "text": "Data Preparation\nPackages used in this part include:\n\n\nCode\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport os\nfrom google.colab import drive\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import metrics\nfrom gensim.models import Word2Vec\nimport ast\n\n\n\nTokenization\nAutoTokenizer.from_pretrained function was used to load the pre-trained tokenizer for the BERT model. The ‘AutoModel.from_pretrained’ method allows us to download and use pre-trained models and tokenizers available in the Hugging Face Transformers library. The embedding output from the BERT model was used as input to the networks in the following.\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load BERT model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n# Load the BERT tokenizer.\nbert_model = AutoModel.from_pretrained(\n    \"bert-base-uncased\",\n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = True, # Whether the model returns all hidden-states.\n        \n    )\nbert_model.cuda();\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\nProcessing the data\n\ndata_path = '/content/drive/My Drive/Final_proj/'\n#data_path = '/content/drive/My Drive/'\ndf = pd.read_csv(data_path+\"qfin_processed_data.csv\")\ndf = df.drop(columns=['Unnamed: 0'])\ncategories = np.unique(df['subcategory']).tolist()\ndf[categories] = pd.get_dummies(df['subcategory'])\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\n...\ncombined_Bert\ntokenized_summary\nCP\nGN\nMF\nPM\nPR\nRM\nST\nTR\n\n\n\n\n0\nhttp://arxiv.org/abs/2304.13610v1\n2023-04-26 15:05:19+00:00\n2023-04-26 15:05:19+00:00\nMaximum Implied Variance Slope -- Practical As...\n[\"Fabien Le Floc'h\", 'Winfried Koller']\nMaximum Implied Variance Slope -- Practical As...\nNaN\nhttp://arxiv.org/abs/2304.13610v1\nNaN\nhttp://arxiv.org/pdf/2304.13610v1\n...\nTitle: Maximum Implied Variance Slope -- Pract...\n['maximum', 'implied', 'variance', 'slope', 'p...\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\nhttp://arxiv.org/abs/2206.02582v2\n2023-04-26 13:13:28+00:00\n2022-06-06 12:43:06+00:00\nMaking heads or tails of systemic risk measures\n['Aleksy Leeuwenkamp']\nMaking heads or tails of systemic risk measure...\nRevised version of the $\\Delta$-CoES paper, no...\nhttp://arxiv.org/abs/2206.02582v2\nNaN\nhttp://arxiv.org/pdf/2206.02582v2\n...\nTitle: Making heads or tails of systemic risk ...\n['making', 'head', 'tail', 'systemic', 'risk',...\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\nhttp://arxiv.org/abs/2301.00790v2\n2023-04-26 10:56:51+00:00\n2022-12-30 17:19:00+00:00\nDynamic Feature Engineering and model selectio...\n['Thomas Wong', 'Mauricio Barahona']\nDynamic Feature Engineering and model selectio...\nNaN\nhttp://arxiv.org/abs/2301.00790v2\nNaN\nhttp://arxiv.org/pdf/2301.00790v2\n...\nTitle: Dynamic Feature Engineering and model s...\n['dynamic', 'feature', 'engineering', 'model',...\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nhttp://arxiv.org/abs/2304.13402v1\n2023-04-26 09:28:41+00:00\n2023-04-26 09:28:41+00:00\nConvexity adjustments à la Malliavin\n['David García-Lorite', 'Raul Merino']\nConvexity adjustments à la Malliavin In this ...\nNaN\nhttp://arxiv.org/abs/2304.13402v1\nNaN\nhttp://arxiv.org/pdf/2304.13402v1\n...\nTitle: Convexity adjustments à la Malliavin; C...\n['convexity', 'adjustment', 'la', 'malliavin',...\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n4\nhttp://arxiv.org/abs/2304.13128v1\n2023-04-25 20:16:36+00:00\n2023-04-25 20:16:36+00:00\nLearning Volatility Surfaces using Generative ...\n['Andrew Na', 'Meixin Zhang', 'Justin Wan']\nLearning Volatility Surfaces using Generative ...\nThis is a working draft\nhttp://arxiv.org/abs/2304.13128v1\nNaN\nhttp://arxiv.org/pdf/2304.13128v1\n...\nTitle: Learning Volatility Surfaces using Gene...\n['learning', 'volatility', 'surface', 'using',...\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n5 rows × 26 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\nLength of each document\nWe do a hist plot on the length of overall documents. We can see that the length of documents is mostly around 150 words, thus we select 200 words as the maximum length of each document.\n\n\nCode\ndf['len_summary'] = df['combined_Bert'].apply(lambda x: len(x.split()))\ndf['len_summary'].hist()\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "dl.html#deep-learning-models",
    "href": "dl.html#deep-learning-models",
    "title": "Deep Learning Models on Classification",
    "section": "Deep Learning Models",
    "text": "Deep Learning Models\n\nHelper function of tokenization\n\ndef bert_tokenize(sentences,tokenizer):\n    input_ids = []\n    attention_masks = []\n\n    # Tokenize each sentence\n    for sent in sentences:\n   \n        encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 200,           # Pad & truncate all sentences.\n                        truncation=True,\n                        padding = 'max_length',\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    \n        # Add the encoded sentence to the list.    \n        input_ids.append(encoded_dict['input_ids'])\n    \n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    return  BertDataset(input_ids, attention_masks)\n\n\n\nEmbedding layer\n\ndef bert_embedding(Bert_Dataset, bert_model):\n    text_loader = DataLoader(Bert_Dataset, batch_size=64, shuffle=False)\n    total_embeddings = []\n    bert_model.eval()\n    with torch.no_grad():\n        for input_ids, attention_masks in text_loader:\n            outputs = bert_model(input_ids.to(device), attention_masks.to(device))   \n            hidden_states = outputs[2]\n            token_embeddings = torch.stack(hidden_states[-4:], dim=0) \n\n            # permute axis\n            token_embeddings = token_embeddings.permute(1,2,0,3)\n\n        # take the mean of the last 4 layers\n            token_embeddings = token_embeddings.mean(axis=2)\n            total_embeddings.append(token_embeddings.cpu().numpy())\n            \n    return np.concatenate(total_embeddings)\n\n\nbert_dataset = bert_tokenize(df['combined_Bert'],tokenizer)\ntotal_embeddings = bert_embedding(bert_dataset, bert_model)\nprint(total_embeddings.shape)\n\n(9649, 200, 768)\n\n\nFor freezing BERT embedding models, we don’t need to pass the embedding layer to the model. For simplicity, we can save them and load them directly.\n\nbool = False\nif bool:\n  features = []\n  for i in range(768):\n      features.append('x'+str(i))\n\n  CLS_df = pd.DataFrame(total_embeddings.mean(axis = 1))\n\n  CLS_df.columns = features\n\n  embeddings_df = pd.concat([df,CLS_df],axis = 1)\n  embeddings_df.to_csv(data_path+'bert_embeddings.csv')\n\n\n\nTrain and evaluate function\ntrain:\n\n\nCode\ndef train_model(model, train_dataset, eval_dataset, test_dataset, device, label_encoder,\n                lr=0.0002, epochs=30, batch_size=256, patience = 5, model_path = 'model.pt'):\n    \n    #add your code here\n    test_acc, history = None, None\n    \n    # add your code\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n    \n    \n    # move model to device\n    model = model.to(device)\n\n    # history\n    history = {'train_loss': [],\n               'train_acc': [],\n               'val_loss': [],\n               'val_acc': []}\n    \n    criterion = nn.NLLLoss()\n    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n    \n    best_loss = float('inf') \n    counter = 0\n    \n    \n    print('Training Start')\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n        val_loss = 0\n        val_acc = 0\n        # training loop\n        for x, y in train_loader:\n            # move data to device\n            x = x.to(device)\n            y = y.to(device)\n            # forward\n            outputs = model(x)\n            pred = torch.argmax(outputs, dim = 1)\n            cur_train_loss = criterion(outputs, y)\n            cur_train_acc = (pred == y).float().mean().item()\n            # backward\n            cur_train_loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            # loss and acc\n            train_loss += cur_train_loss\n            train_acc += cur_train_acc\n        \n    \n        # validation start\n        model.eval()\n        with torch.no_grad():\n             for x, y in val_loader:\n                x = x.to(device)\n                y = y.to(device)\n                # predict\n                outputs = model(x)\n                pred = torch.argmax(outputs, dim = 1)\n                cur_val_loss = criterion(outputs, y)\n                cur_val_acc = (pred == y).float().mean().item() \n                # loss and acc\n                val_loss += cur_val_loss\n                val_acc += cur_val_acc\n\n\n        if val_loss &lt; best_loss:\n            best_loss = val_loss\n            counter = 0\n            torch.save(model, model_path)\n            print(\"Model Saved!\")\n            \n            \n        else:\n            counter += 1\n            \n        if counter &gt;= patience:\n            print(\"Early stopping at epoch: \", epoch)\n            model = torch.load(model_path)\n            break\n    \n    \n        # epoch output\n        train_loss = (train_loss/len(train_loader)).item()\n        train_acc = train_acc/len(train_loader)\n        val_loss = (val_loss/len(val_loader)).item()\n        val_acc = val_acc/len(val_loader)\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        if epoch%10 ==0 or epoch ==epochs-1 or counter ==0:\n            print(f\"Epoch:{epoch + 1} / {epochs}, train loss:{train_loss:.3f} train_acc:{train_acc:.3f}, validation loss:{val_loss:.3f} validation acc:{val_acc:.3f}\")\n\n    return model\n\n\nevaluate:\n\n\nCode\ndef evaluate(model, test_dataset, device, label_encoder, batch_size=256):\n    model = model.to(device)\n    model.eval()\n    test_loader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False)\n    y_true_test_np, y_pred_test_np = [], []\n    test_acc = 0\n\n    with torch.no_grad():\n        for x, y in test_loader:\n            x = x.to(device)\n            y = y.to(device)\n                # predict\n            outputs = model(x)\n            pred = torch.argmax(outputs, dim = 1)\n            cur_test_acc = (pred == y).float().mean().item()\n            test_acc += cur_test_acc\n            y_pred_test_np.extend(pred.cpu().detach().numpy())\n            y_true_test_np.extend(y.cpu().detach().numpy())\n    \n    y_pred_label = encoder.inverse_transform(y_pred_test_np)\n    y_true_label = encoder.inverse_transform(y_true_test_np)\n    \n    print(metrics.classification_report(y_true_label, y_pred_label))\n    test_acc = (test_acc/len(test_loader))\n    print (f'acc on test subset: {test_acc:.3f}')\n    return None\n\n\n\n\nTraining preparation\n\nclass BertClassificationDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = torch.Tensor(embeddings).float()\n        self.labels = torch.Tensor(labels).long()\n    \n    def __getitem__(self, index):\n        return self.embeddings[index], self.labels[index]\n    \n    def __len__(self):\n        return self.embeddings.size()[0]\n\n\n# Create an instance of the LabelEncoder class\nencoder = LabelEncoder()\n\n# Encode the data\nencoded_label = encoder.fit_transform(df['subcategory'])\n\nNN_dataset = BertClassificationDataset(total_embeddings.mean(axis=1),encoded_label)\n\n\ndataset_size = len(NN_dataset)\n\ntrain_size = int(0.6 * dataset_size)\nval_size = int(0.2 * dataset_size)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(NN_dataset, [train_size, val_size, test_size])\n\n\n\nBERT+NN model\nThe Neural Network(NN) architecture consists of three linear layers, each followed by a ReLU activation function and a dropout layer to prevent overfitting. The input dimension of the first linear layer is set to 768, which is the output dimension of the BERT model used as the feature extractor in this particular example. The output dimension of the final linear layer is equal to the number of output classes in the classification task, which is 8 in this project. And then, we trained this neural network with learning rate being 0.0005, epochs being 100 and batch_size being 256. We tried different combinations of these hyperparameters, and this group of values returned the highest accuracy as 0.537 of the classification.\n\nclass Classifier(nn.Module):\n    def __init__(self, hidden_units, output_dim):\n        # initialize parent class\n        super(Classifier, self).__init__()\n        self.hidden_units = hidden_units\n        self.output_dim = output_dim\n        # define layers\n        self.classifier = nn.Sequential(  # define a subcomponent of neural network or another to define model\n            nn.Linear(in_features=768, out_features=self.hidden_units),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=hidden_units, out_features=self.hidden_units),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(in_features=hidden_units, out_features=self.output_dim),\n            #nn.LogSoftmax(dim=1)\n        )\n    \n    def forward(self, x):\n        x =  self.classifier(x)\n        x = F.log_softmax(x,dim = -1)\n        return x\n\n\nmodel = Classifier(hidden_units=64, output_dim = 8)\nmodel_path = data_path + 'Bert_NN.pt'\ntrained_model = train_model(model, train_dataset, val_dataset, test_dataset, \n                            device, encoder,lr=0.0005, epochs = 100, batch_size=256, \n                            patience = 5, model_path = model_path)\n\nTraining Start\nModel Saved!\nEpoch:1 / 100, train loss:2.022 train_acc:0.200, validation loss:1.926 validation acc:0.309\nModel Saved!\nEpoch:2 / 100, train loss:1.909 train_acc:0.271, validation loss:1.823 validation acc:0.323\nModel Saved!\nEpoch:3 / 100, train loss:1.840 train_acc:0.299, validation loss:1.762 validation acc:0.336\nModel Saved!\nEpoch:4 / 100, train loss:1.802 train_acc:0.308, validation loss:1.719 validation acc:0.341\nModel Saved!\nEpoch:5 / 100, train loss:1.760 train_acc:0.340, validation loss:1.706 validation acc:0.340\nModel Saved!\nEpoch:6 / 100, train loss:1.738 train_acc:0.341, validation loss:1.660 validation acc:0.404\nModel Saved!\nEpoch:7 / 100, train loss:1.717 train_acc:0.348, validation loss:1.625 validation acc:0.406\nModel Saved!\nEpoch:8 / 100, train loss:1.692 train_acc:0.358, validation loss:1.608 validation acc:0.399\nModel Saved!\nEpoch:9 / 100, train loss:1.669 train_acc:0.373, validation loss:1.591 validation acc:0.417\nModel Saved!\nEpoch:10 / 100, train loss:1.648 train_acc:0.390, validation loss:1.564 validation acc:0.414\nModel Saved!\nEpoch:11 / 100, train loss:1.634 train_acc:0.398, validation loss:1.547 validation acc:0.453\nModel Saved!\nEpoch:12 / 100, train loss:1.622 train_acc:0.409, validation loss:1.528 validation acc:0.463\nModel Saved!\nEpoch:13 / 100, train loss:1.603 train_acc:0.397, validation loss:1.521 validation acc:0.452\nModel Saved!\nEpoch:14 / 100, train loss:1.598 train_acc:0.404, validation loss:1.495 validation acc:0.470\nModel Saved!\nEpoch:15 / 100, train loss:1.577 train_acc:0.413, validation loss:1.485 validation acc:0.470\nModel Saved!\nEpoch:16 / 100, train loss:1.571 train_acc:0.423, validation loss:1.477 validation acc:0.481\nModel Saved!\nEpoch:17 / 100, train loss:1.558 train_acc:0.424, validation loss:1.460 validation acc:0.472\nModel Saved!\nEpoch:18 / 100, train loss:1.550 train_acc:0.439, validation loss:1.458 validation acc:0.473\nModel Saved!\nEpoch:19 / 100, train loss:1.540 train_acc:0.430, validation loss:1.438 validation acc:0.470\nModel Saved!\nEpoch:20 / 100, train loss:1.512 train_acc:0.443, validation loss:1.427 validation acc:0.485\nEpoch:21 / 100, train loss:1.520 train_acc:0.441, validation loss:1.433 validation acc:0.501\nModel Saved!\nEpoch:23 / 100, train loss:1.500 train_acc:0.446, validation loss:1.407 validation acc:0.505\nModel Saved!\nEpoch:26 / 100, train loss:1.490 train_acc:0.462, validation loss:1.377 validation acc:0.505\nModel Saved!\nEpoch:28 / 100, train loss:1.461 train_acc:0.468, validation loss:1.374 validation acc:0.516\nModel Saved!\nEpoch:29 / 100, train loss:1.465 train_acc:0.468, validation loss:1.366 validation acc:0.518\nModel Saved!\nEpoch:31 / 100, train loss:1.443 train_acc:0.484, validation loss:1.359 validation acc:0.520\nModel Saved!\nEpoch:32 / 100, train loss:1.440 train_acc:0.484, validation loss:1.344 validation acc:0.533\nModel Saved!\nEpoch:33 / 100, train loss:1.431 train_acc:0.485, validation loss:1.344 validation acc:0.533\nModel Saved!\nEpoch:34 / 100, train loss:1.432 train_acc:0.486, validation loss:1.333 validation acc:0.522\nModel Saved!\nEpoch:36 / 100, train loss:1.430 train_acc:0.483, validation loss:1.328 validation acc:0.536\nModel Saved!\nEpoch:39 / 100, train loss:1.408 train_acc:0.490, validation loss:1.328 validation acc:0.531\nModel Saved!\nEpoch:40 / 100, train loss:1.410 train_acc:0.499, validation loss:1.319 validation acc:0.533\nModel Saved!\nEpoch:41 / 100, train loss:1.399 train_acc:0.505, validation loss:1.309 validation acc:0.544\nModel Saved!\nEpoch:43 / 100, train loss:1.396 train_acc:0.509, validation loss:1.301 validation acc:0.550\nModel Saved!\nEpoch:46 / 100, train loss:1.367 train_acc:0.514, validation loss:1.291 validation acc:0.548\nModel Saved!\nEpoch:47 / 100, train loss:1.366 train_acc:0.518, validation loss:1.286 validation acc:0.549\nModel Saved!\nEpoch:49 / 100, train loss:1.357 train_acc:0.521, validation loss:1.282 validation acc:0.548\nEpoch:51 / 100, train loss:1.368 train_acc:0.510, validation loss:1.301 validation acc:0.553\nModel Saved!\nEpoch:53 / 100, train loss:1.363 train_acc:0.510, validation loss:1.277 validation acc:0.551\nModel Saved!\nEpoch:56 / 100, train loss:1.345 train_acc:0.528, validation loss:1.271 validation acc:0.547\nModel Saved!\nEpoch:59 / 100, train loss:1.333 train_acc:0.537, validation loss:1.261 validation acc:0.555\nEpoch:61 / 100, train loss:1.326 train_acc:0.532, validation loss:1.264 validation acc:0.545\nModel Saved!\nEpoch:62 / 100, train loss:1.326 train_acc:0.532, validation loss:1.259 validation acc:0.553\nModel Saved!\nEpoch:63 / 100, train loss:1.320 train_acc:0.533, validation loss:1.256 validation acc:0.560\nModel Saved!\nEpoch:68 / 100, train loss:1.309 train_acc:0.537, validation loss:1.250 validation acc:0.550\nModel Saved!\nEpoch:70 / 100, train loss:1.303 train_acc:0.538, validation loss:1.249 validation acc:0.555\nEpoch:71 / 100, train loss:1.307 train_acc:0.538, validation loss:1.249 validation acc:0.558\nModel Saved!\nEpoch:72 / 100, train loss:1.297 train_acc:0.544, validation loss:1.245 validation acc:0.555\nModel Saved!\nEpoch:76 / 100, train loss:1.294 train_acc:0.539, validation loss:1.244 validation acc:0.560\nModel Saved!\nEpoch:77 / 100, train loss:1.304 train_acc:0.540, validation loss:1.238 validation acc:0.569\nEpoch:81 / 100, train loss:1.275 train_acc:0.545, validation loss:1.262 validation acc:0.557\nEarly stopping at epoch:  81\n\n\n\n\nCode\nevaluate(trained_model, test_dataset, device, encoder, batch_size=256)\n\n\n              precision    recall  f1-score   support\n\n          CP       0.61      0.37      0.46       218\n          GN       0.62      0.63      0.63       288\n          MF       0.34      0.21      0.26       269\n          PM       0.50      0.63      0.56       187\n          PR       0.48      0.60      0.54       207\n          RM       0.54      0.57      0.56       225\n          ST       0.58      0.70      0.64       349\n          TR       0.54      0.55      0.55       188\n\n    accuracy                           0.54      1931\n   macro avg       0.53      0.53      0.52      1931\nweighted avg       0.53      0.54      0.53      1931\n\nacc on test subset: 0.537\n\n\n\n\nBERT+CNN\nThe Convolutional Neural Network architecture is composed of three 1D convolutional neural networks that operate on unigrams, bigrams, and trigrams, respectively. Each of these networks applies a convolutional layer, a ReLU activation function, a max-pooling layer, and a flatten operation. The resulting feature maps are concatenated and fed into a simple classifier consisting of a dropout layer and a linear layer. Similarly, we trained the BERT_CNN model with many different values of parameters such as kernel_size, learning rate and batch size, etc and finally chose the group, with lr = 0.0002, epochs = 200 and bitch_size = 128, that gave the highest classification accuracy as 0.59.\n\nclass BertClassificationDataset(Dataset):\n    def __init__(self, embeddings, labels):\n        self.embeddings = torch.Tensor(embeddings).float()\n        self.labels = torch.Tensor(labels).long()\n    \n    def __getitem__(self, index):\n        return self.embeddings[index], self.labels[index]\n    \n    def __len__(self):\n        return self.embeddings.size()[0]\n\n\nCNN_dataset = BertClassificationDataset(total_embeddings,encoded_label)\n\n\nclass Bert_CNN(nn.Module):\n    def __init__(self, dropout_ratio, DOC_LEN, output_dim):\n        super(Bert_CNN, self).__init__()\n        self.dropout_ratio = dropout_ratio\n        self.DOC_LEN = DOC_LEN\n        self.output_dim = output_dim\n        \n        # 1D CNN\n        # unigram\n        self.unigram = nn.Sequential(\n            nn.Conv1d(in_channels=768, out_channels=64, kernel_size=1),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=self.DOC_LEN),\n            nn.Flatten()\n        )\n        # bigram\n        self.bigram = nn.Sequential(\n            nn.Conv1d(in_channels=768, out_channels=64, kernel_size=4),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=self.DOC_LEN - 4 + 1),\n            nn.Flatten()\n        )\n        # trigram\n        self.trigram = nn.Sequential(\n            nn.Conv1d(in_channels=768, out_channels=64, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=self.DOC_LEN - 3 + 1),\n            nn.Flatten()\n        )\n        # simple classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(self.dropout_ratio),\n            nn.Linear(in_features=64*3, out_features= self.output_dim)\n        )\n\n    def forward(self, x):\n        # make sure we are convolving on each word\n        x = torch.transpose(x, dim0=1, dim1=2)  # (-1, DOC_LEN, embedding_dim): embedding on 1(DOC_LEN) & 2(embedding_dim) dims\n        # 1d cnn output\n        uni_gram_output = self.unigram(x)\n        bi_gram_output = self.bigram(x)\n        tri_gram_output = self.trigram(x)\n        # concatenate\n        x = torch.cat((uni_gram_output, bi_gram_output, tri_gram_output), dim=1)\n        # classifier\n        x = self.classifier(x)\n        x = F.log_softmax(x,dim = -1)\n\n        return x\n\n\ndataset_size = len(CNN_dataset)\n\ntrain_size = int(0.6 * dataset_size)\nval_size = int(0.2 * dataset_size)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(CNN_dataset, [train_size, val_size, test_size])\n\n\nmodel = Bert_CNN(dropout_ratio=0.2, DOC_LEN=200, output_dim=8)\nmodel_path = data_path + 'Bert_CNN.pt'\ntrained_model = train_model(model, train_dataset, val_dataset, test_dataset, \n                            device, encoder,lr=0.0002, epochs = 200, batch_size=128, \n                            patience = 5, model_path = model_path)\n\nTraining Start\nModel Saved!\nEpoch:1 / 200, train loss:1.735 train_acc:0.384, validation loss:1.474 validation acc:0.510\nModel Saved!\nEpoch:2 / 200, train loss:1.289 train_acc:0.567, validation loss:1.319 validation acc:0.543\nModel Saved!\nEpoch:3 / 200, train loss:1.141 train_acc:0.619, validation loss:1.236 validation acc:0.571\nModel Saved!\nEpoch:4 / 200, train loss:1.064 train_acc:0.641, validation loss:1.210 validation acc:0.571\nModel Saved!\nEpoch:5 / 200, train loss:0.990 train_acc:0.667, validation loss:1.185 validation acc:0.586\nModel Saved!\nEpoch:6 / 200, train loss:0.932 train_acc:0.689, validation loss:1.174 validation acc:0.580\nEarly stopping at epoch:  10\n\n\n\n\nCode\nevaluate(trained_model, test_dataset, device, encoder, batch_size=128)\n\n\n              precision    recall  f1-score   support\n\n          CP       0.51      0.45      0.48       198\n          GN       0.70      0.64      0.67       288\n          MF       0.41      0.27      0.33       253\n          PM       0.65      0.58      0.61       188\n          PR       0.58      0.61      0.59       249\n          RM       0.66      0.68      0.67       230\n          ST       0.56      0.82      0.66       350\n          TR       0.69      0.54      0.61       175\n\n    accuracy                           0.59      1931\n   macro avg       0.59      0.58      0.58      1931\nweighted avg       0.59      0.59      0.58      1931\n\nacc on test subset: 0.590\n\n\n\n\nWord2Vec+CNN\nWith two neural networks with embeddings from the BERT model, we’d like to compare different methods to produce embeddings. Therefore, we then used Word2Vec to transform the dataset into embeddings and took it as input to the same CNN structure to do the classification. After several attempts on the parameters, we finally chose the learning rate of 0.0003, 200 epochs and batch_size of 128 to train the W2V_CNN model and get the classification accuracy of 0.584.\n\nDataset\n\nclass W2V_Dataset(Dataset):\n    def __init__(self, input_ids, labels):\n        self.labels = torch.Tensor(labels).long()\n        self.input_ids = torch.Tensor(input_ids).long()\n    \n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        input_id = self.input_ids[idx]\n        label = self.labels[idx]\n        return input_id, label\n\n\ndf[\"tokenized_summary\"] = df[\"tokenized_summary\"].apply(lambda x: ast.literal_eval(x))\ntext_data = df.tokenized_summary.tolist()\ntext_unique = np.unique(np.array([item for sublist in text_data for item in sublist])).tolist()\nprint(len(text_unique))\n\n19823\n\n\n\n\nTrain word2vec\n\ntext_data = df.tokenized_summary.tolist()\nfrom tqdm import tqdm\nsubwords = []\nencoded_text = []\nfor text in tqdm(text_data):\n    temp_list = []\n    for word in text:\n      temp_list.append(text_unique.index(word))\n    encoded_text.append(temp_list)\n\n# Train a Word2Vec model on the subwords\nW2V_model = Word2Vec(text_data, vector_size=100, window=5, min_count=1, workers=4)\n\n100%|██████████| 9649/9649 [02:04&lt;00:00, 77.74it/s]\n\n\n\n\nPadding\n\npadded_lists = []\nmax_length = 200\nfor inner_list in encoded_text:\n  if len(inner_list)&lt;= max_length:\n    padded_list = inner_list + [len(text_unique)] * (max_length - len(inner_list))\n    padded_lists.append(padded_list)\n  else: \n    padded_lists.append(inner_list[:max_length])\npadded_lists = torch.tensor(padded_lists, dtype=torch.long)\nprint(padded_lists.shape)\n\ntorch.Size([9649, 200])\n\n\n\nW2V_dataset = W2V_Dataset(padded_lists,encoded_label)\nvocab = text_unique\ndoc_embeddings = []\nfor token in vocab:\n  doc_embeddings.append(W2V_model.wv[token])\ndoc_embeddings.append(np.zeros(100))\n\ndoc_embeddings = torch.tensor(doc_embeddings).to(dtype=torch.float)\n\n\n\nSplit dataset\n\ndataset_size = len(W2V_dataset)\n\ntrain_size = int(0.6 * dataset_size)\nval_size = int(0.2 * dataset_size)\ntest_size = dataset_size - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(W2V_dataset, [train_size, val_size, test_size])\n\n\n\nModel settings\n\nclass W2V_CNN(nn.Module):\n    def __init__(self, embedding_dim, DOC_LEN, dropout_ratio,output_dim):\n        super(W2V_CNN, self).__init__()\n        #self.num_words_in_dict = num_words_in_dict\n        self.embedding_dim = embedding_dim\n        self.DOC_LEN = DOC_LEN\n        self.dropout_ratio = dropout_ratio\n        self.output_dim = output_dim\n\n        # embedding\n        self.embedding = nn.Embedding.from_pretrained(doc_embeddings, freeze=False)\n\n        self.unigram = nn.Sequential(\n        nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=1), \n        nn.ReLU(),\n        nn.MaxPool1d(kernel_size=self.DOC_LEN),  \n        nn.Flatten() \n        )\n      \n        self.bigram = nn.Sequential(\n        nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=4), \n        nn.ReLU(),\n        nn.MaxPool1d(kernel_size=self.DOC_LEN - 4 + 1),  \n        nn.Flatten() \n        )\n\n        self.trigram = nn.Sequential(\n        nn.Conv1d(in_channels=embedding_dim, out_channels=64, kernel_size=3), \n        nn.ReLU(),\n        nn.MaxPool1d(kernel_size=self.DOC_LEN - 3 + 1),  \n        nn.Flatten() \n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout_ratio),\n            nn.Linear(in_features=64*3, out_features=self.output_dim)\n        )\n        \n        \n    def forward(self, x):\n        # make sure we are convolving on each word\n        x = self.embedding(x)\n        #x = x.to(torch.double)\n        x = torch.transpose(x, dim0=1, dim1=2)  # (-1, DOC_LEN, embedding_dim): embedding on 1(DOC_LEN) & 2(embedding_dim) dims\n\n        uni_gram_output = self.unigram(x)\n        bi_gram_output = self.bigram(x)\n        tri_gram_output = self.trigram(x)\n        x = torch.cat((uni_gram_output, bi_gram_output, tri_gram_output), dim=1)\n        x = self.classifier(x)\n        x = F.log_softmax(x,dim = -1)\n        return x \n\n\nmodel = W2V_CNN(embedding_dim = 100, DOC_LEN = 200, dropout_ratio = 0.2,output_dim = 8)\nmodel_path = data_path + 'W2V_CNN.pt'\ntrained_model = train_model(model, train_dataset, val_dataset, test_dataset, \n                            device, encoder,lr=0.0003, epochs = 200, batch_size=128, \n                            patience = 5, model_path = model_path)\n\nTraining Start\nModel Saved!\nEpoch:1 / 200, train loss:1.587 train_acc:0.437, validation loss:1.344 validation acc:0.546\nModel Saved!\nEpoch:2 / 200, train loss:1.308 train_acc:0.536, validation loss:1.266 validation acc:0.541\nModel Saved!\nEpoch:3 / 200, train loss:1.220 train_acc:0.569, validation loss:1.231 validation acc:0.566\nModel Saved!\nEpoch:4 / 200, train loss:1.189 train_acc:0.577, validation loss:1.204 validation acc:0.562\nModel Saved!\nEpoch:5 / 200, train loss:1.158 train_acc:0.586, validation loss:1.201 validation acc:0.547\nModel Saved!\nEpoch:6 / 200, train loss:1.134 train_acc:0.588, validation loss:1.200 validation acc:0.547\nModel Saved!\nEpoch:7 / 200, train loss:1.094 train_acc:0.605, validation loss:1.175 validation acc:0.565\nModel Saved!\nEpoch:8 / 200, train loss:1.081 train_acc:0.619, validation loss:1.168 validation acc:0.572\nModel Saved!\nEpoch:9 / 200, train loss:1.065 train_acc:0.619, validation loss:1.166 validation acc:0.563\nModel Saved!\nEpoch:10 / 200, train loss:1.047 train_acc:0.626, validation loss:1.160 validation acc:0.561\nEpoch:11 / 200, train loss:1.021 train_acc:0.636, validation loss:1.163 validation acc:0.568\nModel Saved!\nEpoch:13 / 200, train loss:0.997 train_acc:0.647, validation loss:1.155 validation acc:0.565\nModel Saved!\nEpoch:16 / 200, train loss:0.949 train_acc:0.666, validation loss:1.153 validation acc:0.567\nModel Saved!\nEpoch:20 / 200, train loss:0.888 train_acc:0.691, validation loss:1.152 validation acc:0.558\nEpoch:21 / 200, train loss:0.868 train_acc:0.697, validation loss:1.165 validation acc:0.566\nEarly stopping at epoch:  24\n\n\n\n\nCode\nevaluate(trained_model, test_dataset, device, encoder, batch_size=128)\n\n\n              precision    recall  f1-score   support\n\n          CP       0.52      0.46      0.49       205\n          GN       0.70      0.57      0.63       277\n          MF       0.43      0.46      0.45       269\n          PM       0.61      0.68      0.64       190\n          PR       0.58      0.50      0.54       223\n          RM       0.57      0.65      0.61       213\n          ST       0.59      0.75      0.66       357\n          TR       0.69      0.51      0.59       197\n\n    accuracy                           0.58      1931\n   macro avg       0.59      0.57      0.57      1931\nweighted avg       0.59      0.58      0.58      1931\n\nacc on test subset: 0.584"
  },
  {
    "objectID": "dl.html#summary",
    "href": "dl.html#summary",
    "title": "Deep Learning Models on Classification",
    "section": "Summary",
    "text": "Summary\nComparing the classification performance from these three models, we can see that BERT+CNN got the highest accuracy as 0.590 and then Word2Vec+CNN. The BERT+NN model got the lowest accuracy in text classification here. We looked back into the structures in models and tried to understand the reasons for these outcomes. Comparing the first two models, with the same input of embeddings from BERT, we can conclude that CNN model outperformed NN model in out topic modeling. It may be becauseCNN is able to learn hierarchical representations of the input text, capturing both local features, while, in contrast, NN is typically fully connected, and is therefore less effective at learning local features, since all input features are connected to all hidden units. Besides, CNN can be more efficient than NN in processing text data, since it can learn to identify and extract important features from the input data using convolutional filters, reducing the dimensionality of the input data, and allowing for more efficient processing. The comparison between the last two models showed that BERT performed better together with CNN model in text classification than Word2Vec. As we know, BERT is a state-of-the-art pre-trained language model that has been trained on large amounts of text data and can capture the semantic relationships between words. It can help to extract better features from the text data, which is useful for text classification. BERT can capture both local and global relationships between words, which can be a complement to CNN models since CNN may not be able to capture the global relations in text. What’s more, BERT can generate contextual embeddings that capture the meaning of a word based on its surrounding words in a sentence. In contrast, Word2Vec generates static embeddings that do not take context into account. Therefore, it’s reasonable that the performance of BERT+CNN is better than Word2Vec+CNN here."
  },
  {
    "objectID": "fine.html",
    "href": "fine.html",
    "title": "Fine Tune Bert",
    "section": "",
    "text": "This notebook uses the Huggingface’s built in fine tune function BertForSequenceClassification, its source code can be found on github. TL.DR, it’s just BERT embedding + linear classifier."
  },
  {
    "objectID": "fine.html#prepare-data",
    "href": "fine.html#prepare-data",
    "title": "Fine Tune Bert",
    "section": "Prepare data",
    "text": "Prepare data\nPackage used in this section\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport ast\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\n\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf = pd.read_csv('qfin_processed_data.csv')\ndf = df[df.subcategory != 'EC']\n\ndf[\"tokenized_summary\"] = df[\"tokenized_summary\"].apply(lambda x: ast.literal_eval(x))\ndf[\"tokenized_summary_new\"] = df[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n\ncategories = df['subcategory'].unique()\nn_labels = len(categories)\nprint(n_labels)\ncat_dict = dict(zip(categories, range(n_labels)))\ndf['cat_id'] = df['subcategory'].map(cat_dict)\ndf.sample(3)\n\n8\n\n\n\n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\nterm\nnum_figures\nnum_pages\nnum_eqs\nmain_category\nsubcategory\ntokenized_summary\ntokenized_summary_new\ncat_id\n\n\n\n\n768\nhttp://arxiv.org/abs/2208.14267v1\n2022-08-30 13:42:55+00:00\n2022-08-30 13:42:55+00:00\nCommon Idiosyncratic Quantile Risk\n['Jozef Barunik', 'Matej Nevrla']\nWe propose a new model of asset returns with...\nNaN\nhttp://arxiv.org/abs/2208.14267v1\nNaN\nhttp://arxiv.org/pdf/2208.14267v1\nq-fin.GN\n0\n0\n0\nq-fin\nGN\n[propose, new, asset, return, common, factor, ...\npropose new asset return common factor shift r...\n6\n\n\n3981\nhttp://arxiv.org/abs/1902.08821v1\n2019-02-23 17:52:46+00:00\n2019-02-23 17:52:46+00:00\nClosed-End Formula for options linked to Targe...\n['Luca Di Persio', 'Luca Prezioso', 'Kai Wallb...\nRecent years have seen an emerging class of ...\n21 pages, 12 figures\nhttp://arxiv.org/abs/1902.08821v1\nNaN\nhttp://arxiv.org/pdf/1902.08821v1\nq-fin.PR\n12\n21\n0\nq-fin\nPR\n[recent, year, see, emerge, class, structured,...\nrecent year see emerge class structured financ...\n0\n\n\n237\nhttp://arxiv.org/abs/2302.08731v1\n2023-02-17 07:00:00+00:00\n2023-02-17 07:00:00+00:00\nOptimal management of DB pension fund under bo...\n['Guohui Guan', 'Zongxia Liang', 'Yi Xia']\nThis paper investigates the optimal manageme...\nNaN\nhttp://arxiv.org/abs/2302.08731v1\nNaN\nhttp://arxiv.org/pdf/2302.08731v1\nq-fin.PM\n0\n0\n0\nq-fin\nPM\n[investigate, optimal, management, aggregated,...\ninvestigate optimal management aggregated defi...\n7"
  },
  {
    "objectID": "fine.html#tokenization-function",
    "href": "fine.html#tokenization-function",
    "title": "Fine Tune Bert",
    "section": "Tokenization function",
    "text": "Tokenization function\n\ndef get_pretrained_wordvector(sentences, tokenizer):\n\n    input_ids = []\n    attention_masks = []\n\n    # Tokenize each sentence\n    for sent in sentences:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n        encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 150,           # Pad & truncate all sentences.\n                        truncation=True,\n                        padding = 'max_length',\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                        )\n        temp_ids = encoded_dict['input_ids'].squeeze()\n        temp_masks = encoded_dict['attention_mask'].squeeze()\n        input_ids.append(temp_ids)\n        attention_masks.append(temp_masks)\n    return input_ids, attention_masks\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\nsentences = df['tokenized_summary_new'].values\n\ninput_ids, attention_masks = get_pretrained_wordvector(sentences, tokenizer)\n# print(input_ids.shape, attention_masks.shape)\nprint('Original: ', sentences[0])\nprint('Token IDs:', input_ids[0])\nprint('Attention Mask:', attention_masks[0])\n\nOriginal:  black Scholes absence arbitrage impose necessary constraint slope imply variance term log moneyness asymptotically large log moneyness constraint example SVI imply volatility parameterization ensure result smile arbitrage note show arbitrage contraint mild arbitrage guarantee large range slope contraint enforce\nToken IDs: tensor([  101,  2304,  8040, 19990,  6438, 12098, 16313, 24449, 17607,  4072,\n        27142,  9663, 19515, 23284,  2744,  8833,  2769,  2791,  2004, 24335,\n        13876, 20214,  3973,  2312,  8833,  2769,  2791, 27142,  2742, 17917,\n         2072, 19515,  5285, 10450, 18605, 16381,  3989,  5676,  2765,  2868,\n        12098, 16313, 24449,  3602,  2265, 12098, 16313, 24449, 24528, 18447,\n        10256, 12098, 16313, 24449, 11302,  2312,  2846,  9663, 24528, 18447,\n        16306,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\nAttention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0])"
  },
  {
    "objectID": "fine.html#dataset",
    "href": "fine.html#dataset",
    "title": "Fine Tune Bert",
    "section": "Dataset",
    "text": "Dataset\n\nclass BertDataset(Dataset):\n    def __init__(self, features, labels) -&gt; None:\n        super(BertDataset).__init__()\n        self.lengeth = len(labels)\n        self.features = features\n        self.labels = torch.tensor(labels)\n        \n    def __getitem__(self, index):\n        return self.features[index], self.labels[index]\n\n    def __len__(self):\n        return self.lengeth\n\n\nX = list(zip(input_ids, attention_masks))\ny = df['cat_id'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\ntrain_dataset = BertDataset(X_train, y_train)\nval_dataset = BertDataset(X_val, y_val)\ntest_dataset = BertDataset(X_test, y_test)\n\nlen(train_dataset), len(val_dataset), len(test_dataset)\n\n(6947, 772, 1930)"
  },
  {
    "objectID": "fine.html#model-settings",
    "href": "fine.html#model-settings",
    "title": "Fine Tune Bert",
    "section": "Model settings",
    "text": "Model settings\n\nfrom transformers import BertForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = n_labels, # The number of output labels--2 for binary classification.  \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.to(device)\n\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=8, bias=True)\n)\n\n\n\nTrain and evaluate functions\n\n\nCode\ndef tune_bert(model, train_dataset, val_dataset,\n            batch_size=32, epochs=10, lr=2e-5, eps=1e-8):\n    model = model.to(device)\n    train_loader = DataLoader(train_dataset,\n                            batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset,\n                            batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset,\n                            batch_size=batch_size, shuffle=False)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=eps)\n    \n    history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n    \n    for epoch in range(epochs):\n        train_loss, train_acc = 0, 0\n        val_loss, val_acc = 0, 0\n        model.train()\n        for (input_ids, attention_masks), labels in train_loader:\n            input_ids = input_ids.to(device)\n            attention_masks = attention_masks.to(device)\n            labels = labels.to(device)\n            outputs = model(input_ids, \n                            token_type_ids=None, \n                            attention_mask=attention_masks, \n                            labels=labels)\n            pred = torch.argmax(outputs.logits, dim=1)\n            loss = outputs.loss\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            optimizer.zero_grad()\n            train_loss += loss.item()\n            train_acc += (pred == labels).sum().item()\n        \n        model.eval()\n        with torch.no_grad():\n            for (input_ids, attention_masks), labels in val_loader:\n                input_ids = input_ids.to(device)\n                attention_masks = attention_masks.to(device)\n                labels = labels.to(device)\n                outputs = model(input_ids, \n                                token_type_ids=None, \n                                attention_mask=attention_masks, \n                                labels=labels)\n                pred = torch.argmax(outputs.logits, dim=1)\n                loss = outputs.loss\n                val_loss += loss.item()\n                val_acc += (pred == labels).sum().item()\n        \n        train_loss /= len(train_dataset)\n        train_acc /= len(train_dataset)\n        val_loss /= len(val_dataset)\n        val_acc /= len(val_dataset)\n        history['train_acc'].append(train_acc)\n        history['train_loss'].append(train_loss)\n        history['val_acc'].append(val_acc)\n        history['val_loss'].append(val_loss)\n        print(f'Epoch {epoch+1}/{epochs}: train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n\n\n\n\nCode\ndef eval_model(model, test_dataset, batch_size=32):\n    model = model.to(device)\n    test_loader = DataLoader(test_dataset, \n                            batch_size=batch_size, shuffle=False)\n    model.eval()\n    true_labels = np.array([])\n    predictions = np.array([])\n    with torch.no_grad():\n        for (input_ids, attention_masks), labels in test_loader:\n                input_ids = input_ids.to(device)\n                attention_masks = attention_masks.to(device)\n                labels = labels.to(device)\n                outputs = model(input_ids, \n                                token_type_ids=None, \n                                attention_mask=attention_masks, \n                                labels=labels)\n                pred = torch.argmax(outputs.logits, dim=1)\n                true_labels = np.concatenate((true_labels, labels.cpu().numpy()))\n                predictions = np.concatenate((predictions, pred.cpu().numpy()))\n    print('Test Accuracy: ', (true_labels == predictions).sum() / len(true_labels))\n    return true_labels, predictions\n    \n\n\n\ntune_bert(model, train_dataset, val_dataset,\n        batch_size=32, epochs=3, lr=2e-5, eps=1e-8)\n\nEpoch 1/3: train_loss: 0.0486, train_acc: 0.4491, val_loss: 0.0377, val_acc: 0.6282\nEpoch 2/3: train_loss: 0.0363, train_acc: 0.5938, val_loss: 0.0349, val_acc: 0.6438\nEpoch 3/3: train_loss: 0.0309, train_acc: 0.6610, val_loss: 0.0357, val_acc: 0.6269\n\n\n\ntrue, pred = eval_model(model, test_dataset, batch_size=32)\n\nTest Accuracy:  0.5854922279792746\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(true, pred, target_names=categories))\n\n              precision    recall  f1-score   support\n\n          PR       0.57      0.44      0.50       215\n          RM       0.66      0.54      0.59       207\n          CP       0.67      0.42      0.52       217\n          MF       0.44      0.43      0.43       257\n          TR       0.62      0.63      0.63       184\n          ST       0.62      0.70      0.65       371\n          GN       0.56      0.78      0.65       300\n          PM       0.61      0.64      0.62       179\n\n    accuracy                           0.59      1930\n   macro avg       0.59      0.57      0.57      1930\nweighted avg       0.59      0.59      0.58      1930"
  },
  {
    "objectID": "gpt3.5.html",
    "href": "gpt3.5.html",
    "title": "Load API, data and helper functions",
    "section": "",
    "text": "In this section, we are going to leverage the most popular LLM model’s api from OpenAI to see if it could help us to classify papers in quantitive finance filed accurately.\nThe following api_key settings and helper function are adapted from Andrew Ng’s prompt course on deeplearning.ai.\nimport openai\nimport os\nimport pandas as pd\nimport json\nfrom dotenv import load_dotenv, find_dotenv\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\n\n_ = load_dotenv(find_dotenv())\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\ndf = pd.read_csv(\"./qfin_processed_data.csv\")\ndf = df.drop_duplicates()\ndf = df[df.subcategory != 'EC']\nencoder = LabelEncoder()\ndf['label'] = encoder.fit_transform(df['subcategory'].values)\n\n\ntitles = df[\"title\"].values\nabstracts = df[\"summary\"].values\n\nprint(titles[0])\nprint(\"\\n\", abstracts[0])\n\nMaximum Implied Variance Slope -- Practical Aspects\n\n   In the Black-Scholes model, the absence of arbitrages imposes necessary\nconstraints on the slope of the implied variance in terms of log-moneyness,\nasymptotically for large log-moneyness. The constraints are used for example in\nthe SVI implied volatility parameterization to ensure the resulting smile has\nno arbitrages. This note shows that those no-arbitrage contraints are very\nmild, and that arbitrage is almost always guaranteed in a large range of slopes\nwhere the contraints are enforced.\n# Andrew mentioned that the prompt/ completion paradigm is preferable for this class\ndef get_completion(prompt, model=\"gpt-3.5-turbo\"):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        temperature=0,  # this is the degree of randomness of the model's output\n    )\n    return response.choices[0].message[\"content\"]"
  },
  {
    "objectID": "gpt3.5.html#prompt-design",
    "href": "gpt3.5.html#prompt-design",
    "title": "Load API, data and helper functions",
    "section": "Prompt Design",
    "text": "Prompt Design\n\ndef get_prompt(title, abstract):\n    prompt = f\"\"\"\nYour task is to determine a category for the given paper \\\nand give a short justification for your decision.\nThis paper is from Quantitative Finance filed, the category \\\nshould be one of the following: \n    1. Computational Finance \n    2. General Finance \n    3. Mathematical Finance \n    4. Portfolio Management \n    5. Pricing of Securities \n    6. Risk Management \n    7. Statistical Finance \n    8. Trading and Market Microstructure\n    \nGiven the title and abstract below, delimited by triple backticks.\nPlease determine the category and give a short justification for your decision in 30 words.\n\nTitle: ```{title}```\nAbstract: ```{abstract}```\n\nUsing the following format for your response:\nCategory: &lt;category name&gt;\nJustification: &lt;your justification&gt;\nOutput Json: &lt;json with category and justification&gt;\n\"\"\"\n    return prompt"
  },
  {
    "objectID": "gpt3.5.html#simple-example-to-test-the-api",
    "href": "gpt3.5.html#simple-example-to-test-the-api",
    "title": "Load API, data and helper functions",
    "section": "Simple example to test the api",
    "text": "Simple example to test the api\nA simple example could be the following case:\n\ntest_prompt = get_prompt(titles[0], abstracts[0])\nprint(test_prompt)\n\n\nYour task is to determine a category for the given paper and give a short justification for your decision.\nThis paper is from Quantitative Finance filed, the category should be one of the following: \n    1. Computational Finance \n    2. General Finance \n    3. Mathematical Finance \n    4. Portfolio Management \n    5. Pricing of Securities \n    6. Risk Management \n    7. Statistical Finance \n    8. Trading and Market Microstructure\n    \nGiven the title and abstract below, delimited by triple backticks.\nPlease determine the category and give a short justification for your decision in 30 words.\n\nTitle: ```Maximum Implied Variance Slope -- Practical Aspects```\nAbstract: ```  In the Black-Scholes model, the absence of arbitrages imposes necessary\nconstraints on the slope of the implied variance in terms of log-moneyness,\nasymptotically for large log-moneyness. The constraints are used for example in\nthe SVI implied volatility parameterization to ensure the resulting smile has\nno arbitrages. This note shows that those no-arbitrage contraints are very\nmild, and that arbitrage is almost always guaranteed in a large range of slopes\nwhere the contraints are enforced.\n```\n\nUsing the following format for your response:\nCategory: &lt;category name&gt;\nJustification: &lt;your justification&gt;\nOutput Json: &lt;json with category and justification&gt;\n\n\n\nThen we can use the helper functions to get the result:\n\nresponse = get_completion(test_prompt)\nprint(\"\\nResult of test example:\")\nprint(response)\n\n\nResult of test example:\nCategory: Mathematical Finance\nJustification: The paper discusses the necessary constraints on the slope of implied variance in the Black-Scholes model, which is a fundamental concept in mathematical finance. It also highlights the mildness of these constraints and the possibility of arbitrage, which is a crucial aspect of financial modeling. \nOutput Json: {\"Category\": \"Mathematical Finance\", \"Justification\": \"The paper discusses the necessary constraints on the slope of implied variance in the Black-Scholes model, which is a fundamental concept in mathematical finance. It also highlights the mildness of these constraints and the possibility of arbitrage, which is a crucial aspect of financial modeling.\"}\n\n\nWe can further use the json output to parse the result:\n\nstart_index = response.find('{')\nend_index = response.rfind('}')\njson_string = response[start_index:end_index+1]\n\nparsed_dict = json.loads(json_string)\nparsed_dict\n\n{'Category': 'Mathematical Finance',\n 'Justification': 'The paper discusses the necessary constraints on the slope of implied variance in the Black-Scholes model, which is a fundamental concept in mathematical finance. It also highlights the mildness of these constraints and the possibility of arbitrage, which is a crucial aspect of financial modeling.'}"
  },
  {
    "objectID": "gpt3.5.html#chatgpt-can-you-classify-papers-into-the-correct-category",
    "href": "gpt3.5.html#chatgpt-can-you-classify-papers-into-the-correct-category",
    "title": "Load API, data and helper functions",
    "section": "ChatGPT, can you classify papers into the correct category?",
    "text": "ChatGPT, can you classify papers into the correct category?\nNext we define a helper function to automatically generate the prompt and call the api to get the result. Here we save the result every 100 iterations to avoid losing the result due to the api call limit and also we can save money by avoiding accidents.\n\ndef parse_result(response):\n    start_index = response.find('{')\n    end_index = response.rfind('}')\n    json_string = response[start_index:end_index+1]\n    parsed_dict = json.loads(json_string)\n    return parsed_dict['Category'], parsed_dict['Justification']\n\n\nn = len(titles)\nfor i in tqdm(range(n)):\n    try:\n        title = titles[i]\n        abstract = abstracts[i]\n        curr_prompt = get_prompt(title, abstract)\n        response = get_completion(curr_prompt)\n        curr_cat, curr_jus = parse_result(response)\n        df.loc[i, 'predicted_category'] = curr_cat\n        df.loc[i, 'predicted_justification'] = curr_jus\n    except:\n        print(f\"Error at index {i}\")\n    if i % 50 == 0:\n        df.to_csv(\"./qfin_predictions.csv\", index=False)\n\n100%|██████████| 3/3 [00:22&lt;00:00,  7.58s/it]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Main page",
    "section": "",
    "text": "In this website, we present a novel NLP-based approach to classify academic papers into their respective subcategories, extract the most important keywords in each subcategory, and develop a recommendation system to suggest the most relevant papers in relation to a given input.\nOur proposed model leverages state-of-the-art NLP techniques, including web scraping, topic modeling, word embedding, and deep learning, to achieve these objectives. By providing an integrated solution for classification, keyword extraction, and recommendation, our approach aims to streamline the research process for academics, making it easier for them to identify and explore relevant works in their fields. This, in turn, can foster a more efficient knowledge transfer, and enable researchers to build upon the insights of others more effectively. The following posts provide a detailed description of each step in our approach and the results we obtained."
  },
  {
    "objectID": "index.html#summary",
    "href": "index.html#summary",
    "title": "Main page",
    "section": "",
    "text": "In this website, we present a novel NLP-based approach to classify academic papers into their respective subcategories, extract the most important keywords in each subcategory, and develop a recommendation system to suggest the most relevant papers in relation to a given input.\nOur proposed model leverages state-of-the-art NLP techniques, including web scraping, topic modeling, word embedding, and deep learning, to achieve these objectives. By providing an integrated solution for classification, keyword extraction, and recommendation, our approach aims to streamline the research process for academics, making it easier for them to identify and explore relevant works in their fields. This, in turn, can foster a more efficient knowledge transfer, and enable researchers to build upon the insights of others more effectively. The following posts provide a detailed description of each step in our approach and the results we obtained."
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Main page",
    "section": "Contents",
    "text": "Contents"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "In this section we are showing an example of our workflow on applying models on classifying main categories of academic papers, i.e. Computer Science, Mathematics, Physics, Statistics, Quantitative Biology, Quantitative Finance. Our model is based on random forest, and we achieve an accuracy of 0.85 on the test set.\nTherefore, we change our goal to be classifying academic papers inside a main category into sub-categories in the rest of the project, it’s more realistic and challenging, because researchers are more interested in the sub-categories of their research field."
  },
  {
    "objectID": "Introduction.html#introduction",
    "href": "Introduction.html#introduction",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "In this section we are showing an example of our workflow on applying models on classifying main categories of academic papers, i.e. Computer Science, Mathematics, Physics, Statistics, Quantitative Biology, Quantitative Finance. Our model is based on random forest, and we achieve an accuracy of 0.85 on the test set.\nTherefore, we change our goal to be classifying academic papers inside a main category into sub-categories in the rest of the project, it’s more realistic and challenging, because researchers are more interested in the sub-categories of their research field."
  },
  {
    "objectID": "Introduction.html#classification-of-academic-papers",
    "href": "Introduction.html#classification-of-academic-papers",
    "title": "Introduction and Motivation",
    "section": "Classification of Academic Papers",
    "text": "Classification of Academic Papers\nIn the following part, we are going to apply random forest model to classify academic papers into different categories, the dataset is from our scrapper, you can find more details in Scrape data from Arxiv section.\nPackages used in this section:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom sklearn.model_selection import train_test_split\nimport ast\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\n\nData processing:\n\ndf = pd.read_csv(\"processed_data.csv\")\ndf = df.drop(columns = ['Unnamed: 0'])\ndf['summary'] = df['title'] + df['summary'] \n\ncategories = np.unique(df['main_category']).tolist() \ndf[categories] = pd.get_dummies(df['main_category'])\n\n\n\nMain categories of academic papers\n\nunique_label = (list(set(df.main_category)))\nunique_label\n\n['cs', 'math', 'stat', 'q-bio', 'q-fin', 'physics']\n\n\n\nprint(df['main_category'].value_counts())\n\ncs         1840\nphysics    1155\nmath        803\nq-bio       743\nstat        652\nq-fin       591\nName: main_category, dtype: int64\n\n\n\n\nRandom forest classifier\nHere we use random forest to classify academic papers into different categories, its performance is really good, this function is from TODO. Classification report, precision-recall curve and ROC curve are shown below.\n\n\nCode\ndef supervised_learning(train_df, test_df, min_df = 1, stopwords = None, base_model = LinearSVC()):\n    tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    X_train = tfidf_vect.fit_transform(train_df['summary'])\n    y_train = train_df[categories]\n    X_test = tfidf_vect.transform(test_df['summary'])\n    y_test = test_df[categories].values\n\n    model = OneVsRestClassifier(base_model)\n    model.fit(X_train, y_train)\n        \n    try:\n        predict_p = model.predict_proba(X_test)\n        y_pred = np.zeros_like(predict_p)\n        y_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        \n        precision = dict()\n        recall = dict()\n        for i in range(len(categories)):\n            precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        predict_p[:, i])\n            plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(categories[i]))\n    \n        plt.xlabel(\"Recall\")\n        plt.ylabel(\"Precision\")\n        plt.title(\"Precision-Recall Curve\")\n        plt.legend(loc=\"lower left\")\n        plt.show();\n    \n        # roc curve\n        fpr = dict()\n        tpr = dict()\n\n        for i in range(len(categories)):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i],predict_p[:, i])\n            plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(categories[i]))\n\n        plt.xlabel(\"false positive rate\")\n        plt.ylabel(\"true positive rate\")\n        plt.legend(loc=\"best\")\n        plt.title(\"ROC curve\")\n        plt.show();\n        return model,tfidf_vect\n    \n    except:\n        y_pred = model.predict(X_test)\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        print('The base model has no predict_proba function. ROC, PRC curve cannot be plotted!')\n        return model,tfidf_vect\n\n\n\nbase_model = RandomForestClassifier()\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nmodel,tfidf_vect = supervised_learning(train_df, test_df, min_df = 1, stopwords = 'english', base_model =base_model)\n\n              precision    recall  f1-score   support\n\n          cs       0.82      0.95      0.88       366\n        math       0.86      0.94      0.90       152\n     physics       0.90      0.84      0.87       233\n       q-bio       0.86      0.76      0.81       137\n       q-fin       0.94      0.94      0.94       139\n        stat       0.97      0.67      0.79       130\n\n   micro avg       0.87      0.87      0.87      1157\n   macro avg       0.89      0.85      0.86      1157\nweighted avg       0.88      0.87      0.87      1157\n samples avg       0.87      0.87      0.87      1157\n\n\n\n\n\n\n\n\n\n\n\nTop words in each category\n\n\nCode\nfor i in range(len(categories)):\n\n    important_features = []\n\n    for feature, importance in sorted(zip(tfidf_vect.get_feature_names_out(), model.estimators_[i].feature_importances_), \\\n                                  key=lambda x: x[1], reverse=True):\n        #print(feature, importance)\n        important_features.append(feature)\n    top_words=important_features[0:10]\n    print ('{}: {}'.format(categories[i],top_words))\n    print(\"\\n\")\n\n\ncs: ['learning', 'tasks', 'performance', 'training', 'language', 'art', 'trained', 'dataset', 'task', 'existing']\n\n\nmath: ['prove', 'mathbb', 'let', 'conjecture', 'theorem', 'algebras', 'spaces', 'model', 'mathcal', 'result']\n\n\nphysics: ['optical', 'fluid', 'beam', 'radiation', 'waves', 'wave', 'plasma', 'energy', 'magnetic', 'electron']\n\n\nq-bio: ['protein', 'brain', 'biological', 'genome', 'cells', 'metabolic', 'gene', 'population', 'proteins', 'biology']\n\n\nq-fin: ['market', 'volatility', 'risk', 'financial', 'price', 'portfolio', 'pricing', 'stock', 'asset', 'markets']\n\n\nstat: ['bayesian', 'outcome', 'estimators', 'statistical', 'inference', 'covariates', 'estimation', 'data', 'trial', 'simulation']"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning Models",
    "section": "",
    "text": "In this section, we explore unsupervised machine learning algorithms such as K-means clustering, Gaussian Mixture Model (GMM), and Latent Dirichlet Allocation (LDA) to group papers based on their underlying topics and extract the most important keywords within each subcategory. These approaches rely on the assumption that similar papers will have similar distributions of words or features, and thus can be clustered together. LDA, a popular topic modeling technique, can uncover the underlying themes and topics present in a corpus of documents, generating a set of topics represented by a distribution of words.\nNext, we investigate supervised machine learning algorithms, including Support Vector Machines (SVM) and Random Forest, for paper classification and keywords extraction. These algorithms are trained on a labeled dataset, where each paper is associated with its corresponding subcategory, and learn to predict the subcategory of a given paper based on its features. To represent the textual content of the papers, we experiment with various feature extraction methods, such as bag-of-words, term frequency-inverse document frequency (TF-IDF), and pre-trained word embeddings. By training these algorithms on a diverse set of features, we can develop a robust and accurate model that can effectively categorize papers into their corresponding subcategories and extract the most important words in each subcategory, facilitating improved organization and navigation of the research landscape.\nBy analyzing the most probable words within each topic, we can extract the most important keywords characterizing each subcategory, thus having valuable insights into the thematic organization of the Quantitative Finance domain.\nPackages used in this section:\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n#import matplotlib\n#matplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom nltk.cluster import KMeansClusterer, cosine_distance, euclidean_distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import metrics\nfrom nltk.corpus import stopwords\nfrom sklearn import mixture\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.metrics.pairwise import cosine_similarity\n#from sklearn.metrics import pairwise_distances\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nfrom sklearn.model_selection import train_test_split\nimport ast\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import precision_recall_curve, roc_curve\nfrom sklearn.multiclass import OneVsRestClassifier\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\n2023-05-03 15:04:44.566271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "ml.html#topic-modeling",
    "href": "ml.html#topic-modeling",
    "title": "Machine Learning Models",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\ndf = pd.read_csv('qfin_processed_data.csv')\ndf = df.drop(columns=['Unnamed: 0'])\ndf[\"tokenized_summary\"] = df[\"tokenized_summary\"].apply(lambda x: ast.literal_eval(x))\n#Combine the tokenized list back to a string \ndf[\"tokenized_summary_new\"] = df[\"tokenized_summary\"].apply(lambda x: ' '.join(x)) \ndf.head()\n\n\n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\nterm\nnum_figures\nnum_pages\nnum_eqs\nmain_category\nsubcategory\ncombined_Bert\ntokenized_summary\ntokenized_summary_new\n\n\n\n\n0\nhttp://arxiv.org/abs/2304.13610v1\n2023-04-26 15:05:19+00:00\n2023-04-26 15:05:19+00:00\nMaximum Implied Variance Slope -- Practical As...\n[\"Fabien Le Floc'h\", 'Winfried Koller']\nMaximum Implied Variance Slope -- Practical As...\nNaN\nhttp://arxiv.org/abs/2304.13610v1\nNaN\nhttp://arxiv.org/pdf/2304.13610v1\nq-fin.PR\n0\n0\n0\nq-fin\nPR\nTitle: Maximum Implied Variance Slope -- Pract...\n[maximum, implied, variance, slope, practical,...\nmaximum implied variance slope practical aspec...\n\n\n1\nhttp://arxiv.org/abs/2206.02582v2\n2023-04-26 13:13:28+00:00\n2022-06-06 12:43:06+00:00\nMaking heads or tails of systemic risk measures\n['Aleksy Leeuwenkamp']\nMaking heads or tails of systemic risk measure...\nRevised version of the $\\Delta$-CoES paper, no...\nhttp://arxiv.org/abs/2206.02582v2\nNaN\nhttp://arxiv.org/pdf/2206.02582v2\nq-fin.RM\n0\n22\n4\nq-fin\nRM\nTitle: Making heads or tails of systemic risk ...\n[making, head, tail, systemic, risk, measure, ...\nmaking head tail systemic risk measure paper s...\n\n\n2\nhttp://arxiv.org/abs/2301.00790v2\n2023-04-26 10:56:51+00:00\n2022-12-30 17:19:00+00:00\nDynamic Feature Engineering and model selectio...\n['Thomas Wong', 'Mauricio Barahona']\nDynamic Feature Engineering and model selectio...\nNaN\nhttp://arxiv.org/abs/2301.00790v2\nNaN\nhttp://arxiv.org/pdf/2301.00790v2\nq-fin.CP\n0\n0\n0\nq-fin\nCP\nTitle: Dynamic Feature Engineering and model s...\n[dynamic, feature, engineering, model, selecti...\ndynamic feature engineering model selection me...\n\n\n3\nhttp://arxiv.org/abs/2304.13402v1\n2023-04-26 09:28:41+00:00\n2023-04-26 09:28:41+00:00\nConvexity adjustments à la Malliavin\n['David García-Lorite', 'Raul Merino']\nConvexity adjustments à la Malliavin In this ...\nNaN\nhttp://arxiv.org/abs/2304.13402v1\nNaN\nhttp://arxiv.org/pdf/2304.13402v1\nq-fin.MF\n0\n0\n0\nq-fin\nMF\nTitle: Convexity adjustments à la Malliavin; C...\n[convexity, adjustment, la, malliavin, paper, ...\nconvexity adjustment la malliavin paper develo...\n\n\n4\nhttp://arxiv.org/abs/2304.13128v1\n2023-04-25 20:16:36+00:00\n2023-04-25 20:16:36+00:00\nLearning Volatility Surfaces using Generative ...\n['Andrew Na', 'Meixin Zhang', 'Justin Wan']\nLearning Volatility Surfaces using Generative ...\nThis is a working draft\nhttp://arxiv.org/abs/2304.13128v1\nNaN\nhttp://arxiv.org/pdf/2304.13128v1\nq-fin.CP\n0\n0\n0\nq-fin\nCP\nTitle: Learning Volatility Surfaces using Gene...\n[learning, volatility, surface, using, generat...\nlearning volatility surface using generative a...\n\n\n\n\n\n\n\n\ntrain_df, test_df= train_test_split(df, test_size=0.2, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\n\nUnsupervised learning\nWe first tried clustering and LDA that were discussed in topic modeling session. We used the K-means, GMM and LDA. After a comprehensive comparison of three unsupervised machine learning algorithms using various metrics in the classification report, we found that K-means had the best performance while GMM had the worst performance. Thus in the next subsections, we mainly analyze the results of K-means and only report the results of GMM and LDA.\nThe code chunk shows the helper function cluster for all of the three unsupervised learning models.\n\n\nCode\ndef cluster(train_data, test_data, num_clusters, min_df = 1, stopwords = None, method = 'k-mean'):\n    \n    train_temp = train_data.copy() \n    test_temp = test_data.copy()\n    \n    train_temp[\"tokenized_summary\"] = train_data[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n    test_temp[\"tokenized_summary\"] = test_data[\"tokenized_summary\"].apply(lambda x: ' '.join(x))\n    \n    \n   # add your code\n    if method == 'k-mean':\n        clusterer = KMeansClusterer(num_clusters, cosine_distance, repeats=20)\n        \n        tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)     \n        \n        dtm = tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        clusters = clusterer.cluster(dtm.toarray(), assign_clusters=True)\n    \n        test_dtm = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        predicted = [clusterer.classify(v) for v in test_dtm.toarray()]\n        \n    \n    \n    elif method == 'gmm':\n        gmm = mixture.GaussianMixture(n_components=num_clusters,\n                                      covariance_type='diag', random_state=42)\n        \n        tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    \n        dtm= tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        gmm.fit(dtm.toarray())\n    \n        test_dtm = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        predicted = gmm.predict(test_dtm.toarray())\n    \n    else: \n        tfidf_vect = CountVectorizer(min_df = min_df, stop_words = stopwords)\n    \n        X_train = tfidf_vect.fit_transform(train_temp[\"tokenized_summary\"])\n    \n        lda = LatentDirichletAllocation(n_components=num_clusters, \\\n                                max_iter=20,verbose=1,\n                                evaluate_every=1, n_jobs=1,\n                                random_state=0).fit(X_train)\n        \n        X_test = tfidf_vect.transform(test_temp[\"tokenized_summary\"])\n\n        doc_topic = lda.transform(X_test)\n        \n        predicted = doc_topic.argmax(axis = 1)\n    \n    \n    confusion_df = pd.DataFrame(list(zip(test_temp[\"subcategory\"].values, predicted)),columns = [\"label\", \"cluster\"])\n    \n    print (pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label))\n    \n    #Apply majority vote rule to dynamically map each cluster to a ground-truth label in test_data\n    label_allocation = pd.DataFrame(confusion_df.groupby('cluster')['label'].apply\\\n                                    (lambda x: x.value_counts().idxmax()))\n    \n    \n    #Interpret each cluster by centroid\n    \n    if method in ['k-mean','gmm']:\n        \n        if method =='k-mean':\n            centroids=np.array(clusterer.means())\n        else:\n            centroids=np.array(gmm.means_)\n        \n        sorted_centroids = centroids.argsort()[:, ::-1]\n\n        voc_lookup= tfidf_vect.get_feature_names_out()\n    \n        for i in range(num_clusters):\n            top_words=[voc_lookup[word_index] for word_index in sorted_centroids[i, :10]]\n            print ('Cluster {} -&gt; Topic {}'.format(i,label_allocation['label'][i]))\n            print ('    top words: ' +str(top_words))\n        \n    else: \n        voc_lookup= tfidf_vect.get_feature_names_out()\n    \n    \n        for topic_idx, topic in enumerate(lda.components_):\n            top_words=[voc_lookup[i] for i in topic.argsort()[::-1][0:10]]\n            print ('Cluster {} -&gt; Topic {}'.format(topic_idx,label_allocation['label'][topic_idx]))\n            print ('    top words: ' +str(top_words))\n    \n    #classification report\n    cluster_dict = {}\n    for i in range(num_clusters):\n        cluster_dict[i] = label_allocation['label'][i]\n    \n    predicted_target=[cluster_dict[i] for i in predicted]\n    \n    print(metrics.classification_report(test_temp[\"subcategory\"], predicted_target))\n     \n    return None\n\n\n\nK-means\nThe next contents show results of the K-means algorithm, which provides valuable insights into the classification of academic papers into their respective subcategories within the Quantitative Finance domain. By examining the distribution of paper labels within each cluster, we observe that Cluster 0 is predominantly associated with Computational Finance (CP), Cluster 1 with Pricing of Securities (PR), Cluster 2 with Risk Management (RM), Cluster 3 with General Finance (GN), Cluster 4 with Portfolio Management (PM), Cluster 5 with Trading and Market Microstructure (TR), and Cluster 7 with Statistical Finance (ST). Notably, Cluster 6 shows a mix of topics and does not correspond to a specific subcategory, indicating that further refinement may be required in the clustering process.\nExamining the top words associated with each cluster, we find that they are coherent and representative of the respective subcategories. For example, the top words for Cluster 0 include ‘option’, ‘pricing’, ‘model’, and ‘price’, which are indicative of Computational Finance. Similarly, the top words for Cluster 2, which is associated with Risk Management, include ‘risk’, ‘portfolio’, ‘measure’, and ‘model’.\nIn terms of classification performance, we report an overall accuracy of 0.44, with varying levels of precision, recall, and F1-score across the different subcategories. While some subcategories, such as General Finance and Trading and Market Microstructure, demonstrate relatively high performance (F1-scores of 0.55 and 0.58, respectively), others, such as Mathematical Finance, show room for improvement (F1-score of 0.00).\nThese results suggest that while our K-means clustering approach has been successful in identifying and characterizing the majority of subcategories within the Quantitative Finance domain, further refinements in the methodology, such as incorporating more sophisticated feature extraction techniques or alternative clustering algorithms, may be necessary to improve the classification performance for certain subcategories. Overall, our analysis demonstrates the potential of unsupervised learning techniques, such as K-means clustering, for effectively classifying academic papers and identifying important keywords in the rapidly evolving field of Quantitative Finance.\n\ncluster(train_df, test_df, num_clusters = 8, min_df = 5, stopwords = 'english', method = 'k-mean')\n\nlabel    CP   GN  MF  PM  PR   RM   ST   TR\ncluster                                    \n0        86    3  46   2  78    6    3    3\n1        29    6  40   2  43    3   37    4\n2        11   21  17  63   3  101   17    0\n3        17   20  39   3  58   43   10    4\n4        22    8  73  82  18   15    3    9\n5        19  176   7   3   3   24   93   20\n6         8   21  27   7   7    4   26  115\n7        25   45   8  17   5   11  182   29\nCluster 0 -&gt; Topic CP\n    top words: ['option', 'pricing', 'model', 'price', 'method', 'hedging', 'black', 'scholes', 'numerical', 'american']\nCluster 1 -&gt; Topic PR\n    top words: ['volatility', 'model', 'implied', 'stochastic', 'price', 'option', 'process', 'rough', 'heston', 'jump']\nCluster 2 -&gt; Topic RM\n    top words: ['risk', 'portfolio', 'measure', 'model', 'optimization', 'asset', 'return', 'approach', 'distribution', 'value']\nCluster 3 -&gt; Topic PR\n    top words: ['default', 'credit', 'model', 'rate', 'risk', 'arbitrage', 'bond', 'bank', 'market', 'pricing']\nCluster 4 -&gt; Topic PM\n    top words: ['optimal', 'problem', 'utility', 'strategy', 'portfolio', 'investment', 'function', 'solution', 'time', 'process']\nCluster 5 -&gt; Topic GN\n    top words: ['distribution', 'model', 'financial', 'economic', 'network', 'time', 'data', 'agent', 'market', 'growth']\nCluster 6 -&gt; Topic TR\n    top words: ['order', 'trading', 'market', 'price', 'book', 'impact', 'strategy', 'limit', 'model', 'agent']\nCluster 7 -&gt; Topic ST\n    top words: ['stock', 'market', 'price', 'correlation', 'data', 'model', 'return', 'financial', 'time', 'learning']\n              precision    recall  f1-score   support\n\n          CP       0.38      0.40      0.39       217\n          GN       0.51      0.59      0.55       300\n          MF       0.00      0.00      0.00       257\n          PM       0.36      0.46      0.40       179\n          PR       0.28      0.47      0.35       215\n          RM       0.43      0.49      0.46       207\n          ST       0.57      0.49      0.53       371\n          TR       0.53      0.62      0.58       184\n\n    accuracy                           0.44      1930\n   macro avg       0.38      0.44      0.41      1930\nweighted avg       0.39      0.44      0.41      1930\n\n\n\n\n\nGaussian Mixture Model\n\ncluster(train_df, test_df, num_clusters = 8, min_df = 5, stopwords = 'english', method = 'gmm')\n\nlabel    CP   GN  MF  PM  PR  RM   ST  TR\ncluster                                  \n0         2   13   4   5   5   9   25   4\n1        23    1  31   2  27   3   17   2\n2         8   14  20  16   4   7   28  45\n3         1    0   2   2   1   3    8  13\n4        84  243  61  61  58  91  251  98\n5        69    3  52   3  66  15    9   3\n6        15   15  24  27  28  58   26  10\n7        15   11  63  63  26  21    7   9\nCluster 0 -&gt; Topic ST\n    top words: ['distribution', 'model', 'return', 'income', 'tail', 'wealth', 'time', 'law', 'probability', 'financial']\nCluster 1 -&gt; Topic MF\n    top words: ['volatility', 'model', 'implied', 'stochastic', 'option', 'price', 'rough', 'process', 'time', 'market']\nCluster 2 -&gt; Topic TR\n    top words: ['trading', 'market', 'agent', 'strategy', 'price', 'model', 'learning', 'impact', 'cost', 'stock']\nCluster 3 -&gt; Topic TR\n    top words: ['order', 'book', 'limit', 'price', 'market', 'model', 'impact', 'lob', 'flow', 'stock']\nCluster 4 -&gt; Topic ST\n    top words: ['market', 'stock', 'model', 'financial', 'time', 'network', 'price', 'data', 'correlation', 'analysis']\nCluster 5 -&gt; Topic CP\n    top words: ['option', 'pricing', 'model', 'price', 'method', 'hedging', 'equation', 'process', 'numerical', 'american']\nCluster 6 -&gt; Topic RM\n    top words: ['risk', 'measure', 'model', 'portfolio', 'default', 'credit', 'systemic', 'financial', 'network', 'capital']\nCluster 7 -&gt; Topic MF\n    top words: ['optimal', 'portfolio', 'problem', 'utility', 'strategy', 'risk', 'investment', 'function', 'asset', 'optimization']\n              precision    recall  f1-score   support\n\n          CP       0.31      0.32      0.32       217\n          GN       0.00      0.00      0.00       300\n          MF       0.29      0.37      0.33       257\n          PM       0.00      0.00      0.00       179\n          PR       0.00      0.00      0.00       215\n          RM       0.29      0.28      0.28       207\n          ST       0.27      0.74      0.40       371\n          TR       0.34      0.32      0.33       184\n\n    accuracy                           0.29      1930\n   macro avg       0.19      0.25      0.21      1930\nweighted avg       0.19      0.29      0.22      1930\n\n\n\n\n\nLatent Dirichlet Allocation\n\ncluster(train_df, test_df, num_clusters = 8, min_df = 5, stopwords = 'english', method = 'lda')\n\niteration: 1 of max_iter: 20, perplexity: 1662.1941\niteration: 2 of max_iter: 20, perplexity: 1474.4942\niteration: 3 of max_iter: 20, perplexity: 1372.6859\niteration: 4 of max_iter: 20, perplexity: 1318.5665\niteration: 5 of max_iter: 20, perplexity: 1288.2508\niteration: 6 of max_iter: 20, perplexity: 1270.3625\niteration: 7 of max_iter: 20, perplexity: 1259.0478\niteration: 8 of max_iter: 20, perplexity: 1251.2028\niteration: 9 of max_iter: 20, perplexity: 1245.4672\niteration: 10 of max_iter: 20, perplexity: 1241.2476\niteration: 11 of max_iter: 20, perplexity: 1238.0928\niteration: 12 of max_iter: 20, perplexity: 1235.7949\niteration: 13 of max_iter: 20, perplexity: 1234.0271\niteration: 14 of max_iter: 20, perplexity: 1232.5545\niteration: 15 of max_iter: 20, perplexity: 1231.4097\niteration: 16 of max_iter: 20, perplexity: 1230.4636\niteration: 17 of max_iter: 20, perplexity: 1229.6428\niteration: 18 of max_iter: 20, perplexity: 1228.9761\niteration: 19 of max_iter: 20, perplexity: 1228.3584\niteration: 20 of max_iter: 20, perplexity: 1227.8524\nlabel    CP   GN   MF  PM  PR  RM   ST  TR\ncluster                                   \n0         5    9   16  63   1   5   17  70\n1        19   17  128  66  49  13    4  12\n2        55    3   57   1  85   7   37   5\n3        13  150    6   7  11  38   39  18\n4        14   73   10   6   7  16  149  44\n5        33   28    8  16   2  20  119  32\n6        74    2   26  17  53  75    4   2\n7         4   18    6   3   7  33    2   1\nCluster 0 -&gt; Topic TR\n    top words: ['portfolio', 'market', 'trading', 'strategy', 'order', 'asset', 'optimal', 'risk', 'price', 'model']\nCluster 1 -&gt; Topic MF\n    top words: ['optimal', 'problem', 'time', 'market', 'price', 'model', 'utility', 'process', 'strategy', 'function']\nCluster 2 -&gt; Topic PR\n    top words: ['model', 'volatility', 'process', 'option', 'price', 'stochastic', 'pricing', 'time', 'rate', 'jump']\nCluster 3 -&gt; Topic GN\n    top words: ['financial', 'market', 'network', 'risk', 'bank', 'economic', 'firm', 'crisis', 'systemic', 'country']\nCluster 4 -&gt; Topic ST\n    top words: ['market', 'time', 'distribution', 'model', 'stock', 'price', 'return', 'financial', 'correlation', 'dynamic']\nCluster 5 -&gt; Topic ST\n    top words: ['stock', 'data', 'model', 'market', 'learning', 'price', 'financial', 'based', 'using', 'network']\nCluster 6 -&gt; Topic RM\n    top words: ['risk', 'measure', 'option', 'method', 'pricing', 'approach', 'value', 'hedging', 'problem', 'model']\nCluster 7 -&gt; Topic RM\n    top words: ['model', 'risk', 'rate', 'insurance', 'capital', 'bond', 'company', 'loss', 'life', 'income']\n              precision    recall  f1-score   support\n\n          CP       0.00      0.00      0.00       217\n          GN       0.53      0.50      0.52       300\n          MF       0.42      0.50      0.45       257\n          PM       0.00      0.00      0.00       179\n          PR       0.34      0.40      0.37       215\n          RM       0.33      0.52      0.40       207\n          ST       0.46      0.72      0.57       371\n          TR       0.38      0.38      0.38       184\n\n    accuracy                           0.42      1930\n   macro avg       0.31      0.38      0.34      1930\nweighted avg       0.34      0.42      0.37      1930\n\n\n\n\n\n\nSupervised Learning\nSince our dataset has the label for each observation, we also apply the supervised machine learning algorithms, Linear SVC and Random Forest Classifiers. As it is a multi-class classification problem, we used the OneVsRestClassifer in the scikit-learn package.\nSupervised machine learning algorithms generally have better performance than unsupervised machine learning algorithms. Compared to Linear SVC, the results of our Random Forest Classifier for categorizing academic papers within the Quantitative Finance domain demonstrate promising performance, with an overall accuracy of 0.62. The classification metrics across the subcategories display variability, as shown by the differences in precision, recall, and F1-score. Certain subcategories, such as General Finance (GN) and Trading and Market Microstructure (TR), exhibit relatively high performance with F1-scores of 0.69 and 0.65, respectively. In contrast, Mathematical Finance (MF) and Computational Finance (CP) present lower performance with F1-scores of 0.43 and 0.51, respectively. These findings illustrate the effectiveness of supervised machine learning techniques, such as the Random Forest Classifier, in accurately classifying academic papers and identifying significant keywords within the Quantitative Finance field.\nIn addition, for the best model in this subsection, we further show the Precision-Recall Curve and ROC Curve to enhance the visualization of the performance of Random Forest Classifiers. However, the results also highlight the necessity for further refinements in the methodology to improve classification performance for specific subcategories.\n\n#One Hot Encoding \ncategories = np.unique(df['subcategory']).tolist()\ndf[categories] = pd.get_dummies(df['subcategory'])\ndf[categories]\n\n\n\n\n\n\n\n\nCP\nGN\nMF\nPM\nPR\nRM\nST\nTR\n\n\n\n\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n4\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9644\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n9645\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n9646\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n9647\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n9648\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n\n\n9649 rows × 8 columns\n\n\n\nLikewise, we defined a helper function supervised_learning to offer the flexibility of using different base classifiers.\n\n\nCode\ndef supervised_learning(train_df, test_df, min_df = 1, stopwords = None, base_model = LinearSVC()):\n    tfidf_vect = TfidfVectorizer(stop_words=stopwords, min_df=min_df)\n    X_train = tfidf_vect.fit_transform(train_df['tokenized_summary_new'])\n    y_train = train_df[categories]\n    X_test = tfidf_vect.transform(test_df['tokenized_summary_new'])\n    y_test = test_df[categories].values\n\n    model = OneVsRestClassifier(base_model)\n    model.fit(X_train, y_train)\n        \n    try:\n        predict_p = model.predict_proba(X_test)\n        y_pred = np.zeros_like(predict_p)\n        y_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        \n        precision = dict()\n        recall = dict()\n        for i in range(len(categories)):\n            precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        predict_p[:, i])\n            plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(categories[i]))\n    \n        plt.xlabel(\"Recall\")\n        plt.ylabel(\"Precision\")\n        plt.title(\"Precision-Recall Curve\")\n        plt.legend(loc=\"lower left\")\n        plt.show();\n    \n        # roc curve\n        fpr = dict()\n        tpr = dict()\n\n        for i in range(len(categories)):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i],predict_p[:, i])\n            plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(categories[i]))\n\n        plt.xlabel(\"false positive rate\")\n        plt.ylabel(\"true positive rate\")\n        plt.legend(loc=\"best\")\n        plt.title(\"ROC curve\")\n        plt.show();\n        return model,tfidf_vect\n    \n    except:\n        y_pred = model.predict(X_test)\n        print(metrics.classification_report(y_test, y_pred, target_names=categories))\n        print('The base model has no predict_proba function. ROC, PRC curve cannot be plotted!')\n        return model,tfidf_vect\n\n\n\nSupport Vector Machine\n\nbase_model = LinearSVC()\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n_,_= supervised_learning(train_df, test_df, min_df = 1, stopwords = 'english', base_model =base_model)\n\n              precision    recall  f1-score   support\n\n          CP       0.64      0.36      0.46       217\n          GN       0.77      0.55      0.64       300\n          MF       0.50      0.30      0.37       257\n          PM       0.68      0.49      0.57       179\n          PR       0.56      0.43      0.49       215\n          RM       0.71      0.50      0.59       207\n          ST       0.71      0.56      0.63       371\n          TR       0.73      0.46      0.56       184\n\n   micro avg       0.67      0.46      0.55      1930\n   macro avg       0.66      0.46      0.54      1930\nweighted avg       0.67      0.46      0.55      1930\n samples avg       0.45      0.46      0.45      1930\n\nThe base model has no predict_proba function. ROC, PRC curve cannot be plotted!\n\n\n\n\nRandom Forest\n\nbase_model = RandomForestClassifier()\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\nmodel,tfidf_vect = supervised_learning(train_df, test_df, min_df = 1, stopwords = 'english', base_model =base_model)\n\n              precision    recall  f1-score   support\n\n          CP       0.66      0.41      0.51       217\n          GN       0.66      0.72      0.69       300\n          MF       0.49      0.38      0.43       257\n          PM       0.61      0.64      0.62       179\n          PR       0.56      0.67      0.61       215\n          RM       0.69      0.64      0.66       207\n          ST       0.62      0.76      0.68       371\n          TR       0.66      0.64      0.65       184\n\n   micro avg       0.62      0.62      0.62      1930\n   macro avg       0.62      0.61      0.61      1930\nweighted avg       0.62      0.62      0.61      1930\n samples avg       0.62      0.62      0.62      1930\n\n\n\n\n\n\n\n\n\n\n\nConfusion Matrix\n\n\nCode\nX_test = tfidf_vect.transform(test_df['tokenized_summary_new'])\ny_test = test_df[categories].values\ny_test = np.argmax(y_test, axis=1)\npredict_p = model.predict_proba(X_test)\ny_pred = np.zeros_like(predict_p)\ny_pred[np.arange(len(predict_p)), predict_p.argmax(1)] = 1\ny_pred = np.argmax(y_pred, axis=1)\n\nlabel_dict = dict()\nfor i in range(8):\n    label_dict[i] = categories[i]\n    \ny_true_labels = [label_dict[num] for num in y_test]\ny_pred_labels = [label_dict[num] for num in y_pred]\n\nConfusionMatrixDisplay.from_predictions(y_true_labels, y_pred_labels,cmap=plt.cm.Blues,values_format='g');\n\n\n\n\n\n\n\nCode\n#feature importance \n\nfor i in range(len(categories)):\n\n    important_features = []\n\n    for feature, importance in sorted(zip(tfidf_vect.get_feature_names_out(), model.estimators_[i].feature_importances_), \\\n                                  key=lambda x: x[1], reverse=True):\n        #print(feature, importance)\n        important_features.append(feature)\n    top_words=important_features[0:10]\n    print ('{}: {}'.format(categories[i],top_words))\n    print(\"\\n\")\n\n\nCP: ['carlo', 'monte', 'method', 'numerical', 'option', 'scheme', 'algorithm', 'computational', 'approximation', 'neural']\n\n\nGN: ['economic', 'economy', 'country', 'growth', 'economics', 'risk', 'income', 'production', 'business', 'firm']\n\n\nMF: ['arbitrage', 'problem', 'existence', 'stochastic', 'process', 'optimal', 'model', 'martingale', 'pricing', 'price']\n\n\nPM: ['portfolio', 'optimization', 'investment', 'optimal', 'problem', 'markowitz', 'utility', 'selection', 'strategy', 'constraint']\n\n\nPR: ['pricing', 'option', 'formula', 'price', 'swap', 'volatility', 'derivative', 'european', 'implied', 'model']\n\n\nRM: ['risk', 'measure', 'shortfall', 'loss', 'insurance', 'price', 'systemic', 'market', 'var', 'default']\n\n\nST: ['series', 'stock', 'correlation', 'index', 'data', 'return', 'prediction', 'time', 'analysis', 'financial']\n\n\nTR: ['book', 'order', 'trading', 'execution', 'impact', 'trader', 'market', 'limit', 'trade', 'volume']"
  },
  {
    "objectID": "scrape.html",
    "href": "scrape.html",
    "title": "Scrape data from Arxiv",
    "section": "",
    "text": "In this part we levarage the official API of Arxiv to scrape the data we need. The api is well documented in Arxiv’s user manual."
  },
  {
    "objectID": "scrape.html#auxiliary-functions-for-extracting-extra-features-from-scraped-data",
    "href": "scrape.html#auxiliary-functions-for-extracting-extra-features-from-scraped-data",
    "title": "Scrape data from Arxiv",
    "section": "Auxiliary functions for extracting extra features from scraped data",
    "text": "Auxiliary functions for extracting extra features from scraped data\nSometimes the features we need are not directly available from the API. We need to extract them from the raw data. Here are some auxiliary functions for this purpose:\n\nget_num_figures: Get the number of figures in the paper from comment field.\nget_num_pages: Get the number of pages in the paper from comment field.\nget_num_eqs: For heavy math papers, we can get the number of equations from summary field.\nreplace_eq: Replace the equation in summary field with a placeholder.\n\n\n\nCode\nimport re\n\n\ndef get_num_figures(comment: str):\n    if comment is None:\n        return 0\n\n    # regex pattern to match \"n figures\" or \"n + m figures\", or mixtures of both\n    pattern = r\"\\d+\\s*\\+\\s*\\d+\\s*figures|\\d+\\s*figures\"\n    matches = re.findall(pattern, comment)\n\n    if len(matches) == 0:\n        return 0\n    else:\n        num_figures = sum(sum(int(n) for n in re.findall(r\"\\d+\", match))\n                        for match in matches)\n    return num_figures\n\n\ndef get_num_pages(comment: str):\n    if comment is None:\n        return 0\n\n    # regex pattern to match \"n pages\" or \"n + m pages\", no mixtures\n    pattern = r\"\\d+\\s*\\+\\s*\\d+\\s*pages|\\d+\\s*pages\"\n    match = re.search(pattern, comment)\n\n    if not match:\n        return 0\n    else:\n        match = match.group()\n        num_pages = sum(int(n) for n in re.findall(r\"\\d+\", match))\n    return num_pages\n\n\ndef get_number_eqs(summary: str):\n    if summary is None:\n        return 0\n\n    pattern = r\"\\${1,2}([\\s\\S]+?)\\${1,2}\"\n    matches = re.findall(pattern, summary)\n    return len(matches)\n\n\ndef replace_eq(summary: str, replacement: str):\n    if summary is None:\n        return summary\n\n    pattern = r\"\\${1,2}([\\s\\S]+?)\\${1,2}\"\n    return re.sub(pattern, replacement, summary)"
  },
  {
    "objectID": "scrape.html#scraper",
    "href": "scrape.html#scraper",
    "title": "Scrape data from Arxiv",
    "section": "Scraper",
    "text": "Scraper\nCompared to the pre-existing package(arxiv.arxiv) and official API, we use request instead of urllib to make the code more readable and easier to maintain, and we leverage the beautifulsoup package to parse the xml response.\nOur scraper contains two classes Paper and Search:\n\nPaper: A class for storing the information of a single paper.\nSearch: A class for sending request to the API and parsing the response, the responses will be stored in a list of Paper objects and return by the results method. We also implement a to_dataframe method to convert the results to a pandas dataframe, which is useful in the later part.\n\n\n\nCode\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n\nclass Paper:\n    def __init__(self, *args, **kwargs) -&gt; None:\n        self.paper_id = kwargs.get('paper_id')\n        self.updated = kwargs.get('updated')\n        self.published = kwargs.get('published')\n        self.title = kwargs.get('title')\n        self.authors = kwargs.get('authors')\n        self.summary = kwargs.get('summary')\n        self.comment = kwargs.get('comment')\n        self.link = kwargs.get('link')\n        self.doi = kwargs.get('doi')\n        self.pdf_link = kwargs.get('pdf_link')\n        self.term = kwargs.get('term')\n        self.metadata = self.to_dict()\n\n    def __repr__(self) -&gt; str:\n        return f'Paper({self.paper_id})'\n\n    def __str__(self) -&gt; str:\n        return (f\"title: {self.title}\\n\"\n                f\"authors: {self.authors}\\n\"\n                f\"link: {self.link}\\n\"\n                f\"doi: {self.doi}\\n\"\n                f\"pdf_link: {self.pdf_link}\\n\"\n                f\"term: {self.term}\\n\")\n\n    def to_dict(self) -&gt; dict:\n        return {\n            'paper_id': self.paper_id,\n            'updated': self.updated,\n            'published': self.published,\n            'title': self.title,\n            'authors': self.authors,\n            'summary': self.summary,\n            'comment': self.comment,\n            'link': self.link,\n            'doi': self.doi,\n            'pdf_link': self.pdf_link,\n            'term': self.term\n        }\n\n    def download_pdf(self, path=\"./data\"):\n        # TODO: implement\n        pass\n\n\nclass Search:\n    def __init__(self, query: str = None, id_list: list = None, start: int = 0,\n                max_results: int = 2000, sort_by: str = \"relevance\", sort_order: str = 'descending') -&gt; None:\n        self.query = query\n        self.id_list = id_list\n        self.start = start\n        self.max_results = max_results\n        self.sort_by = sort_by\n        self.sort_order = sort_order\n        self.url = self._url()  # query url\n        self.response = self._check_response()  # response from query\n\n    def __str__(self) -&gt; str:\n        return f\"query at {self.query_date} for {self.query}/{self.id_list}\"\n\n    def _url(self) -&gt; str:\n        base_url = \"http://export.arxiv.org/api/query?\"\n        if self.query:\n            query = f\"search_query={self.query}\"\n        elif self.id_list:\n            query = f\"id_list={','.join(self.id_list)}\"\n        else:\n            raise ValueError(\"Must provide query or id_list\")\n\n        url =  (f\"{base_url}\"\n                f\"{query}&start={self.start}\"\n                f\"&max_results={self.max_results}\"\n                f\"&sortBy={self.sort_by}\"\n                f\"&sortOrder={self.sort_order}\")\n        return url\n\n    def _check_response(self) -&gt; None:\n        url = self._url()\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'xml')\n        if soup.find(\"entry\") is None:\n            print(f\"No results found for {url}\")\n        if soup.find(\"entry\").find(\"title\", recursive=False).text == \"Error\":\n            print(url)\n            raise ValueError(\"Invalid query or id_list\")\n        else:\n            return soup\n\n    def _parse_xml(self, xml):\n        paper_id = xml.find('id').text\n        updated = xml.find('updated').text\n        published = xml.find('published').text\n        title = xml.find('title').text\n        authors = [author.find(\n            'name').text for author in xml.find_all('author')]\n        summary = xml.find('summary').text\n        comment = xml.find('arxiv:comment').text if xml.find(\n            'arxiv:comment') else None\n        link = xml.find(\"link\", attrs={\"rel\": \"alternate\"})['href']\n        pdf_link = xml.find(\"link\", attrs={\"title\": \"pdf\"})['href']\n        doi = xml.find(\"arxiv:doi\").text if xml.find(\"arxiv:doi\") else None\n        term = xml.find('arxiv:primary_category')['term']\n        return Paper(paper_id=paper_id, updated=updated, published=published, title=title, authors=authors,\n                    summary=summary, comment=comment, link=link, doi=doi, pdf_link=pdf_link, term=term)\n\n    def results(self) -&gt; list:\n        soup = self.response\n        entries = soup.find_all('entry')\n        papers = []\n        for entry in entries:\n            paper = self._parse_xml(entry)\n            papers.append(paper)\n        return papers\n\n    def to_dataframe(self) -&gt; pd.DataFrame:\n        papers = self.results()\n        df = pd.DataFrame([paper.to_dict() for paper in papers])\n        df['num_figures'] = df['comment'].apply(get_num_figures)\n        df['num_pages'] = df['comment'].apply(get_num_pages)\n        df['num_eqs'] = df['summary'].apply(get_number_eqs)\n        df['main_category'] = df['term'].apply(lambda x: x.split('.')[0])\n        df['updated'] = pd.to_datetime(df['updated'])\n        df['published'] = pd.to_datetime(df['published'])\n        return df\n\n\nA typical workflow of using the scraper is as follows:\n\nsearch = Search(query=\"cat:q-fin.*\", start=0, max_results=10,\n                sort_by=\"submittedDate\", sort_order=\"descending\")\ndf = search.to_dataframe()\nprint(len(df))\ndf.sample(3)\n\n10\n\n\n\n\n\n\n\n\n\npaper_id\nupdated\npublished\ntitle\nauthors\nsummary\ncomment\nlink\ndoi\npdf_link\nterm\nnum_figures\nnum_pages\nnum_eqs\nmain_category\n\n\n\n\n5\nhttp://arxiv.org/abs/2305.00541v1\n2023-04-30 17:55:16+00:00\n2023-04-30 17:55:16+00:00\nA Stationary Mean-Field Equilibrium Model of I...\n[René Aid, Matteo Basei, Giorgio Ferrari]\nWe consider a mean-field model of firms comp...\n32 pages; 4 figures\nhttp://arxiv.org/abs/2305.00541v1\nNone\nhttp://arxiv.org/pdf/2305.00541v1\nmath.OC\n4\n32\n0\nmath\n\n\n4\nhttp://arxiv.org/abs/2305.00545v1\n2023-04-30 18:11:34+00:00\n2023-04-30 18:11:34+00:00\nOptimal multi-action treatment allocation: A t...\n[Achim Ahrens, Alessandra Stampi-Bombelli, Sel...\nThe challenge of assigning optimal treatment...\nNone\nhttp://arxiv.org/abs/2305.00545v1\nNone\nhttp://arxiv.org/pdf/2305.00545v1\necon.GN\n0\n0\n0\necon\n\n\n9\nhttp://arxiv.org/abs/2305.00200v1\n2023-04-29 08:54:20+00:00\n2023-04-29 08:54:20+00:00\nCalibration of Local Volatility Models with St...\n[Gregoire Loeper, Jan Obloj, Benjamin Joseph]\nWe develop a non-parametric, optimal transpo...\nNone\nhttp://arxiv.org/abs/2305.00200v1\nNone\nhttp://arxiv.org/pdf/2305.00200v1\nq-fin.MF\n0\n0\n0\nq-fin"
  },
  {
    "objectID": "scrape.html#scraping-data",
    "href": "scrape.html#scraping-data",
    "title": "Scrape data from Arxiv",
    "section": "Scraping data",
    "text": "Scraping data\nIn this part, we scrape 16000 papers from all the categories of Arxiv. And to reuse the code, we create two json files main_cats.json and physics_cats.json to store the categories.(Physics has too many subcategories, so we split all the categories into two parts.)\n\nimport json\n\nwith open(\"data/main_cats.json\") as f:\n    main_cats = json.load(f)\n\nwith open('data/physics_cats.json') as f:\n    physics_cats = json.load(f)\n    \nall_cats = list(main_cats.keys()) + list(physics_cats.keys())\nprint(all_cats)\n\n['cs', 'econ', 'eess', 'math', 'q-bio', 'q-fin', 'stat', 'astro-ph', 'cond-mat', 'gr-qc', 'hep-ex', 'math-ph', 'nlin', 'nucl-ex', 'nucl-th', 'physics', 'quant-ph']\n\n\nAs documented in arxiv API,\n\nthe maximum number of a single call is limited to 30000 in slices of 2000 at time, using the start and max_results parameters. A request for 30000 results will typically take a little over 2 minutes to return a response of over 15MB. Requests for fewer results are much faster and correspondingly smaller.\n\nSo we set max_results to 2000 and scrape 8 times for each category such that the total number of papers for each category is 16000.\nSometimes the API returns error or empty response, for such cases we will save the failed categoreis and retry them later.\n\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm\n\ndef query_range(cat, start, max_results):\n    search = Search(query=f\"cat:{cat}.*\", start=start, max_results=max_results,\n                    sort_by='lastUpdatedDate', sort_order='descending')\n    temp_df = search.to_dataframe()\n    if not os.path.exists(f\"datasets/{cat}\"):\n        os.mkdir(f\"datasets/{cat}\")\n    temp_df.to_csv(f\"datasets/{cat}/{cat}_{int(start/2000)}.csv\", index=False)\n\n\nfailed = {}\n\nfor cat in physics_cats:\n    print(f\"Querying {cat}\")\n    for i in tqdm(range(0, 16000, 2000)):\n        start = i\n        end = i + 2000\n        try:\n            query_range(cat, start, 2000)\n            time.sleep(3)\n        except:\n            failed.setdefault(cat, []).append(start)\n            print(f\"Error at {cat} from {start} to {end}\")\n\n\nfailed\n\n{'gr-qc': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'hep-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'math-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-th': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'physics': [4000, 12000],\n 'quant-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]}\n\n\n\nredos = failed.copy()\nfailed = {}\n\nfor cat in redos:\n    print(f\"Querying {cat}\")\n    for i in tqdm(redos[cat]):\n        start = i\n        end = i + 2000\n        try:\n            query_range(cat, start, 2000)\n            time.sleep(3)\n        except:\n            failed.setdefault(cat, []).append(start)\n            print(f\"Error at {cat} from {start} to {end}\")\n\n{'gr-qc': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'hep-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'math-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-ex': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'nucl-th': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000],\n 'quant-ph': [0, 2000, 4000, 6000, 8000, 10000, 12000, 14000]}\n\n\nSometimes the number of papers returned by a single call is less than 1600, we check the number of papers returned by each call and save the failed categories and retry them later.\n\nimport glob\nimport re\ncats_data = glob.glob(\"datasets/*\")\n\nall_data = []\nfor cat in cats_data:\n    all_data += glob.glob(f\"{cat}/*.csv\")\n\n\nincomplete = []\n\nfor d in all_data:\n    len_df = len(pd.read_csv(d))\n    match = re.search(r\"datasets/([\\w-]+)/\\1_(\\d+)\\.csv$\", d)\n    cat = match.group(1)\n    num = int(match.group(2))\n    if len_df &lt; 2000 and num != 7:\n        print(f\"{cat} {num} has {len_df} rows\")\n        incomplete.append((cat, num))\n\necon 3 has 1573 rows\n\n\n\n\nCode\nfor cat, num in incomplete:\n    start = num*2000\n    end = start + 2000\n    try:\n        query_range(cat, num*2000, 2000)\n        time.sleep(3)\n    except:\n        failed2.setdefault(cat, []).append(start)\n        print(f\"Error at {cat} from {start} to {end}\")"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "In this section, based on the data we have collected, we will visualize the data in different ways to help us better understand the data.\nPacakges used in this notebook:\nCode\nimport numpy as np\nimport pandas as pd\nimport os\nimport glob\n\nimport seaborn as sns\nimport seaborn.objects as so\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "visualization.html#merge-datasets",
    "href": "visualization.html#merge-datasets",
    "title": "Data Visualization",
    "section": "Merge datasets",
    "text": "Merge datasets\nConcatenate all the dataframes from different subjects into one dataframe.\n\nif not os.path.exists('data.csv'):\n    df = pd.DataFrame()\n    for path in glob.glob(\"./datasets/*\"):\n        cat_csv = glob.glob(f\"{path}/*.csv\")\n        for csv in cat_csv:\n            df = pd.concat([df, pd.read_csv(csv)], axis=0)\n    df.to_csv(f\"./data.csv\", index=False)\n\nelse:\n    df = pd.read_csv(\"./data.csv\")"
  },
  {
    "objectID": "visualization.html#data-cleaning",
    "href": "visualization.html#data-cleaning",
    "title": "Data Visualization",
    "section": "Data cleaning",
    "text": "Data cleaning\nGenerally the data returned by arxiv api is pretty clean, we only need to perform some basic cleaning on merged dataset:\n\nDrop columns if no title and summary\nDrop duplicates if title and summary are the same\nSort by date again\n\nFurther cleaning will be done in the specific tasks.\n\ndf = df.dropna(subset=['title', 'authors'])\ndf = df.sort_values(by='updated', ascending=False)\ndf = df.drop_duplicates(subset=['title', 'authors'])\nlen(df)\n\n146021\n\n\n\nExtract papers with doi for introduction part\n\ndoi_papers = df[df['doi'].notnull()]\ndoi_papers.to_csv(\"./doi_papers.csv\", index=False)\ndoi_papers['main_category'].value_counts()\n\nmain_category\nastro-ph    8470\ncond-mat    6128\nphysics     5665\nnlin        4184\nq-bio       3110\ncs          2778\nq-fin       2395\nmath        1812\neess        1323\nquant-ph    1065\nstat         850\nmath-ph      819\necon         690\nhep-th       668\ngr-qc        630\nhep-ph       395\nnucl-th      104\nhep-ex        63\nhep-lat       28\nnucl-ex       23\nchao-dyn       1\nName: count, dtype: int64\n\n\n\n\nUse q-fin data for analysis inside one subject\n\nfin_papers = df[df['main_category'] == 'q-fin']\nfin_papers.to_csv(\"./fin_papers.csv\", index=False)\n\nfin_papers['term'].value_counts()\n\nterm\nq-fin.ST    1738\nq-fin.GN    1424\nq-fin.MF    1316\nq-fin.PR    1137\nq-fin.RM    1104\nq-fin.CP    1027\nq-fin.PM     983\nq-fin.TR     920\nq-fin.EC     383\nName: count, dtype: int64"
  },
  {
    "objectID": "visualization.html#data-visualization",
    "href": "visualization.html#data-visualization",
    "title": "Data Visualization",
    "section": "Data visualization",
    "text": "Data visualization\n\nCategories distribution\n\n\nCode\ndef filter(data, pct: float):\n    \"\"\"make entries with less than pct of total sum as others\"\"\"\n    n = data.sum()\n    data['others'] = data[data &lt; n * pct].sum()\n    data = data[data &gt;= n * pct]\n    return data.sort_values(ascending=False).to_dict()\n\n\n\n\nCode\nall_counts = df['main_category'].value_counts()\nall_counts = filter(all_counts, 0.02)\n\nsns.set_style(\"dark\")\nsns.set_palette('pastel')\nplt.pie(all_counts.values(), labels=all_counts.keys(),\n        autopct='%1.1f%%', labeldistance=1.05, pctdistance=0.75,)\nplt.title('Main Category Distribution')\nplt.show()\n\n\n\n\n\nCategory Distribution\n\n\n\n\nOur whole dataset is balanced, next look at the distribution of each subject in the dataset.\n\n\nCode\nall_cats = all_counts.keys()\nall_cats = set(all_cats) - set(['others'])\n\n\nfig, axs = plt.subplots(4, 3, figsize=(20, 15))\nfor i, cat in enumerate(all_cats):\n    row = i // 3\n    col = i % 3\n    cat_df = df.query(f\"main_category == '{cat}'\")\n    cat_count = filter(cat_df['term'].value_counts(), 0.02)\n    axs[row, col].pie(cat_count.values(), labels=cat_count.keys(),\n                      autopct='%1.1f%%', labeldistance=1.05, pctdistance=0.75,)\n    axs[row, col].set_title(f\"{cat} Distribution\")\nplt.show()\n\n\n\n\n\nCategory Distribution For Each Sub-Category\n\n\n\n\n\n\nPublication percentage\n\n\nCode\npublished = {}\n\nif all_counts.get('others'):\n    all_counts.pop('others')\n\nfor cat in all_counts:\n    cat_df = df.query(f\"main_category == '{cat}'\")\n    cat_published = cat_df['doi'].count()\n    published[cat] = cat_published\ncategories = list(published.keys())\npaper_num = list(all_counts.values())\npublished_num = list(published.values())\npct_published = [p / n for p, n in zip(published_num, paper_num)]\n\nsns.set_style('darkgrid')  # Set the plot style\ncolors = sns.color_palette(\"Paired\")\nplt.figure(figsize=(12, 8))\nplt.title('Published Papers vs All Papers')\nax = sns.barplot(x=categories, y=paper_num, alpha=0.9,\n                 color=colors[0], errorbar=\"sd\", width=0.5,\n                 label='Num of Papers')\nsns.barplot(x=categories, y=published_num,\n            alpha=0.8, color=colors[1], errorbar=\"sd\", width=0.5,\n            label='Num of Published Papers')\nfor container in ax.containers:\n    ax.bar_label(container, fmt='%.0f', label_type='edge')\n\nplt.legend()\nplt.show()\n\n\n\n\n\nPublished Papers vs All Papers\n\n\n\n\n\n\nWho likes to publish on arxiv?\nPaper submitted/updated from March 2023 to April 2023\n\n\nCode\ncolors = sns.color_palette(\"Set2\")\ncut_off = '2023-3-15'\nsub_df = df.query(\"main_category in @categories\")\nsub_df = df[df['updated'] &gt; cut_off]\ngrouped = sub_df.groupby([pd.Grouper(key='updated', freq='D'),\n                      'main_category']).count()\n\npivot = grouped['paper_id'].unstack().fillna(0)\npivot = pivot.cumsum()\n\n# drop categories with less than 500 papers\nnum_papers = pivot.iloc[-1]\ncols = num_papers[num_papers &gt; 500].index\npivot = pivot[cols]\n\nax = pivot.plot(figsize=(10, 6))\nax.set_title('Number of paper updated over time')\nax.set_xlabel('Update Date')\nplt.show()\n\n\n\n\n\nNumber of Papers Updated Over Time"
  }
]